{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Comprehensive course materials for Linux High Availability (HA) clustering and virtualization technologies including Corosync, Pacemaker, QEMU, KVM, CEPH, iSCSI, FC/NVMe-oF, GFS2, libvirt, virsh, and Open vSwitch. Table of Contents Core Technologies Cluster Technologies - Corosync, Pacemaker Virtualization - QEMU, KVM, libvirt, virsh, Open vSwitch Storage and File Systems - CEPH, iSCSI, NVMe-oF, GFS2 Reference Quick Reference - Command cheatsheets Deployment Workflows - Setup guides Troubleshooting - Code behavior diagnostics Additional Network and Switching - libvirt networking Resources - External links License - License information Quick Start # Clone repository git clone https://github.com/CrakA2/Linux_HA.git cd Linux_HA # View locally mkdocs serve # Open http://127.0.0.1:8000 Topics Covered Cluster Management Corosync messaging and quorum Pacemaker resource management STONITH and high availability configuration Cluster deployment and configuration Virtualization QEMU/KVM hypervisor setup libvirt VM management Open vSwitch networking Live migration and resource optimization Distributed Storage CEPH HCI deployment RBD block storage CephFS shared filesystem iSCSI/NVMe-oF gateways File Systems GFS2 clustered filesystem DLM lock management CLVM volume management Documentation Structure docs/ \u251c\u2500\u2500 cluster/ # Corosync, Pacemaker \u251c\u2500\u2500 virtualization/ # QEMU, KVM, libvirt, virsh, OVS \u251c\u2500\u2500 storage/ # CEPH, iSCSI, NVMe-oF, GFS2 \u251c\u2500\u2500 network/ # libvirt networking \u251c\u2500\u2500 reference.md # Quick command reference \u251c\u2500\u2500 deployment.md # Deployment guides \u2514\u2500\u2500 troubleshooting-code-behavior.md # Diagnostics Source Code References Technology Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org CEPH ceph/ceph docs.ceph.com Open vSwitch openvswitch/ovs openvswitch.org GFS2 torvalds/linux gfs2-utils Contributing This is a personal knowledge base for Linux HA and virtualization technologies. Corrections and additions are welcome via pull requests to the GitHub repository.","title":"Home"},{"location":"#table-of-contents","text":"","title":"Table of Contents"},{"location":"#core-technologies","text":"Cluster Technologies - Corosync, Pacemaker Virtualization - QEMU, KVM, libvirt, virsh, Open vSwitch Storage and File Systems - CEPH, iSCSI, NVMe-oF, GFS2","title":"Core Technologies"},{"location":"#reference","text":"Quick Reference - Command cheatsheets Deployment Workflows - Setup guides Troubleshooting - Code behavior diagnostics","title":"Reference"},{"location":"#additional","text":"Network and Switching - libvirt networking Resources - External links License - License information","title":"Additional"},{"location":"#quick-start","text":"# Clone repository git clone https://github.com/CrakA2/Linux_HA.git cd Linux_HA # View locally mkdocs serve # Open http://127.0.0.1:8000","title":"Quick Start"},{"location":"#topics-covered","text":"","title":"Topics Covered"},{"location":"#cluster-management","text":"Corosync messaging and quorum Pacemaker resource management STONITH and high availability configuration Cluster deployment and configuration","title":"Cluster Management"},{"location":"#virtualization","text":"QEMU/KVM hypervisor setup libvirt VM management Open vSwitch networking Live migration and resource optimization","title":"Virtualization"},{"location":"#distributed-storage","text":"CEPH HCI deployment RBD block storage CephFS shared filesystem iSCSI/NVMe-oF gateways","title":"Distributed Storage"},{"location":"#file-systems","text":"GFS2 clustered filesystem DLM lock management CLVM volume management","title":"File Systems"},{"location":"#documentation-structure","text":"docs/ \u251c\u2500\u2500 cluster/ # Corosync, Pacemaker \u251c\u2500\u2500 virtualization/ # QEMU, KVM, libvirt, virsh, OVS \u251c\u2500\u2500 storage/ # CEPH, iSCSI, NVMe-oF, GFS2 \u251c\u2500\u2500 network/ # libvirt networking \u251c\u2500\u2500 reference.md # Quick command reference \u251c\u2500\u2500 deployment.md # Deployment guides \u2514\u2500\u2500 troubleshooting-code-behavior.md # Diagnostics","title":"Documentation Structure"},{"location":"#source-code-references","text":"Technology Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org CEPH ceph/ceph docs.ceph.com Open vSwitch openvswitch/ovs openvswitch.org GFS2 torvalds/linux gfs2-utils","title":"Source Code References"},{"location":"#contributing","text":"This is a personal knowledge base for Linux HA and virtualization technologies. Corrections and additions are welcome via pull requests to the GitHub repository.","title":"Contributing"},{"location":"deployment/","text":"HA Cluster Setup # 1. Install components apt-get install corosync pacemaker pcs # 2. Generate keys corosync-keygen # 3. Configure Corosync vim /etc/corosync/corosync.conf # 4. Start cluster systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd # 5. Configure Pacemaker pcs cluster setup --name mycluster node1 node2 node3 pcs cluster start --all Virtualization Setup # 1. Install packages apt-get install qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch-switch # 2. Enable services systemctl enable libvirtd systemctl start libvirtd # 3. Configure networking systemctl enable openvswitch systemctl start openvswitch # 4. Create VM virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path = /var/lib/libvirt/images/vm1.qcow2,size = 20 \\ --network network = default CEPH HCI Setup # 1. Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm # 2. Bootstrap cluster ./cephadm bootstrap --mon-ip <mon-ip> # 3. Add storage ceph orch apply osd --all-available-devices # 4. Verify cluster ceph -s ceph status","title":"Deployment"},{"location":"deployment/#ha-cluster-setup","text":"# 1. Install components apt-get install corosync pacemaker pcs # 2. Generate keys corosync-keygen # 3. Configure Corosync vim /etc/corosync/corosync.conf # 4. Start cluster systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd # 5. Configure Pacemaker pcs cluster setup --name mycluster node1 node2 node3 pcs cluster start --all","title":"HA Cluster Setup"},{"location":"deployment/#virtualization-setup","text":"# 1. Install packages apt-get install qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch-switch # 2. Enable services systemctl enable libvirtd systemctl start libvirtd # 3. Configure networking systemctl enable openvswitch systemctl start openvswitch # 4. Create VM virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path = /var/lib/libvirt/images/vm1.qcow2,size = 20 \\ --network network = default","title":"Virtualization Setup"},{"location":"deployment/#ceph-hci-setup","text":"# 1. Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm # 2. Bootstrap cluster ./cephadm bootstrap --mon-ip <mon-ip> # 3. Add storage ceph orch apply osd --all-available-devices # 4. Verify cluster ceph -s ceph status","title":"CEPH HCI Setup"},{"location":"license/","text":"Course materials follow documentation licenses of their respective projects: Corosync: BSD License Pacemaker: GPL-2.0 and LGPL-2.1 QEMU/KVM: GPL-2.0 CEPH: GPL-2.0, LGPL-2.1, LGPL-3.0 GFS2: GPL-2.0 libvirt: LGPL-2.1 Open vSwitch: Apache 2.0","title":"License"},{"location":"reference/","text":"Corosync Command Description corosync-cfgtool -s Show cluster status corosync-quorumtool -s Show quorum status corosync-cmapctl Query configuration corosync-keygen Generate auth key Pacemaker Command Description pcs status Display cluster status pcs resource create Create resource pcs constraint order Add ordering pcs constraint colocation Add colocation pcs stonith create Create STONITH QEMU/KVM Command Description qemu-system-x86_64 Run QEMU VM virsh list List VMs virsh start Start VM virsh shutdown Shutdown VM kvm-ok Check KVM support CEPH Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image ceph orch apply Apply service iSCSI Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets rbd showmapped Show mapped RBDs NVMe-oF Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems nvme show-ctrl Show controller GFS2 Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes dlm_tool dump Dump locks Open vSwitch Command Description ovs-vsctl add-br Add bridge ovs-vsctl del-br Delete bridge ovs-vsctl show Show config ovs-ofctl show Show OpenFlow rules","title":"Reference"},{"location":"reference/#corosync","text":"Command Description corosync-cfgtool -s Show cluster status corosync-quorumtool -s Show quorum status corosync-cmapctl Query configuration corosync-keygen Generate auth key","title":"Corosync"},{"location":"reference/#pacemaker","text":"Command Description pcs status Display cluster status pcs resource create Create resource pcs constraint order Add ordering pcs constraint colocation Add colocation pcs stonith create Create STONITH","title":"Pacemaker"},{"location":"reference/#qemukvm","text":"Command Description qemu-system-x86_64 Run QEMU VM virsh list List VMs virsh start Start VM virsh shutdown Shutdown VM kvm-ok Check KVM support","title":"QEMU/KVM"},{"location":"reference/#ceph","text":"Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image ceph orch apply Apply service","title":"CEPH"},{"location":"reference/#iscsi","text":"Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets rbd showmapped Show mapped RBDs","title":"iSCSI"},{"location":"reference/#nvme-of","text":"Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems nvme show-ctrl Show controller","title":"NVMe-oF"},{"location":"reference/#gfs2","text":"Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes dlm_tool dump Dump locks","title":"GFS2"},{"location":"reference/#open-vswitch","text":"Command Description ovs-vsctl add-br Add bridge ovs-vsctl del-br Delete bridge ovs-vsctl show Show config ovs-ofctl show Show OpenFlow rules","title":"Open vSwitch"},{"location":"resources/","text":"Cluster Technologies Technology Official Site Source Code Corosync https://corosync.github.io/corosync/ https://github.com/corosync/corosync Pacemaker https://www.clusterlabs.org/pacemaker/ https://github.com/ClusterLabs/pacemaker Virtualization Technology Official Site Source Code QEMU https://www.qemu.org/ https://gitlab.com/qemu-project/qemu KVM https://www.linux-kvm.org/ https://github.com/torvalds/linux/tree/master/virt/kvm libvirt https://libvirt.org/ https://gitlab.com/libvirt/libvirt Open vSwitch https://www.openvswitch.org/ https://github.com/openvswitch/ovs Storage Technology Official Site Source Code CEPH https://docs.ceph.com/ https://github.com/ceph/ceph iSCSI https://linux-iscsi.org/ https://github.com/ceph/ceph NVMe-oF https://nvmexpress.org/ https://github.com/ceph/ceph GFS2 /usr/share/doc/gfs2-utils/ https://github.com/torvalds/linux/tree/master/fs/gfs2 Community and Support CEPH: https://ceph.io/community/ ClusterLabs: https://clusterlabs.org/community/ QEMU: qemu-devel@nongnu.org KVM: kvm@vger.kernel.org Bug Trackers Corosync: https://github.com/corosync/corosync/issues Pacemaker: https://bugs.clusterlabs.org/ CEPH: https://tracker.ceph.com/projects/ceph QEMU: https://gitlab.com/qemu-project/qemu/-/issues libvirt: https://gitlab.com/libvirt/libvirt/-/issues Open vSwitch: https://bugs.openvswitch.org/","title":"Resources"},{"location":"resources/#cluster-technologies","text":"Technology Official Site Source Code Corosync https://corosync.github.io/corosync/ https://github.com/corosync/corosync Pacemaker https://www.clusterlabs.org/pacemaker/ https://github.com/ClusterLabs/pacemaker","title":"Cluster Technologies"},{"location":"resources/#virtualization","text":"Technology Official Site Source Code QEMU https://www.qemu.org/ https://gitlab.com/qemu-project/qemu KVM https://www.linux-kvm.org/ https://github.com/torvalds/linux/tree/master/virt/kvm libvirt https://libvirt.org/ https://gitlab.com/libvirt/libvirt Open vSwitch https://www.openvswitch.org/ https://github.com/openvswitch/ovs","title":"Virtualization"},{"location":"resources/#storage","text":"Technology Official Site Source Code CEPH https://docs.ceph.com/ https://github.com/ceph/ceph iSCSI https://linux-iscsi.org/ https://github.com/ceph/ceph NVMe-oF https://nvmexpress.org/ https://github.com/ceph/ceph GFS2 /usr/share/doc/gfs2-utils/ https://github.com/torvalds/linux/tree/master/fs/gfs2","title":"Storage"},{"location":"resources/#community-and-support","text":"CEPH: https://ceph.io/community/ ClusterLabs: https://clusterlabs.org/community/ QEMU: qemu-devel@nongnu.org KVM: kvm@vger.kernel.org","title":"Community and Support"},{"location":"resources/#bug-trackers","text":"Corosync: https://github.com/corosync/corosync/issues Pacemaker: https://bugs.clusterlabs.org/ CEPH: https://tracker.ceph.com/projects/ceph QEMU: https://gitlab.com/qemu-project/qemu/-/issues libvirt: https://gitlab.com/libvirt/libvirt/-/issues Open vSwitch: https://bugs.openvswitch.org/","title":"Bug Trackers"},{"location":"troubleshooting-code-behavior/","text":"Troubleshooting Code Behavior and Diagnostics This section provides comprehensive guidance on diagnosing and resolving issues with clustering, virtualization, and storage technologies from a code behavior perspective. Debugging Techniques Enable Debug Logging Corosync: # Enable debug mode in corosync.conf logging { to_logfile: yes logfile: /var/log/corosync/corosync.log debug: on debug_logfile: /var/log/corosync/debug.log } # Or use cormapctl corosync-cmapctl totem.debug = 1 corosync-cmapctl totem.logging_debug = 1 Pacemaker: # Enable verbose logging crm_simulate --live-check crm_simulate --show-failcounts crm_simulate --showscores # Check detailed resource actions pcs resource show-status # Monitor CIB changes crm_mon --show-detail crm_mon --show-node-attributes QEMU/KVM: # Enable QEMU debug output qemu-system-x86_64 -d in_asm,op,exec,cpu_reset,guest_errors,int,mmu # QMP debugging qemu-system-x86_64 -qmp stdio -monitor stdio # Enable KVM debug echo 1 > /sys/module/kvm/parameters/debug echo 1 > /sys/module/kvm_intel/parameters/debug echo 1 > /sys/module/kvm_amd/parameters/debug Ceph: # Enable debug logging ceph config set global debug_msgr 10 ceph config set mon debug_mon 10 # Enable object logging ceph config set global debug_filestore 20 ceph config set global debug_rados 20 # Monitor OSD operations ceph tell osd.0 dump_ops Common Code-Level Issues Corosync Issues 1. Token Circulation Problems Symptoms: - Nodes unable to send messages - Token stuck on one node - Messages delayed excessively Causes: - Network partition - Token timeout too short - Network buffer overflow Diagnosis: corosync-cfgtool -s corosync-quorumtool -s Check token status: corosync-cmapctl runtime.totem.token Check ring state: corosync-cmapctl runtime.totem.token_hold_time Solution: Increase token timeout totem { token: 10000 # Increase from default 10000 token_retransmit: 500 } Reduce network traffic totem { window_size: 50 # Reduce from default 100 max_messages: 17 } Check network configuration ping -c 3 tcpdump -i any 'port 5405' **2. Quorum Failures** Symptoms: - Cluster loses quorum - Nodes operate independently - Resources stop responding Diagnosis: corosync-quorumtool -s Check expected vs actual votes Check node connectivity Solution: # Verify network connectivity ping -c 5 <all-cluster-nodes> # Check Corosync communication corosync-cfgtool -s # Adjust quorum settings nodelist { node { quorum_votes: 3 } } # Ensure proper node count # Use odd number of nodes for majority Pacemaker Issues 1. Resource Start Failures Symptoms: - Resources stuck in \"Starting\" state - Resources fail to start repeatedly - CRM crashes or restarts Diagnosis: pcs status Check resource status: pcs resource show-status resource-name Check CRM logs: journalctl -u pacemaker -n 50 | less Check transition history: crm_history --show Solution: Increase operation timeout pcs resource op add resource-name start \\ timeout=60s interval=0s on-fail=restart Check resource agent pcs resource debug-resource resource-name crm_resource --validate --resource resource-name Check for configuration errors crm_verify -L -V **2. Constraint Violations** Symptoms: - Resources not placed according to constraints - Unexpected resource placement - Colocation not respected Diagnosis: pcs constraint show --full Check constraint scores: crm_simulate --showscores Verify constraint definitions: pcs constraint order list pcs constraint colocation list Solution: # Verify constraints are correct pcs constraint order list pcs constraint colocation list # Adjust constraint scores pcs constraint order set order-id \\ symmetrical=false # Clear broken constraints pcs constraint remove constraint-id # Recalculate cluster state pcs resource cleanup 3. Resource Agent Bugs Symptoms: - OCF agent returns invalid status codes - Agent timeout during start/stop - Agent crashes Diagnosis: # Test agent manually ocf_resource:ocf:heartbeat:IPaddr2 monitor # Check agent logs grep \"resource-name\" /var/log/pacemaker/pengine/* # Validate agent syntax crm_resource --validate --resource resource-name Solution: Test agent manually export OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat /usr/lib/ocf/resource.d/heartbeat/IPaddr2 monitor OCF_RESKEY=OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat Update agent operation timeout pcs resource op add resource-name start \\ timeout=120s Monitor agent behavior pcs resource op monitor resource-name \\ timeout=30s interval=10s Use standard agents if possible Ensure OCF-compliant agents #### QEMU/KVM Issues **1. VM Startup Failures** Symptoms: - VM fails to boot - Kernel panic in guest - No console output Diagnosis: qemu-system-x86_64 -d in_asm,cpu_reset Check VM configuration virsh dumpxml vm-name Monitor QMP events echo '{\"execute\":\"qmp_capabilities\"}' | \\ socat UNIX-CONNECT:/var/run/libvirt/qemu/vm-name.monitor Solution: # Simplify VM configuration virt-install --name vm1 --memory 1024 --vcpus 1 # Disable problematic features qemu-system-x86_64 -no-hpet -no-acpi # Use different machine type virt-install --name vm1 --machine pc # Check disk image integrity qemu-img check disk.qcow2 # Enable serial console qemu-system-x86_64 -serial pty -monitor stdio 2. Performance Issues Symptoms: - VM runs slowly - High CPU usage on host - Poor I/O performance Diagnosis: # Monitor host CPU top -H -p qemu-system-x86_64 # Check KVM configuration lsmod | grep kvm # Monitor VM internals virsh qemu-monitor-command vm-name \"info status\" # Check disk I/O iotop -o -a # Check network I/O iftop -i br0 Solution: Enable KVM qemu-system-x86_64 -enable-kvm Use virtio drivers virt-install --disk bus=virtio --network model=virtio Enable virtio multiqueue virt-install --cpu host-passthrough,cache=writeback Tune scheduler echo 1 > /sys/module/kvm/parameters/lapic **3. Memory Issues** Symptoms: - Out of memory errors - Swap usage high - Memory leak in QEMU process Diagnosis: Check host memory free -h Check VM memory virsh dommemstat vm-name Monitor KVM memory cat /sys/module/kvm/parameters/hugepages Check QEMU memory ps -o pid,vsz,pmem -C qemu-system-x86_64 Check for ballooning virsh dominfo vm-name | grep balloon Solution: # Enable memory ballooning virt-install --balloon virtio # Enable hugepages echo 1 > /sys/kernel/mm/hugepages/hugepages-2048kB echo 1024 > /proc/sys/vm/nr_hugepages # Tune memory allocation virt-install --memory 1024 --memballoc=interleave # Limit VM memory virsh setmem vm-name 512M # Disable unused features virt-install --no-usb --no-sound Ceph Issues 1. OSD Performance Problems Symptoms: - Slow I/O operations - High latency - OSD rebalancing constantly Diagnosis: ceph tell osd.* iostat ceph osd perf # Check OSD stats ceph osd dump_ops osd.0 # Check PG states ceph pg dump # Monitor recovery ceph -w Solution: Tune OSD configuration ceph config set osd osd_op_threads 2 ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3 Enable BlueStore ceph config set osd osd_objectstore bluestore Tune memory ceph config set osd osd_memory_target 4G Adjust max open files ceph config set osd max_open_files 4096 **2. PG Distribution Issues** Symptoms: - CRUSH misplacement - Uneven data distribution - High latency on specific OSDs Diagnosis: ceph osd tree ceph pg dump Check CRUSH map ceph osd getcrushmap -o crush.map Analyze PG distribution ceph pg dump | grep pg_num Check OSD utilization ceph osd perf Solution: # Adjust PG count ceph osd pool set <pool-name> pg_num <new-pg-count> # Rebalance cluster ceph osd reweight-by-utilization # Check CRUSH rules ceph osd getcrushmap -o crush.map crushtool -i crush.map --test --num-rep 3 --rules <rule-id> # Adjust CRUSH tunables ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3 3. Network Partition Issues Symptoms: - OSDs marked down - Network timeouts - Split-brain scenario Diagnosis: ceph -s ceph quorum_status # Check network connectivity ping -c 3 <mon-host> # Monitor OSD status ceph osd tree # Check Corosync status corosync-cfgtool -s Solution: Adjust network timeouts totem { token: 10000 # Increase timeout join: 120 } Enable redundant network totem { interface { ringnumber: 1 bindnetaddr: 192.168.2.10 mcastport: 5405 } } Check firewall iptables -L -n corosync Use dedicated network Separate cluster and public networks ### Performance Analysis #### Profiling Techniques **Corosync:** ```bash # Profile with perf perf record -e cycles,instructions -g -o perf.data corosync # Analyze token circulation perf report -i perf.data -s token # Profile memory usage perf record -e cycles -g -o mem.data corosync Pacemaker: # Profile PE calculations time crm_simulate --simulate > /dev/null # Monitor PE frequency grep \"calculated transition\" /var/log/pacemaker/pengine/* QEMU: # Profile QEMU perf record -e cycles,instructions,cache-misses -g -o qemu.perf \\ qemu-system-x86_64 ... # Profile KVM operations perf record -e cycles,instructions,kvm:kvm_exit,kvm:kvm_entry -g -o kvm.perf \\ qemu-system-x86_64 -enable-kvm ... Ceph: # Profile OSD perf record -e cycles,instructions,cache-misses -g -o osd.perf \\ -p <osd-pid> # Profile network perf record -e cycles,instructions,kvm:kvm_exit -g -o network.perf \\ tcpdump -i any 'port 6800' # Profile I/O iostat -x 5 -d -c <rados-device> Memory Analysis Detecting Memory Leaks: # Monitor process memory watch -n 1 'ps -o pid,vsz,pmem -C corosync' # Check for memory growth ps -o pid,rss,vsz -C pacemaker \\ | awk '{print $2}' | sort -n # Check for unfreed objects valgrind --leak-check = full ./corosync # Analyze heap usage gdb -p corosync -batch -ex \"bt\" Concurrency Issues Pacemaker: # Monitor concurrent operations crm_mon --show-detail # Check for deadlocks grep \"deadlock\" /var/log/pacemaker/* # Monitor transition queue crm_simulate --show-failcounts Ceph: # Check for lock contention ceph daemon --admin socket tell osd.0 dump_historic_ops # Monitor thread pool ceph tell osd.0 dump_ops_in_flight # Check recovery activity ceph -w Log Analysis Understanding Log Levels Corosync: - DEBUG : Detailed diagnostic information - INFO : General informational messages - WARN : Warning conditions - ERROR : Error conditions Pacemaker: - notice : Information about cluster events - warning : Potential issues - error : Error conditions - crit : Critical failures Ceph: - debug/10 : Detailed information (level 10) - debug/20 : More information - info : General messages - warn : Warning messages - err : Error messages Log Locations Corosync: /var/log/corosync/corosync.log /var/log/corosync/debug.log /var/log/syslog Pacemaker: /var/log/pacemaker/pengine/* /var/log/pacemaker/crmd/* /var/log/pacemaker/attrd/* Ceph: /var/log/ceph/ceph.log /var/log/ceph/ceph-mon.log /var/log/ceph/ceph-osd.*.log Log Analysis Commands Corosync: # Token analysis grep \"token\" /var/log/corosync/corosync.log # Quorum events grep \"quorum\" /var/log/corosync/corosync.log # Network issues grep \"network\" /var/log/corosync/corosync.log Pacemaker: # Find failed resources grep \"FAILED\" /var/log/pacemaker/pengine/* # Analyze transitions grep \"calculated transition\" /var/log/pacemaker/pengine/* # Check for errors grep \"error\" /var/log/pacemaker/* Ceph: # OSD crashes grep \"segfault\" /var/log/ceph/ceph-osd.*.log # Slow requests grep \"slow request\" /var/log/ceph/ceph-osd.*.log # Network issues grep \"timed out\" /var/log/ceph/ceph-osd.*.log Configuration Problems Validation and Testing Before Applying Changes: # Validate Pacemaker configuration crm_verify -L -V # Test with simulation crm_simulate --live-check # Dry-run Ceph commands ceph tell osd.* injectargs --dry-run # Test iSCSI configuration gwcli.py target list Rollback Procedures: # Pacemaker snapshot cibadmin --query --local > cib-backup.xml # Rollback if needed cibadmin --replace --local --xml-file cib-backup.xml # Restore Ceph configuration ceph config set <option> <previous-value> Integration Issues Cluster Coordination Corosync and Pacemaker: # Check Corosync status from Pacemaker crm_resource --list # Verify quorum crm_attribute -q -n quorum # Check node status pcs status nodes # Ensure Pacemaker can communicate crm_mon --show-detail Ceph with Cluster Managers: # Check if cluster manager sees Ceph ceph -s # Verify network connectivity ping <mon-host> # Check monitor status ceph -w Storage Integration iSCSI Issues: # Check gateway status gwcli.py target list # Check initiator connections iscsiadm -m session # Check multipath status multipath -ll # Reset initiator state iscsiadm -m node logout -T <target-iqn> -p <portal-ip> iscsiadm -m node login -T <target-iqn> -p <portal-ip> --login RBD Issues: # Check mapped images rbd showmapped # Check image status rbd info <pool>/<image> # Unmap and remap rbd unmap <pool>/<image> rbd map <pool>/<image> # Check for stale mappings dmesg | grep rbd Advanced Diagnostics System-Level Debugging # Enable kernel debugging echo 1 > /proc/sys/kernel/sched_sched_min_granularity echo 1 > /proc/sys/kernel/khung_task_timeout_secs # Enable KVM tracing echo 1 > /sys/module/kvm/parameters/trace_gue # Enable Corosync tracing corosync-cmapctl totem.trace_buffer_size 1000000 # Monitor system resources top -b -H -d 2 vmstat 1 iostat -x 2 Network Diagnostics # Check network latency ping -i 3 -s 64 <target-host> # Trace routes traceroute -n <target-host> # Check bandwidth iperf -c 3 -t 10 <target-host> # Capture packets tcpdump -i any -w /tmp/capture.pcap host <target-ip> Best Practices for Troubleshooting Always start with debug enabled when investigating issues Collect logs before clearing them - keep a complete copy Document the issue - note exact error messages, timestamps Test in isolated environment if possible Check configuration drift - ensure all nodes have same configuration Monitor system resources - CPU, memory, disk I/O, network Use incremental debugging - narrow down the problem step by step Have rollback plan - know how to quickly revert changes Additional Resources Corosync Wiki : https://github.com/corosync/corosync/wiki/Troubleshooting Pacemaker Guide : https://www.clusterlabs.org/pacemaker/doc/2.0/Pacemaker_Explained/ Ceph Troubleshooting : https://docs.ceph.com/en/latest/rados/troubleshooting/ QEMU Debugging : https://wiki.qemu.org/Documentation/Debugging/ Linux Performance : https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/tuning_guide/","title":"Troubleshooting code behavior"},{"location":"troubleshooting-code-behavior/#troubleshooting-code-behavior-and-diagnostics","text":"This section provides comprehensive guidance on diagnosing and resolving issues with clustering, virtualization, and storage technologies from a code behavior perspective.","title":"Troubleshooting Code Behavior and Diagnostics"},{"location":"troubleshooting-code-behavior/#debugging-techniques","text":"","title":"Debugging Techniques"},{"location":"troubleshooting-code-behavior/#enable-debug-logging","text":"Corosync: # Enable debug mode in corosync.conf logging { to_logfile: yes logfile: /var/log/corosync/corosync.log debug: on debug_logfile: /var/log/corosync/debug.log } # Or use cormapctl corosync-cmapctl totem.debug = 1 corosync-cmapctl totem.logging_debug = 1 Pacemaker: # Enable verbose logging crm_simulate --live-check crm_simulate --show-failcounts crm_simulate --showscores # Check detailed resource actions pcs resource show-status # Monitor CIB changes crm_mon --show-detail crm_mon --show-node-attributes QEMU/KVM: # Enable QEMU debug output qemu-system-x86_64 -d in_asm,op,exec,cpu_reset,guest_errors,int,mmu # QMP debugging qemu-system-x86_64 -qmp stdio -monitor stdio # Enable KVM debug echo 1 > /sys/module/kvm/parameters/debug echo 1 > /sys/module/kvm_intel/parameters/debug echo 1 > /sys/module/kvm_amd/parameters/debug Ceph: # Enable debug logging ceph config set global debug_msgr 10 ceph config set mon debug_mon 10 # Enable object logging ceph config set global debug_filestore 20 ceph config set global debug_rados 20 # Monitor OSD operations ceph tell osd.0 dump_ops","title":"Enable Debug Logging"},{"location":"troubleshooting-code-behavior/#common-code-level-issues","text":"","title":"Common Code-Level Issues"},{"location":"troubleshooting-code-behavior/#corosync-issues","text":"1. Token Circulation Problems Symptoms: - Nodes unable to send messages - Token stuck on one node - Messages delayed excessively Causes: - Network partition - Token timeout too short - Network buffer overflow Diagnosis: corosync-cfgtool -s corosync-quorumtool -s Check token status: corosync-cmapctl runtime.totem.token Check ring state: corosync-cmapctl runtime.totem.token_hold_time Solution:","title":"Corosync Issues"},{"location":"troubleshooting-code-behavior/#increase-token-timeout","text":"totem { token: 10000 # Increase from default 10000 token_retransmit: 500 }","title":"Increase token timeout"},{"location":"troubleshooting-code-behavior/#reduce-network-traffic","text":"totem { window_size: 50 # Reduce from default 100 max_messages: 17 }","title":"Reduce network traffic"},{"location":"troubleshooting-code-behavior/#check-network-configuration","text":"ping -c 3 tcpdump -i any 'port 5405' **2. Quorum Failures** Symptoms: - Cluster loses quorum - Nodes operate independently - Resources stop responding Diagnosis: corosync-quorumtool -s Check expected vs actual votes Check node connectivity Solution: # Verify network connectivity ping -c 5 <all-cluster-nodes> # Check Corosync communication corosync-cfgtool -s # Adjust quorum settings nodelist { node { quorum_votes: 3 } } # Ensure proper node count # Use odd number of nodes for majority","title":"Check network configuration"},{"location":"troubleshooting-code-behavior/#pacemaker-issues","text":"1. Resource Start Failures Symptoms: - Resources stuck in \"Starting\" state - Resources fail to start repeatedly - CRM crashes or restarts Diagnosis: pcs status Check resource status: pcs resource show-status resource-name Check CRM logs: journalctl -u pacemaker -n 50 | less Check transition history: crm_history --show Solution:","title":"Pacemaker Issues"},{"location":"troubleshooting-code-behavior/#increase-operation-timeout","text":"pcs resource op add resource-name start \\ timeout=60s interval=0s on-fail=restart","title":"Increase operation timeout"},{"location":"troubleshooting-code-behavior/#check-resource-agent","text":"pcs resource debug-resource resource-name crm_resource --validate --resource resource-name","title":"Check resource agent"},{"location":"troubleshooting-code-behavior/#check-for-configuration-errors","text":"crm_verify -L -V **2. Constraint Violations** Symptoms: - Resources not placed according to constraints - Unexpected resource placement - Colocation not respected Diagnosis: pcs constraint show --full Check constraint scores: crm_simulate --showscores Verify constraint definitions: pcs constraint order list pcs constraint colocation list Solution: # Verify constraints are correct pcs constraint order list pcs constraint colocation list # Adjust constraint scores pcs constraint order set order-id \\ symmetrical=false # Clear broken constraints pcs constraint remove constraint-id # Recalculate cluster state pcs resource cleanup 3. Resource Agent Bugs Symptoms: - OCF agent returns invalid status codes - Agent timeout during start/stop - Agent crashes Diagnosis: # Test agent manually ocf_resource:ocf:heartbeat:IPaddr2 monitor # Check agent logs grep \"resource-name\" /var/log/pacemaker/pengine/* # Validate agent syntax crm_resource --validate --resource resource-name Solution:","title":"Check for configuration errors"},{"location":"troubleshooting-code-behavior/#test-agent-manually","text":"export OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat /usr/lib/ocf/resource.d/heartbeat/IPaddr2 monitor OCF_RESKEY=OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat","title":"Test agent manually"},{"location":"troubleshooting-code-behavior/#update-agent-operation-timeout","text":"pcs resource op add resource-name start \\ timeout=120s","title":"Update agent operation timeout"},{"location":"troubleshooting-code-behavior/#monitor-agent-behavior","text":"pcs resource op monitor resource-name \\ timeout=30s interval=10s","title":"Monitor agent behavior"},{"location":"troubleshooting-code-behavior/#use-standard-agents-if-possible","text":"","title":"Use standard agents if possible"},{"location":"troubleshooting-code-behavior/#ensure-ocf-compliant-agents","text":"#### QEMU/KVM Issues **1. VM Startup Failures** Symptoms: - VM fails to boot - Kernel panic in guest - No console output Diagnosis: qemu-system-x86_64 -d in_asm,cpu_reset","title":"Ensure OCF-compliant agents"},{"location":"troubleshooting-code-behavior/#check-vm-configuration","text":"virsh dumpxml vm-name","title":"Check VM configuration"},{"location":"troubleshooting-code-behavior/#monitor-qmp-events","text":"echo '{\"execute\":\"qmp_capabilities\"}' | \\ socat UNIX-CONNECT:/var/run/libvirt/qemu/vm-name.monitor Solution: # Simplify VM configuration virt-install --name vm1 --memory 1024 --vcpus 1 # Disable problematic features qemu-system-x86_64 -no-hpet -no-acpi # Use different machine type virt-install --name vm1 --machine pc # Check disk image integrity qemu-img check disk.qcow2 # Enable serial console qemu-system-x86_64 -serial pty -monitor stdio 2. Performance Issues Symptoms: - VM runs slowly - High CPU usage on host - Poor I/O performance Diagnosis: # Monitor host CPU top -H -p qemu-system-x86_64 # Check KVM configuration lsmod | grep kvm # Monitor VM internals virsh qemu-monitor-command vm-name \"info status\" # Check disk I/O iotop -o -a # Check network I/O iftop -i br0 Solution:","title":"Monitor QMP events"},{"location":"troubleshooting-code-behavior/#enable-kvm","text":"qemu-system-x86_64 -enable-kvm","title":"Enable KVM"},{"location":"troubleshooting-code-behavior/#use-virtio-drivers","text":"virt-install --disk bus=virtio --network model=virtio","title":"Use virtio drivers"},{"location":"troubleshooting-code-behavior/#enable-virtio-multiqueue","text":"virt-install --cpu host-passthrough,cache=writeback","title":"Enable virtio multiqueue"},{"location":"troubleshooting-code-behavior/#tune-scheduler","text":"echo 1 > /sys/module/kvm/parameters/lapic **3. Memory Issues** Symptoms: - Out of memory errors - Swap usage high - Memory leak in QEMU process Diagnosis:","title":"Tune scheduler"},{"location":"troubleshooting-code-behavior/#check-host-memory","text":"free -h","title":"Check host memory"},{"location":"troubleshooting-code-behavior/#check-vm-memory","text":"virsh dommemstat vm-name","title":"Check VM memory"},{"location":"troubleshooting-code-behavior/#monitor-kvm-memory","text":"cat /sys/module/kvm/parameters/hugepages","title":"Monitor KVM memory"},{"location":"troubleshooting-code-behavior/#check-qemu-memory","text":"ps -o pid,vsz,pmem -C qemu-system-x86_64","title":"Check QEMU memory"},{"location":"troubleshooting-code-behavior/#check-for-ballooning","text":"virsh dominfo vm-name | grep balloon Solution: # Enable memory ballooning virt-install --balloon virtio # Enable hugepages echo 1 > /sys/kernel/mm/hugepages/hugepages-2048kB echo 1024 > /proc/sys/vm/nr_hugepages # Tune memory allocation virt-install --memory 1024 --memballoc=interleave # Limit VM memory virsh setmem vm-name 512M # Disable unused features virt-install --no-usb --no-sound","title":"Check for ballooning"},{"location":"troubleshooting-code-behavior/#ceph-issues","text":"1. OSD Performance Problems Symptoms: - Slow I/O operations - High latency - OSD rebalancing constantly Diagnosis: ceph tell osd.* iostat ceph osd perf # Check OSD stats ceph osd dump_ops osd.0 # Check PG states ceph pg dump # Monitor recovery ceph -w Solution:","title":"Ceph Issues"},{"location":"troubleshooting-code-behavior/#tune-osd-configuration","text":"ceph config set osd osd_op_threads 2 ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3","title":"Tune OSD configuration"},{"location":"troubleshooting-code-behavior/#enable-bluestore","text":"ceph config set osd osd_objectstore bluestore","title":"Enable BlueStore"},{"location":"troubleshooting-code-behavior/#tune-memory","text":"ceph config set osd osd_memory_target 4G","title":"Tune memory"},{"location":"troubleshooting-code-behavior/#adjust-max-open-files","text":"ceph config set osd max_open_files 4096 **2. PG Distribution Issues** Symptoms: - CRUSH misplacement - Uneven data distribution - High latency on specific OSDs Diagnosis: ceph osd tree ceph pg dump","title":"Adjust max open files"},{"location":"troubleshooting-code-behavior/#check-crush-map","text":"ceph osd getcrushmap -o crush.map","title":"Check CRUSH map"},{"location":"troubleshooting-code-behavior/#analyze-pg-distribution","text":"ceph pg dump | grep pg_num","title":"Analyze PG distribution"},{"location":"troubleshooting-code-behavior/#check-osd-utilization","text":"ceph osd perf Solution: # Adjust PG count ceph osd pool set <pool-name> pg_num <new-pg-count> # Rebalance cluster ceph osd reweight-by-utilization # Check CRUSH rules ceph osd getcrushmap -o crush.map crushtool -i crush.map --test --num-rep 3 --rules <rule-id> # Adjust CRUSH tunables ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3 3. Network Partition Issues Symptoms: - OSDs marked down - Network timeouts - Split-brain scenario Diagnosis: ceph -s ceph quorum_status # Check network connectivity ping -c 3 <mon-host> # Monitor OSD status ceph osd tree # Check Corosync status corosync-cfgtool -s Solution:","title":"Check OSD utilization"},{"location":"troubleshooting-code-behavior/#adjust-network-timeouts","text":"totem { token: 10000 # Increase timeout join: 120 }","title":"Adjust network timeouts"},{"location":"troubleshooting-code-behavior/#enable-redundant-network","text":"totem { interface { ringnumber: 1 bindnetaddr: 192.168.2.10 mcastport: 5405 } }","title":"Enable redundant network"},{"location":"troubleshooting-code-behavior/#check-firewall","text":"iptables -L -n corosync","title":"Check firewall"},{"location":"troubleshooting-code-behavior/#use-dedicated-network","text":"","title":"Use dedicated network"},{"location":"troubleshooting-code-behavior/#separate-cluster-and-public-networks","text":"### Performance Analysis #### Profiling Techniques **Corosync:** ```bash # Profile with perf perf record -e cycles,instructions -g -o perf.data corosync # Analyze token circulation perf report -i perf.data -s token # Profile memory usage perf record -e cycles -g -o mem.data corosync Pacemaker: # Profile PE calculations time crm_simulate --simulate > /dev/null # Monitor PE frequency grep \"calculated transition\" /var/log/pacemaker/pengine/* QEMU: # Profile QEMU perf record -e cycles,instructions,cache-misses -g -o qemu.perf \\ qemu-system-x86_64 ... # Profile KVM operations perf record -e cycles,instructions,kvm:kvm_exit,kvm:kvm_entry -g -o kvm.perf \\ qemu-system-x86_64 -enable-kvm ... Ceph: # Profile OSD perf record -e cycles,instructions,cache-misses -g -o osd.perf \\ -p <osd-pid> # Profile network perf record -e cycles,instructions,kvm:kvm_exit -g -o network.perf \\ tcpdump -i any 'port 6800' # Profile I/O iostat -x 5 -d -c <rados-device>","title":"Separate cluster and public networks"},{"location":"troubleshooting-code-behavior/#memory-analysis","text":"Detecting Memory Leaks: # Monitor process memory watch -n 1 'ps -o pid,vsz,pmem -C corosync' # Check for memory growth ps -o pid,rss,vsz -C pacemaker \\ | awk '{print $2}' | sort -n # Check for unfreed objects valgrind --leak-check = full ./corosync # Analyze heap usage gdb -p corosync -batch -ex \"bt\"","title":"Memory Analysis"},{"location":"troubleshooting-code-behavior/#concurrency-issues","text":"Pacemaker: # Monitor concurrent operations crm_mon --show-detail # Check for deadlocks grep \"deadlock\" /var/log/pacemaker/* # Monitor transition queue crm_simulate --show-failcounts Ceph: # Check for lock contention ceph daemon --admin socket tell osd.0 dump_historic_ops # Monitor thread pool ceph tell osd.0 dump_ops_in_flight # Check recovery activity ceph -w","title":"Concurrency Issues"},{"location":"troubleshooting-code-behavior/#log-analysis","text":"","title":"Log Analysis"},{"location":"troubleshooting-code-behavior/#understanding-log-levels","text":"Corosync: - DEBUG : Detailed diagnostic information - INFO : General informational messages - WARN : Warning conditions - ERROR : Error conditions Pacemaker: - notice : Information about cluster events - warning : Potential issues - error : Error conditions - crit : Critical failures Ceph: - debug/10 : Detailed information (level 10) - debug/20 : More information - info : General messages - warn : Warning messages - err : Error messages","title":"Understanding Log Levels"},{"location":"troubleshooting-code-behavior/#log-locations","text":"Corosync: /var/log/corosync/corosync.log /var/log/corosync/debug.log /var/log/syslog Pacemaker: /var/log/pacemaker/pengine/* /var/log/pacemaker/crmd/* /var/log/pacemaker/attrd/* Ceph: /var/log/ceph/ceph.log /var/log/ceph/ceph-mon.log /var/log/ceph/ceph-osd.*.log","title":"Log Locations"},{"location":"troubleshooting-code-behavior/#log-analysis-commands","text":"Corosync: # Token analysis grep \"token\" /var/log/corosync/corosync.log # Quorum events grep \"quorum\" /var/log/corosync/corosync.log # Network issues grep \"network\" /var/log/corosync/corosync.log Pacemaker: # Find failed resources grep \"FAILED\" /var/log/pacemaker/pengine/* # Analyze transitions grep \"calculated transition\" /var/log/pacemaker/pengine/* # Check for errors grep \"error\" /var/log/pacemaker/* Ceph: # OSD crashes grep \"segfault\" /var/log/ceph/ceph-osd.*.log # Slow requests grep \"slow request\" /var/log/ceph/ceph-osd.*.log # Network issues grep \"timed out\" /var/log/ceph/ceph-osd.*.log","title":"Log Analysis Commands"},{"location":"troubleshooting-code-behavior/#configuration-problems","text":"","title":"Configuration Problems"},{"location":"troubleshooting-code-behavior/#validation-and-testing","text":"Before Applying Changes: # Validate Pacemaker configuration crm_verify -L -V # Test with simulation crm_simulate --live-check # Dry-run Ceph commands ceph tell osd.* injectargs --dry-run # Test iSCSI configuration gwcli.py target list Rollback Procedures: # Pacemaker snapshot cibadmin --query --local > cib-backup.xml # Rollback if needed cibadmin --replace --local --xml-file cib-backup.xml # Restore Ceph configuration ceph config set <option> <previous-value>","title":"Validation and Testing"},{"location":"troubleshooting-code-behavior/#integration-issues","text":"","title":"Integration Issues"},{"location":"troubleshooting-code-behavior/#cluster-coordination","text":"Corosync and Pacemaker: # Check Corosync status from Pacemaker crm_resource --list # Verify quorum crm_attribute -q -n quorum # Check node status pcs status nodes # Ensure Pacemaker can communicate crm_mon --show-detail Ceph with Cluster Managers: # Check if cluster manager sees Ceph ceph -s # Verify network connectivity ping <mon-host> # Check monitor status ceph -w","title":"Cluster Coordination"},{"location":"troubleshooting-code-behavior/#storage-integration","text":"iSCSI Issues: # Check gateway status gwcli.py target list # Check initiator connections iscsiadm -m session # Check multipath status multipath -ll # Reset initiator state iscsiadm -m node logout -T <target-iqn> -p <portal-ip> iscsiadm -m node login -T <target-iqn> -p <portal-ip> --login RBD Issues: # Check mapped images rbd showmapped # Check image status rbd info <pool>/<image> # Unmap and remap rbd unmap <pool>/<image> rbd map <pool>/<image> # Check for stale mappings dmesg | grep rbd","title":"Storage Integration"},{"location":"troubleshooting-code-behavior/#advanced-diagnostics","text":"","title":"Advanced Diagnostics"},{"location":"troubleshooting-code-behavior/#system-level-debugging","text":"# Enable kernel debugging echo 1 > /proc/sys/kernel/sched_sched_min_granularity echo 1 > /proc/sys/kernel/khung_task_timeout_secs # Enable KVM tracing echo 1 > /sys/module/kvm/parameters/trace_gue # Enable Corosync tracing corosync-cmapctl totem.trace_buffer_size 1000000 # Monitor system resources top -b -H -d 2 vmstat 1 iostat -x 2","title":"System-Level Debugging"},{"location":"troubleshooting-code-behavior/#network-diagnostics","text":"# Check network latency ping -i 3 -s 64 <target-host> # Trace routes traceroute -n <target-host> # Check bandwidth iperf -c 3 -t 10 <target-host> # Capture packets tcpdump -i any -w /tmp/capture.pcap host <target-ip>","title":"Network Diagnostics"},{"location":"troubleshooting-code-behavior/#best-practices-for-troubleshooting","text":"Always start with debug enabled when investigating issues Collect logs before clearing them - keep a complete copy Document the issue - note exact error messages, timestamps Test in isolated environment if possible Check configuration drift - ensure all nodes have same configuration Monitor system resources - CPU, memory, disk I/O, network Use incremental debugging - narrow down the problem step by step Have rollback plan - know how to quickly revert changes","title":"Best Practices for Troubleshooting"},{"location":"troubleshooting-code-behavior/#additional-resources","text":"Corosync Wiki : https://github.com/corosync/corosync/wiki/Troubleshooting Pacemaker Guide : https://www.clusterlabs.org/pacemaker/doc/2.0/Pacemaker_Explained/ Ceph Troubleshooting : https://docs.ceph.com/en/latest/rados/troubleshooting/ QEMU Debugging : https://wiki.qemu.org/Documentation/Debugging/ Linux Performance : https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/tuning_guide/","title":"Additional Resources"},{"location":"cluster/","text":"Complete reference guide for Corosync and Pacemaker cluster management technologies. Corosync Corosync is a Group Communication System providing virtual synchrony guarantees and quorum management for Linux HA clusters. Architecture Overview graph TB A[Applications] --> B[Corosync Cluster Engine] B --> C[TOTEM Protocol] B --> D[Quorum System] B --> E[Configuration DB] B --> F[Availability Manager] C --> G[Network Layer] Key Features Virtual Synchrony : Consistent message ordering across all nodes TOTEM Protocol : Reliable group communication Quorum Management : Split-brain prevention Configuration Database : In-memory key-value store Ring Topology : Efficient message delivery Quick Commands # Installation apt-get install corosync # Generate authentication key corosync-keygen # Status corosync-cfgtool -s corosync-quorumtool -s # Configuration corosync-cmapctl Deployment # 1. Install and configure apt-get install corosync corosync-keygen vim /etc/corosync/corosync.conf # 2. Start on all nodes systemctl enable corosync systemctl start corosync # 3. Verify cluster corosync-cfgtool -s Source Code Repository : corosync/corosync Documentation : corosync.github.io Common Issues Issue Solution Token timeout errors Increase token: 10000 in corosync.conf Quorum loss Check network connectivity and node count Authentication failures Verify authkey permissions (600) Pacemaker Pacemaker is an advanced cluster resource manager for Linux high availability. Architecture Overview graph TB A[CRM - Cluster Resource Manager] --> B[PE - Policy Engine] A --> C[TE - Transition Engine] A --> D[LRM - Local Resource Manager] A --> E[CIB - Cluster Info Base] B --> F[Calculate Transitions] C --> G[Execute Actions] D --> H[Monitor Resources] E --> I[Cluster State Database] Key Features Resource Classes : OCF, LSB, systemd, Upstart Constraints : Ordering, colocation, location STONITH : Fencing integration Cloning : Resource and node templates Batches : Group operations Quick Commands # Cluster status pcs status # Resource management pcs resource create vip ocf:heartbeat:IPaddr2 ip = 192 .168.1.100 pcs resource start vip # Constraints pcs constraint order start vip then apache pcs constraint colocation add apache with vip # STONITH pcs stonith create fence-device fence_ipmilan Deployment # 1. Install components apt-get install pacemaker pcs # 2. Configure cluster pcs cluster setup --name mycluster node1 node2 node3 # 3. Start cluster pcs cluster start --all # 4. Enable STONITH pcs property set stonith-enabled = true Source Code Repository : ClusterLabs/pacemaker Documentation : clusterlabs.org/pacemaker/doc/ Common Issues Issue Solution Resource stuck starting Check agent with pcs resource debug-resource Constraint violations Verify constraints with pcs constraint show --full STONITH failures Configure fencing devices properly Quorum issues Check network and node connectivity Deployment Workflow Complete Corosync and Pacemaker HA cluster setup. 1. Initial Setup # Install all packages apt-get install corosync pacemaker pcs # Generate Corosync key corosync-keygen # Distribute key scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ # Configure Corosync # Edit /etc/corosync/corosync.conf on first node # Copy configuration to all nodes scp /etc/corosync/corosync.conf node2:/etc/corosync/ scp /etc/corosync/corosync.conf node3:/etc/corosync/ # Start services systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd 2. Verify Cluster # Check Corosync corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker pcs status 3. Configure STONITH # Create fencing device pcs stonith create fence-device fence_ipmilan \\ ipaddr = 192 .168.1.200 login = admin passwd = password # Verify fencing pcs stonith list 4. Add Resources # Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip = 192 .168.1.100 cidr_netmask = 24 # Create Apache resource pcs resource create apache ocf:heartbeat:apache \\ configfile = /etc/apache2/apache2.conf # Add ordering constraint pcs constraint order start vip then apache # Add colocation constraint pcs constraint colocation add apache with vip # Start resources pcs resource start vip 5. Testing # Test resource failover pcs resource move apache node2 # Verify cluster health pcs status crm_mon Source Code References Component Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org/pacemaker/doc/ Nifty Behaviors Corosync Optimizations Kronosnet : Multi-link transport with automatic failover Token tuning : token: 5000 for faster failure detection Network buffers : window_size: 50 for better throughput Pacemaker Tips Resource stickiness : Keep resources on current node Batch operations : Apply multiple constraints together Symmetric monitoring : Enable pacemaker alerts Security Considerations Corosync Protect authentication key: chmod 600 /etc/corosync/authkey Use encrypted transport (knet) Isolate cluster network Pacemaker Always enable STONITH for production Limit cluster daemon permissions Secure cluster communication Troubleshooting For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment section.","title":"Overview"},{"location":"cluster/#corosync","text":"Corosync is a Group Communication System providing virtual synchrony guarantees and quorum management for Linux HA clusters.","title":"Corosync"},{"location":"cluster/#architecture-overview","text":"graph TB A[Applications] --> B[Corosync Cluster Engine] B --> C[TOTEM Protocol] B --> D[Quorum System] B --> E[Configuration DB] B --> F[Availability Manager] C --> G[Network Layer]","title":"Architecture Overview"},{"location":"cluster/#key-features","text":"Virtual Synchrony : Consistent message ordering across all nodes TOTEM Protocol : Reliable group communication Quorum Management : Split-brain prevention Configuration Database : In-memory key-value store Ring Topology : Efficient message delivery","title":"Key Features"},{"location":"cluster/#quick-commands","text":"# Installation apt-get install corosync # Generate authentication key corosync-keygen # Status corosync-cfgtool -s corosync-quorumtool -s # Configuration corosync-cmapctl","title":"Quick Commands"},{"location":"cluster/#deployment","text":"# 1. Install and configure apt-get install corosync corosync-keygen vim /etc/corosync/corosync.conf # 2. Start on all nodes systemctl enable corosync systemctl start corosync # 3. Verify cluster corosync-cfgtool -s","title":"Deployment"},{"location":"cluster/#source-code","text":"Repository : corosync/corosync Documentation : corosync.github.io","title":"Source Code"},{"location":"cluster/#common-issues","text":"Issue Solution Token timeout errors Increase token: 10000 in corosync.conf Quorum loss Check network connectivity and node count Authentication failures Verify authkey permissions (600)","title":"Common Issues"},{"location":"cluster/#pacemaker","text":"Pacemaker is an advanced cluster resource manager for Linux high availability.","title":"Pacemaker"},{"location":"cluster/#architecture-overview_1","text":"graph TB A[CRM - Cluster Resource Manager] --> B[PE - Policy Engine] A --> C[TE - Transition Engine] A --> D[LRM - Local Resource Manager] A --> E[CIB - Cluster Info Base] B --> F[Calculate Transitions] C --> G[Execute Actions] D --> H[Monitor Resources] E --> I[Cluster State Database]","title":"Architecture Overview"},{"location":"cluster/#key-features_1","text":"Resource Classes : OCF, LSB, systemd, Upstart Constraints : Ordering, colocation, location STONITH : Fencing integration Cloning : Resource and node templates Batches : Group operations","title":"Key Features"},{"location":"cluster/#quick-commands_1","text":"# Cluster status pcs status # Resource management pcs resource create vip ocf:heartbeat:IPaddr2 ip = 192 .168.1.100 pcs resource start vip # Constraints pcs constraint order start vip then apache pcs constraint colocation add apache with vip # STONITH pcs stonith create fence-device fence_ipmilan","title":"Quick Commands"},{"location":"cluster/#deployment_1","text":"# 1. Install components apt-get install pacemaker pcs # 2. Configure cluster pcs cluster setup --name mycluster node1 node2 node3 # 3. Start cluster pcs cluster start --all # 4. Enable STONITH pcs property set stonith-enabled = true","title":"Deployment"},{"location":"cluster/#source-code_1","text":"Repository : ClusterLabs/pacemaker Documentation : clusterlabs.org/pacemaker/doc/","title":"Source Code"},{"location":"cluster/#common-issues_1","text":"Issue Solution Resource stuck starting Check agent with pcs resource debug-resource Constraint violations Verify constraints with pcs constraint show --full STONITH failures Configure fencing devices properly Quorum issues Check network and node connectivity","title":"Common Issues"},{"location":"cluster/#deployment-workflow","text":"Complete Corosync and Pacemaker HA cluster setup.","title":"Deployment Workflow"},{"location":"cluster/#1-initial-setup","text":"# Install all packages apt-get install corosync pacemaker pcs # Generate Corosync key corosync-keygen # Distribute key scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ # Configure Corosync # Edit /etc/corosync/corosync.conf on first node # Copy configuration to all nodes scp /etc/corosync/corosync.conf node2:/etc/corosync/ scp /etc/corosync/corosync.conf node3:/etc/corosync/ # Start services systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd","title":"1. Initial Setup"},{"location":"cluster/#2-verify-cluster","text":"# Check Corosync corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker pcs status","title":"2. Verify Cluster"},{"location":"cluster/#3-configure-stonith","text":"# Create fencing device pcs stonith create fence-device fence_ipmilan \\ ipaddr = 192 .168.1.200 login = admin passwd = password # Verify fencing pcs stonith list","title":"3. Configure STONITH"},{"location":"cluster/#4-add-resources","text":"# Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip = 192 .168.1.100 cidr_netmask = 24 # Create Apache resource pcs resource create apache ocf:heartbeat:apache \\ configfile = /etc/apache2/apache2.conf # Add ordering constraint pcs constraint order start vip then apache # Add colocation constraint pcs constraint colocation add apache with vip # Start resources pcs resource start vip","title":"4. Add Resources"},{"location":"cluster/#5-testing","text":"# Test resource failover pcs resource move apache node2 # Verify cluster health pcs status crm_mon","title":"5. Testing"},{"location":"cluster/#source-code-references","text":"Component Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org/pacemaker/doc/","title":"Source Code References"},{"location":"cluster/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"cluster/#corosync-optimizations","text":"Kronosnet : Multi-link transport with automatic failover Token tuning : token: 5000 for faster failure detection Network buffers : window_size: 50 for better throughput","title":"Corosync Optimizations"},{"location":"cluster/#pacemaker-tips","text":"Resource stickiness : Keep resources on current node Batch operations : Apply multiple constraints together Symmetric monitoring : Enable pacemaker alerts","title":"Pacemaker Tips"},{"location":"cluster/#security-considerations","text":"","title":"Security Considerations"},{"location":"cluster/#corosync_1","text":"Protect authentication key: chmod 600 /etc/corosync/authkey Use encrypted transport (knet) Isolate cluster network","title":"Corosync"},{"location":"cluster/#pacemaker_1","text":"Always enable STONITH for production Limit cluster daemon permissions Secure cluster communication","title":"Pacemaker"},{"location":"cluster/#troubleshooting","text":"For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment section.","title":"Troubleshooting"},{"location":"cluster/corosync/","text":"Cluster messaging and synchronization framework providing virtual synchrony guarantees and quorum management. Architecture graph TB A[Application Layer] --> B[Corosync Cluster Engine] B --> C[TOTEM Protocol] B --> D[Quorum System] B --> E[Configuration DB] B --> F[Availability Manager] C --> G[Network Layer] D --> H[Ring Topology] Key Features Virtual Synchrony model for consistent message ordering TOTEM protocol for reliable group communication Quorum system to prevent split-brain scenarios In-memory configuration database Quick Commands # Installation apt-get install corosync corosync-keygen # Status corosync-cfgtool -s corosync-quorumtool -s # Configuration corosync-cmapctl Nifty Behaviors Kronosnet Multi-Link Transport totem { transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.10 } interface { ringnumber: 1 bindnetaddr: 192.168.2.10 } } Nifty : Automatic failover between network links, better performance Token Timeout Optimization totem { token: 5000 # Reduce for faster failure detection token_retransmit: 250 } Nifty : Faster cluster response to failures Security Protect authentication key: chmod 600 /etc/corosync/authkey Use encrypted transport (knet) Isolate cluster network Source Code Repository: https://github.com/corosync/corosync Documentation: https://corosync.github.io/corosync/","title":"Corosync"},{"location":"cluster/corosync/#architecture","text":"graph TB A[Application Layer] --> B[Corosync Cluster Engine] B --> C[TOTEM Protocol] B --> D[Quorum System] B --> E[Configuration DB] B --> F[Availability Manager] C --> G[Network Layer] D --> H[Ring Topology]","title":"Architecture"},{"location":"cluster/corosync/#key-features","text":"Virtual Synchrony model for consistent message ordering TOTEM protocol for reliable group communication Quorum system to prevent split-brain scenarios In-memory configuration database","title":"Key Features"},{"location":"cluster/corosync/#quick-commands","text":"# Installation apt-get install corosync corosync-keygen # Status corosync-cfgtool -s corosync-quorumtool -s # Configuration corosync-cmapctl","title":"Quick Commands"},{"location":"cluster/corosync/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"cluster/corosync/#kronosnet-multi-link-transport","text":"totem { transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.10 } interface { ringnumber: 1 bindnetaddr: 192.168.2.10 } } Nifty : Automatic failover between network links, better performance","title":"Kronosnet Multi-Link Transport"},{"location":"cluster/corosync/#token-timeout-optimization","text":"totem { token: 5000 # Reduce for faster failure detection token_retransmit: 250 } Nifty : Faster cluster response to failures","title":"Token Timeout Optimization"},{"location":"cluster/corosync/#security","text":"Protect authentication key: chmod 600 /etc/corosync/authkey Use encrypted transport (knet) Isolate cluster network","title":"Security"},{"location":"cluster/corosync/#source-code","text":"Repository: https://github.com/corosync/corosync Documentation: https://corosync.github.io/corosync/","title":"Source Code"},{"location":"cluster/pacemaker/","text":"Advanced cluster resource manager coordinating configuration, start-up, monitoring, and recovery of interrelated services. Architecture graph TB A[CRM - Cluster Resource Manager] --> B[PE - Policy Engine] A --> C[TE - Transition Engine] A --> D[LRM - Local Resource Manager] A --> E[CIB - Cluster Information Base] B --> F[Calculate Transitions] C --> G[Execute Actions] D --> H[Monitor Resources] E --> I[Cluster State Database] Key Features Detection and recovery of host- and application-level failures Support for multiple redundancy configurations Configurable quorum loss strategies Ordering of service starts and stops Colocation and location constraints Quick Commands # Cluster status pcs status # Resource management pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip = 192 .168.1.100 cidr_netmask = 24 pcs resource start vip # Constraints pcs constraint order start vip then apache pcs constraint colocation add apache with vip Nifty Behaviors Resource Stickiness <meta_attributes> <nvpair name= \"resource-stickiness\" value= \"100\" /> <nvpair name= \"migration-threshold\" value= \"3\" /> </meta_attributes> Nifty : Resources prefer current location but move after failures Batch Constraint Updates pcs constraint order start vip then apache pcs constraint colocation add apache with vip pcs constraint location web-group prefers node1 = 100 Nifty : Constraints applied as a set, atomic configuration Security Always enable STONITH Secure cluster communication Limit cluster daemon permissions Source Code Repository: https://github.com/ClusterLabs/pacemaker Documentation: https://www.clusterlabs.org/pacemaker/doc/","title":"Pacemaker"},{"location":"cluster/pacemaker/#architecture","text":"graph TB A[CRM - Cluster Resource Manager] --> B[PE - Policy Engine] A --> C[TE - Transition Engine] A --> D[LRM - Local Resource Manager] A --> E[CIB - Cluster Information Base] B --> F[Calculate Transitions] C --> G[Execute Actions] D --> H[Monitor Resources] E --> I[Cluster State Database]","title":"Architecture"},{"location":"cluster/pacemaker/#key-features","text":"Detection and recovery of host- and application-level failures Support for multiple redundancy configurations Configurable quorum loss strategies Ordering of service starts and stops Colocation and location constraints","title":"Key Features"},{"location":"cluster/pacemaker/#quick-commands","text":"# Cluster status pcs status # Resource management pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip = 192 .168.1.100 cidr_netmask = 24 pcs resource start vip # Constraints pcs constraint order start vip then apache pcs constraint colocation add apache with vip","title":"Quick Commands"},{"location":"cluster/pacemaker/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"cluster/pacemaker/#resource-stickiness","text":"<meta_attributes> <nvpair name= \"resource-stickiness\" value= \"100\" /> <nvpair name= \"migration-threshold\" value= \"3\" /> </meta_attributes> Nifty : Resources prefer current location but move after failures","title":"Resource Stickiness"},{"location":"cluster/pacemaker/#batch-constraint-updates","text":"pcs constraint order start vip then apache pcs constraint colocation add apache with vip pcs constraint location web-group prefers node1 = 100 Nifty : Constraints applied as a set, atomic configuration","title":"Batch Constraint Updates"},{"location":"cluster/pacemaker/#security","text":"Always enable STONITH Secure cluster communication Limit cluster daemon permissions","title":"Security"},{"location":"cluster/pacemaker/#source-code","text":"Repository: https://github.com/ClusterLabs/pacemaker Documentation: https://www.clusterlabs.org/pacemaker/doc/","title":"Source Code"},{"location":"network/libvirt-network/","text":"Network management for virtualization environments. Network Types graph TB subgraph \"libvirt Networks\" A[NAT Network] B[Bridged Network] C[Isolated Network] D[Direct Passthrough] end subgraph \"Backend\" E[Linux Bridge] F[Open vSwitch] G[MACvtap] end A --> E B --> F C --> F D --> F style A fill:#c8e6c9 style B fill:#bbdefb style C fill:#e1f5fe style D fill:#ff9800 style E fill:#ffecb3 style F fill:#a5d6a7 Quick Commands # Network management virsh net-list virsh net-define network.xml virsh net-start <network-name> # Bridge commands brctl show brctl addbr br0 brctl delbr br0","title":"libvirt Networking"},{"location":"network/libvirt-network/#network-types","text":"graph TB subgraph \"libvirt Networks\" A[NAT Network] B[Bridged Network] C[Isolated Network] D[Direct Passthrough] end subgraph \"Backend\" E[Linux Bridge] F[Open vSwitch] G[MACvtap] end A --> E B --> F C --> F D --> F style A fill:#c8e6c9 style B fill:#bbdefb style C fill:#e1f5fe style D fill:#ff9800 style E fill:#ffecb3 style F fill:#a5d6a7","title":"Network Types"},{"location":"network/libvirt-network/#quick-commands","text":"# Network management virsh net-list virsh net-define network.xml virsh net-start <network-name> # Bridge commands brctl show brctl addbr br0 brctl delbr br0","title":"Quick Commands"},{"location":"storage/","text":"Complete reference guide for CEPH, iSCSI, NVMe-oF, and GFS2 storage technologies. Topics CEPH - Distributed storage system iSCSI - SCSI gateway for Ceph NVMe-oF - NVMe over Fabrics gateway GFS2 - Global File System 2 Quick Reference CEPH Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image iSCSI Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets NVMe-oF Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems GFS2 Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes CEPH Distributed storage system providing object, block, and file storage in unified platform. Architecture graph TB A[Clients] --> B[RADOS] B --> C[MON Cluster] B --> D[OSD Daemons] B --> E[MDS Daemons] B --> F[RGW] C --> G[Monitor Map] D --> H[OSD Map] E --> I[MDS Map] Key Features Object storage with S3/Swift API Block device (RBD) with kernel support POSIX-compliant file system (CephFS) Erasure coding for data protection Automatic data rebalancing Quick Commands # Cluster status ceph -s ceph health # Storage management ceph osd pool create rbd 64 rbd create rbd/image1 --size 100G rbd map rbd/image1 # CephFS ceph fs new myfs metadata data mount -t ceph <mon-ip>:6789:/ /mnt/cephfs Source Code Repository : ceph/ceph Documentation : docs.ceph.com iSCSI with CEPH iSCSI gateway presenting RBD images as SCSI disks over TCP/IP network. Architecture graph TB A[iSCSI Initiator] --> B[LIO Target] B --> C[TCMU Backend] C --> D[RBD Library] D --> E[RADOS Cluster] A --> F[TCP/IP Network] Key Features LIO target framework for SCSI protocol TCMU userspace passthrough RBD backend for Ceph integration CHAP authentication support Multipath I/O support Quick Commands # Deploy gateway ceph orch apply iscsi gateway.yml # Target management gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1 # Initiator configuration iscsiadm -m discovery -t st -p <target-ip> iscsiadm -m node -T <target-iqn> -p <target-ip> --login Source Code Repository : ceph/ceph Documentation : docs.ceph.com/rbd/iscsi-overview/ NVMe-oF with CEPH NVMe over Fabrics gateway providing high-performance block access. Architecture graph TB A[NVMe Host] --> B[NVMe-oF Gateway] B --> C[SPDK/RBD] C --> D[RADOS Cluster] A --> E[TCP/IP or RDMA] Key Features NVMe/TCP protocol for block access SPDK integration for high performance HA with gateway groups Load balancing across gateways RDMA/RoCE support Quick Commands # Deploy gateway ceph orch apply nvmeof gateway.yml # Subsystem management gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 \\ --pool rbd --image disk1 # Initiator connection nvme connect -t tcp -n <subsystem-nqn> \\ -a <gateway-ip> -s 4420 Source Code Repository : ceph/ceph Documentation : docs.ceph.com/rbd/nvmeof-overview/ GFS2 Global File System 2 for shared-disk file system in Linux clusters. Architecture graph TB A[GFS2 Node 1] --> B[GFS2 Module] A --> C[DLM Lock Manager] A --> D[Shared Block Device] E[GFS2 Node 2] --> F[GFS2 Module] E --> G[DLM Lock Manager] E --> D D --> H[Lock Synchronization] Key Features POSIX-compliant file system Distributed lock manager (DLM) Cluster-wide volume management (CLVM) Journaling for metadata integrity Quota support Quick Commands # Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 # DLM management dlm_tool ls dlm_tool dump Source Code Location : fs/gfs2/ in Linux kernel Repository : torvalds/linux Documentation : /usr/share/doc/gfs2-utils/ Deployment Workflow 1. CEPH HCI Setup # Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm # Bootstrap cluster ./cephadm bootstrap --mon-ip <mon-ip> # Add storage ceph orch apply osd --all-available-devices # Verify ceph -s ceph status 2. iSCSI Gateway Setup # Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply iscsi gateway.yml # Create target gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1 3. NVMe-oF Gateway Setup # Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply nvmeof gateway.yml # Create subsystem gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 --pool rbd --image disk1 4. GFS2 Setup # Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 Security Considerations Enable CephX encryption Use CHAP authentication for iSCSI Use RDMA/RoCE for secure transport Implement proper network segmentation Source Code References Technology Repository Documentation CEPH ceph/ceph docs.ceph.com iSCSI ceph/ceph linux-iscsi.org NVMe-oF ceph/ceph nvmexpress.org GFS2 torvalds/linux gfs2-utils Troubleshooting For in-depth troubleshooting focused on code behavior and diagnostics, see Cluster Technologies section. Common Issues Issue Solution OSD high latency Check ceph tell osd.* iostat Cannot discover iSCSI Verify iscsiadm -m discovery NVMe connection fails Check gateway connectivity GFS2 stale locks Run dlm_tool dump to clear","title":"Overview"},{"location":"storage/#topics","text":"CEPH - Distributed storage system iSCSI - SCSI gateway for Ceph NVMe-oF - NVMe over Fabrics gateway GFS2 - Global File System 2","title":"Topics"},{"location":"storage/#quick-reference","text":"","title":"Quick Reference"},{"location":"storage/#ceph","text":"Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image","title":"CEPH"},{"location":"storage/#iscsi","text":"Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets","title":"iSCSI"},{"location":"storage/#nvme-of","text":"Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems","title":"NVMe-oF"},{"location":"storage/#gfs2","text":"Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes","title":"GFS2"},{"location":"storage/#ceph_1","text":"Distributed storage system providing object, block, and file storage in unified platform.","title":"CEPH"},{"location":"storage/#architecture","text":"graph TB A[Clients] --> B[RADOS] B --> C[MON Cluster] B --> D[OSD Daemons] B --> E[MDS Daemons] B --> F[RGW] C --> G[Monitor Map] D --> H[OSD Map] E --> I[MDS Map]","title":"Architecture"},{"location":"storage/#key-features","text":"Object storage with S3/Swift API Block device (RBD) with kernel support POSIX-compliant file system (CephFS) Erasure coding for data protection Automatic data rebalancing","title":"Key Features"},{"location":"storage/#quick-commands","text":"# Cluster status ceph -s ceph health # Storage management ceph osd pool create rbd 64 rbd create rbd/image1 --size 100G rbd map rbd/image1 # CephFS ceph fs new myfs metadata data mount -t ceph <mon-ip>:6789:/ /mnt/cephfs","title":"Quick Commands"},{"location":"storage/#source-code","text":"Repository : ceph/ceph Documentation : docs.ceph.com","title":"Source Code"},{"location":"storage/#iscsi-with-ceph","text":"iSCSI gateway presenting RBD images as SCSI disks over TCP/IP network.","title":"iSCSI with CEPH"},{"location":"storage/#architecture_1","text":"graph TB A[iSCSI Initiator] --> B[LIO Target] B --> C[TCMU Backend] C --> D[RBD Library] D --> E[RADOS Cluster] A --> F[TCP/IP Network]","title":"Architecture"},{"location":"storage/#key-features_1","text":"LIO target framework for SCSI protocol TCMU userspace passthrough RBD backend for Ceph integration CHAP authentication support Multipath I/O support","title":"Key Features"},{"location":"storage/#quick-commands_1","text":"# Deploy gateway ceph orch apply iscsi gateway.yml # Target management gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1 # Initiator configuration iscsiadm -m discovery -t st -p <target-ip> iscsiadm -m node -T <target-iqn> -p <target-ip> --login","title":"Quick Commands"},{"location":"storage/#source-code_1","text":"Repository : ceph/ceph Documentation : docs.ceph.com/rbd/iscsi-overview/","title":"Source Code"},{"location":"storage/#nvme-of-with-ceph","text":"NVMe over Fabrics gateway providing high-performance block access.","title":"NVMe-oF with CEPH"},{"location":"storage/#architecture_2","text":"graph TB A[NVMe Host] --> B[NVMe-oF Gateway] B --> C[SPDK/RBD] C --> D[RADOS Cluster] A --> E[TCP/IP or RDMA]","title":"Architecture"},{"location":"storage/#key-features_2","text":"NVMe/TCP protocol for block access SPDK integration for high performance HA with gateway groups Load balancing across gateways RDMA/RoCE support","title":"Key Features"},{"location":"storage/#quick-commands_2","text":"# Deploy gateway ceph orch apply nvmeof gateway.yml # Subsystem management gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 \\ --pool rbd --image disk1 # Initiator connection nvme connect -t tcp -n <subsystem-nqn> \\ -a <gateway-ip> -s 4420","title":"Quick Commands"},{"location":"storage/#source-code_2","text":"Repository : ceph/ceph Documentation : docs.ceph.com/rbd/nvmeof-overview/","title":"Source Code"},{"location":"storage/#gfs2_1","text":"Global File System 2 for shared-disk file system in Linux clusters.","title":"GFS2"},{"location":"storage/#architecture_3","text":"graph TB A[GFS2 Node 1] --> B[GFS2 Module] A --> C[DLM Lock Manager] A --> D[Shared Block Device] E[GFS2 Node 2] --> F[GFS2 Module] E --> G[DLM Lock Manager] E --> D D --> H[Lock Synchronization]","title":"Architecture"},{"location":"storage/#key-features_3","text":"POSIX-compliant file system Distributed lock manager (DLM) Cluster-wide volume management (CLVM) Journaling for metadata integrity Quota support","title":"Key Features"},{"location":"storage/#quick-commands_3","text":"# Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 # DLM management dlm_tool ls dlm_tool dump","title":"Quick Commands"},{"location":"storage/#source-code_3","text":"Location : fs/gfs2/ in Linux kernel Repository : torvalds/linux Documentation : /usr/share/doc/gfs2-utils/","title":"Source Code"},{"location":"storage/#deployment-workflow","text":"","title":"Deployment Workflow"},{"location":"storage/#1-ceph-hci-setup","text":"# Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm # Bootstrap cluster ./cephadm bootstrap --mon-ip <mon-ip> # Add storage ceph orch apply osd --all-available-devices # Verify ceph -s ceph status","title":"1. CEPH HCI Setup"},{"location":"storage/#2-iscsi-gateway-setup","text":"# Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply iscsi gateway.yml # Create target gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1","title":"2. iSCSI Gateway Setup"},{"location":"storage/#3-nvme-of-gateway-setup","text":"# Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply nvmeof gateway.yml # Create subsystem gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 --pool rbd --image disk1","title":"3. NVMe-oF Gateway Setup"},{"location":"storage/#4-gfs2-setup","text":"# Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2","title":"4. GFS2 Setup"},{"location":"storage/#security-considerations","text":"Enable CephX encryption Use CHAP authentication for iSCSI Use RDMA/RoCE for secure transport Implement proper network segmentation","title":"Security Considerations"},{"location":"storage/#source-code-references","text":"Technology Repository Documentation CEPH ceph/ceph docs.ceph.com iSCSI ceph/ceph linux-iscsi.org NVMe-oF ceph/ceph nvmexpress.org GFS2 torvalds/linux gfs2-utils","title":"Source Code References"},{"location":"storage/#troubleshooting","text":"For in-depth troubleshooting focused on code behavior and diagnostics, see Cluster Technologies section.","title":"Troubleshooting"},{"location":"storage/#common-issues","text":"Issue Solution OSD high latency Check ceph tell osd.* iostat Cannot discover iSCSI Verify iscsiadm -m discovery NVMe connection fails Check gateway connectivity GFS2 stale locks Run dlm_tool dump to clear","title":"Common Issues"},{"location":"storage/ceph/","text":"Distributed storage system providing object, block, and file storage in unified platform. Architecture graph TB A[Clients] --> B[LIBRADOS/LIBRBD/CEPHFS/RADOSGW] B --> C[MON Cluster] B --> D[OSD Daemons] B --> E[MGR Daemons] B --> F[MDS Daemons] D --> G[BlueStore/FileStore] G --> H[Storage Hardware] Key Features Object storage with S3/Swift API Block device (RBD) with kernel support POSIX-compliant file system (CephFS) Erasure coding for data protection Automatic data rebalancing Quick Commands # Cluster status ceph -s ceph health # Storage management ceph osd pool create rbd 64 rbd create rbd/image1 --size 100G rbd map rbd/image1 # CephFS ceph fs new myfs metadata data mount -t ceph <mon-ip>:6789:/ /mnt/cephfs Nifty Behaviors BlueStore Configuration # Enable BlueStore ceph config set osd osd_objectstore bluestore # Tune BlueStore ceph config set osd bluestore_max_blob_size 1M Nifty : Optimized storage engine for better performance Cache Tiering ceph osd tier add base-pool cache-pool ceph osd tier cache-mode cache-pool writeback ceph osd pool set cache-pool cache_target_dirty_ratio 0 .4 Nifty : Improve read performance with caching, ideally 4gb per drive Source Code Repository: https://github.com/ceph/ceph Documentation: https://docs.ceph.com/","title":"CEPH"},{"location":"storage/ceph/#architecture","text":"graph TB A[Clients] --> B[LIBRADOS/LIBRBD/CEPHFS/RADOSGW] B --> C[MON Cluster] B --> D[OSD Daemons] B --> E[MGR Daemons] B --> F[MDS Daemons] D --> G[BlueStore/FileStore] G --> H[Storage Hardware]","title":"Architecture"},{"location":"storage/ceph/#key-features","text":"Object storage with S3/Swift API Block device (RBD) with kernel support POSIX-compliant file system (CephFS) Erasure coding for data protection Automatic data rebalancing","title":"Key Features"},{"location":"storage/ceph/#quick-commands","text":"# Cluster status ceph -s ceph health # Storage management ceph osd pool create rbd 64 rbd create rbd/image1 --size 100G rbd map rbd/image1 # CephFS ceph fs new myfs metadata data mount -t ceph <mon-ip>:6789:/ /mnt/cephfs","title":"Quick Commands"},{"location":"storage/ceph/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"storage/ceph/#bluestore-configuration","text":"# Enable BlueStore ceph config set osd osd_objectstore bluestore # Tune BlueStore ceph config set osd bluestore_max_blob_size 1M Nifty : Optimized storage engine for better performance","title":"BlueStore Configuration"},{"location":"storage/ceph/#cache-tiering","text":"ceph osd tier add base-pool cache-pool ceph osd tier cache-mode cache-pool writeback ceph osd pool set cache-pool cache_target_dirty_ratio 0 .4 Nifty : Improve read performance with caching, ideally 4gb per drive","title":"Cache Tiering"},{"location":"storage/ceph/#source-code","text":"Repository: https://github.com/ceph/ceph Documentation: https://docs.ceph.com/","title":"Source Code"},{"location":"storage/gfs2/","text":"Global File System 2 for shared-disk file system in Linux clusters. Architecture graph TB A[GFS2 Node 1] --> B[GFS2 Module] A --> C[DLM Lock Manager] A --> D[Shared Block Device] E[GFS2 Node 2] --> F[GFS2 Module] E --> G[DLM Lock Manager] E --> D subgraph \"Shared Storage\" H[Physical Storage or DRBD] end subgraph \"Lock Synchronization\" I[Network Lock Exchange] end D --> H G --> H D --> I G --> I Key Features POSIX-compliant file system Distributed lock manager (DLM) Cluster-wide volume management (CLVM) Journaling for metadata integrity Quota support Quick Commands # Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster \\ -j 2 /dev/drbd/by-res/resource-data # Mount GFS2 mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 # DLM management dlm_tool ls dlm_tool dump Nifty Behaviors Quota Management gfs2_quota enable /mnt/gfs2 gfs2_quota limit -u <uid> 100G /mnt/gfs2 Nifty : Enforce user quotas on shared filesystem Snapshots with CLVM lvcreate -L 10G -s -n my_lv_snap mycluster_vg/my_lv lvdisplay mycluster_vg Nifty : Point-in-time snapshots with LVM Source Code Location in Linux kernel: fs/gfs2/ Repository: https://github.com/torvalds/linux/tree/master/fs/gfs2 Documentation: /usr/share/doc/gfs2-utils/","title":"GFS2"},{"location":"storage/gfs2/#architecture","text":"graph TB A[GFS2 Node 1] --> B[GFS2 Module] A --> C[DLM Lock Manager] A --> D[Shared Block Device] E[GFS2 Node 2] --> F[GFS2 Module] E --> G[DLM Lock Manager] E --> D subgraph \"Shared Storage\" H[Physical Storage or DRBD] end subgraph \"Lock Synchronization\" I[Network Lock Exchange] end D --> H G --> H D --> I G --> I","title":"Architecture"},{"location":"storage/gfs2/#key-features","text":"POSIX-compliant file system Distributed lock manager (DLM) Cluster-wide volume management (CLVM) Journaling for metadata integrity Quota support","title":"Key Features"},{"location":"storage/gfs2/#quick-commands","text":"# Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster \\ -j 2 /dev/drbd/by-res/resource-data # Mount GFS2 mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 # DLM management dlm_tool ls dlm_tool dump","title":"Quick Commands"},{"location":"storage/gfs2/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"storage/gfs2/#quota-management","text":"gfs2_quota enable /mnt/gfs2 gfs2_quota limit -u <uid> 100G /mnt/gfs2 Nifty : Enforce user quotas on shared filesystem","title":"Quota Management"},{"location":"storage/gfs2/#snapshots-with-clvm","text":"lvcreate -L 10G -s -n my_lv_snap mycluster_vg/my_lv lvdisplay mycluster_vg Nifty : Point-in-time snapshots with LVM","title":"Snapshots with CLVM"},{"location":"storage/gfs2/#source-code","text":"Location in Linux kernel: fs/gfs2/ Repository: https://github.com/torvalds/linux/tree/master/fs/gfs2 Documentation: /usr/share/doc/gfs2-utils/","title":"Source Code"},{"location":"storage/iscsi/","text":"iSCSI gateway presenting RBD images as SCSI disks over TCP/IP network. Architecture graph TB A[SCSI Initiator] --> B[LIO Target Framework] B --> C[TCMU Backend] C --> D[RBD Library] D --> E[RADOS Cluster] A --> F[TCP/IP Network] B --> F Key Features LIO target framework for SCSI protocol TCMU userspace passthrough RBD backend for Ceph integration CHAP authentication support Multipath I/O support Quick Commands # Create RBD image rbd create rbd/disk1 --size 100G # Configure iSCSI gateway ceph orch apply iscsi gateway.yml # Target management gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 \\ --pool rbd --image disk1 # Initiator discovery iscsiadm -m discovery -t st -p <target-ip> iscsiadm -m node -T <target-iqn> -p <target-ip> --login Nifty Behaviors CHAP Authentication # Configure CHAP on gateway gwcli.py target set_chap <target-iqn> \\ --user <username> --password <password> Nifty : Secure iSCSI connections HA Gateway Groups # Create gateway group gwcli.py group create <group-name> gwcli.py group add_gateway <group-name> <gateway-host> Nifty : Redundant iSCSI gateways Source Code Repository: https://github.com/ceph/ceph Documentation: https://docs.ceph.com/en/latest/rbd/iscsi-overview/","title":"iSCSI"},{"location":"storage/iscsi/#architecture","text":"graph TB A[SCSI Initiator] --> B[LIO Target Framework] B --> C[TCMU Backend] C --> D[RBD Library] D --> E[RADOS Cluster] A --> F[TCP/IP Network] B --> F","title":"Architecture"},{"location":"storage/iscsi/#key-features","text":"LIO target framework for SCSI protocol TCMU userspace passthrough RBD backend for Ceph integration CHAP authentication support Multipath I/O support","title":"Key Features"},{"location":"storage/iscsi/#quick-commands","text":"# Create RBD image rbd create rbd/disk1 --size 100G # Configure iSCSI gateway ceph orch apply iscsi gateway.yml # Target management gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 \\ --pool rbd --image disk1 # Initiator discovery iscsiadm -m discovery -t st -p <target-ip> iscsiadm -m node -T <target-iqn> -p <target-ip> --login","title":"Quick Commands"},{"location":"storage/iscsi/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"storage/iscsi/#chap-authentication","text":"# Configure CHAP on gateway gwcli.py target set_chap <target-iqn> \\ --user <username> --password <password> Nifty : Secure iSCSI connections","title":"CHAP Authentication"},{"location":"storage/iscsi/#ha-gateway-groups","text":"# Create gateway group gwcli.py group create <group-name> gwcli.py group add_gateway <group-name> <gateway-host> Nifty : Redundant iSCSI gateways","title":"HA Gateway Groups"},{"location":"storage/iscsi/#source-code","text":"Repository: https://github.com/ceph/ceph Documentation: https://docs.ceph.com/en/latest/rbd/iscsi-overview/","title":"Source Code"},{"location":"storage/nvme-of/","text":"NVMe over Fabrics gateway providing high-performance block access to Ceph. Architecture graph TB A[NVMe Host] --> B[NVMe-oF Gateway] B --> C[SPDK/RBD Integration] C --> D[RADOS Cluster] A --> E[TCP/IP or RDMA] B --> E[HA Gateway Group] Key Features NVMe/TCP protocol for block access SPDK integration for high performance HA with gateway groups Load balancing across gateways RDMA/RoCE support Quick Commands # Create subsystem gwcli.py subsystem create <subsystem-nqn> # Create namespace gwcli.py namespace create <subsystem-nqn> 1 \\ --pool rbd --image disk1 # Configure gateway ceph orch apply nvmeof gateway.yml # Connect initiator nvme connect -t tcp -n <subsystem-nqn> \\ -a <gateway-ip> -s 4420 Nifty Behaviors CHAP Authentication # Configure CHAP on gateway gwcli.py target set_chap <target-iqn> \\ --user <username> --password <password> Nifty : Secure iSCSI connections HA Gateway Groups # Create gateway group gwcli.py group create <group-name> gwcli.py group add_gateway <group-name> <gateway-host> Nifty : Redundant iSCSI/NVMe-oF gateways Source Code Repository: https://github.com/ceph/ceph Documentation: https://docs.ceph.com/en/latest/rbd/nvmeof-overview/","title":"NVMe-oF"},{"location":"storage/nvme-of/#architecture","text":"graph TB A[NVMe Host] --> B[NVMe-oF Gateway] B --> C[SPDK/RBD Integration] C --> D[RADOS Cluster] A --> E[TCP/IP or RDMA] B --> E[HA Gateway Group]","title":"Architecture"},{"location":"storage/nvme-of/#key-features","text":"NVMe/TCP protocol for block access SPDK integration for high performance HA with gateway groups Load balancing across gateways RDMA/RoCE support","title":"Key Features"},{"location":"storage/nvme-of/#quick-commands","text":"# Create subsystem gwcli.py subsystem create <subsystem-nqn> # Create namespace gwcli.py namespace create <subsystem-nqn> 1 \\ --pool rbd --image disk1 # Configure gateway ceph orch apply nvmeof gateway.yml # Connect initiator nvme connect -t tcp -n <subsystem-nqn> \\ -a <gateway-ip> -s 4420","title":"Quick Commands"},{"location":"storage/nvme-of/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"storage/nvme-of/#chap-authentication","text":"# Configure CHAP on gateway gwcli.py target set_chap <target-iqn> \\ --user <username> --password <password> Nifty : Secure iSCSI connections","title":"CHAP Authentication"},{"location":"storage/nvme-of/#ha-gateway-groups","text":"# Create gateway group gwcli.py group create <group-name> gwcli.py group add_gateway <group-name> <gateway-host> Nifty : Redundant iSCSI/NVMe-oF gateways","title":"HA Gateway Groups"},{"location":"storage/nvme-of/#source-code","text":"Repository: https://github.com/ceph/ceph Documentation: https://docs.ceph.com/en/latest/rbd/nvmeof-overview/","title":"Source Code"},{"location":"virtualization/","text":"Complete reference guide for QEMU, KVM, libvirt, virsh, and Open vSwitch virtualization technologies. QEMU Generic and open source machine emulator providing full-system and user-mode emulation. Architecture graph TB A[QEMU Process] --> B[VCPU Threads] A --> C[IO Thread] A --> D[Timer Thread] B --> E[TCG Translator] B --> F[KVM Acceleration] E --> G[Dynamic Binary Translation] F --> H[VM Exit/Entry] Key Features Full-system emulation (x86, ARM, PowerPC, etc.) User-mode emulation TCG dynamic translation KVM integration for hardware acceleration Device emulation (network, storage, graphics) Quick Commands # Create VM qemu-system-x86_64 -name vm1 -m 2048 -smp 2 \\ -drive file = disk.qcow2,format = qcow2 \\ -netdev user,id = net0,hostfwd = tcp::2222-:22 # Enable KVM qemu-system-x86_64 -enable-kvm -name vm1 -m 4096 -smp 4 # Disk management qemu-img create -f qcow2 disk.qcow2 20G qemu-img info disk.qcow2 qemu-img resize disk.qcow2 +10G Source Code Repository : qemu-project/qemu Documentation : qemu.org KVM Kernel-based virtual machine providing hardware-assisted virtualization. Architecture graph TB A[KVM Module] --> B[VMX/SVM Support] A --> C[KVM Kernel Modules] B --> D[QEMU Userspace] C --> E[Virtual Machine Monitor] E --> F[Guest VM] Key Features Hardware virtualization (Intel VT-x, AMD-V) Near-native performance EPT/NPT memory management Virtio I/O virtualization Live migration support Quick Commands # Check support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo # Load modules modprobe kvm-intel # Intel modprobe kvm-amd # AMD # Verify kvm-ok Source Code Location : virt/kvm/ in Linux kernel Repository : torvalds/linux Documentation : linux-kvm.org libvirt Virtualization API and daemon for managing virtualization capabilities. Architecture graph TB A[libvirtd Daemon] --> B[QEMU/KVM] A --> C[LXC Containers] A --> D[Xen] E[Management Tools] --> F[virsh CLI] E --> G[virt-manager GUI] F --> A G --> A Key Features Unified API for multiple hypervisors XML-based VM configuration Network and storage management Live migration support Secure API with authentication Quick Commands # List VMs virsh list --all # VM operations virsh start vm1 virsh shutdown vm1 virsh destroy vm1 virsh dumpxml vm1 # Network management virsh net-list virsh net-define network.xml virsh net-start default Source Code Repository : libvirt/libvirt Documentation : libvirt.org virsh Command-line interface for managing libvirt virtualization. Quick Commands # VM Management virsh list # List running VMs virsh list --all # List all VMs virsh start <vm-name> # Start VM virsh shutdown <vm-name> # Shutdown VM virsh destroy <vm-name> # Force stop VM virsh undefine <vm-name> # Remove VM # VM Information virsh dominfo <vm-name> # Domain info virsh dumpxml <vm-name> # XML config virsh vcpuinfo <vm-name> # VCPU info virsh dommemstat <vm-name> # Memory stats # Snapshot Management virsh snapshot-create <vm-name> <snap-name> virsh snapshot-list <vm-name> virsh snapshot-revert <vm-name> <snap-name> # Console Access virsh console <vm-name> virsh qemu-monitor-command <vm-name> \"info status\" Open vSwitch Open source multilayer virtual switch for managing VM networking. Architecture graph TB A[VM1 eth0] --> B[OVS Bridge br0] C[VM2 eth0] --> B D[VM3 eth0] --> B B --> E[OVS Database] E --> F[Controller] G[Host eth0] --> H[OVS Bridge br0] I[Management Tools] --> E Key Features Standard Linux bridge compatibility Flow-based switching VLAN isolation Bonding and trunking support Remote management protocol Quick Commands # Bridge Management ovs-vsctl add-br br0 ovs-vsctl del-br br0 ovs-vsctl list-br # Port Management ovs-vsctl add-port br0 eth0 ovs-vsctl del-port br0 eth0 ovs-vsctl list-ports br0 # Configuration ovs-vsctl show ovs-ofctl show br0 ovs-vsctls show Source Code Repository : openvswitch/ovs Documentation : openvswitch.org Deployment Workflow 1. Hardware Verification # Check virtualization support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo 2. Software Installation # Install packages apt-get install qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch # Enable services systemctl enable libvirtd systemctl start libvirtd systemctl enable openvswitch systemctl start openvswitch # Verify virsh version ovs-vsctl show 3. Network Setup # Create OVS bridge ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 eth1 4. Create VM # Using virt-install virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path = /var/lib/libvirt/images/vm1.qcow2,size = 20 \\ --network bridge = br0 # Or using virsh virsh define vm1.xml virsh start vm1 Troubleshooting For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment . Common Issues Issue Solution VM slow performance Enable KVM with -enable-kvm Network issues Check OVS bridges with ovs-vsctl show Storage issues Use virtio drivers for better I/O Resource constraints Adjust memory/CPU allocation Security Considerations Use AppArmor/SELinux profiles Use virtio drivers for better isolation Enable seccomp filters Use IOMMU for device passthrough Source Code References Technology Repository Documentation QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org Open vSwitch openvswitch/ovs openvswitch.org","title":"Overview"},{"location":"virtualization/#qemu","text":"Generic and open source machine emulator providing full-system and user-mode emulation.","title":"QEMU"},{"location":"virtualization/#architecture","text":"graph TB A[QEMU Process] --> B[VCPU Threads] A --> C[IO Thread] A --> D[Timer Thread] B --> E[TCG Translator] B --> F[KVM Acceleration] E --> G[Dynamic Binary Translation] F --> H[VM Exit/Entry]","title":"Architecture"},{"location":"virtualization/#key-features","text":"Full-system emulation (x86, ARM, PowerPC, etc.) User-mode emulation TCG dynamic translation KVM integration for hardware acceleration Device emulation (network, storage, graphics)","title":"Key Features"},{"location":"virtualization/#quick-commands","text":"# Create VM qemu-system-x86_64 -name vm1 -m 2048 -smp 2 \\ -drive file = disk.qcow2,format = qcow2 \\ -netdev user,id = net0,hostfwd = tcp::2222-:22 # Enable KVM qemu-system-x86_64 -enable-kvm -name vm1 -m 4096 -smp 4 # Disk management qemu-img create -f qcow2 disk.qcow2 20G qemu-img info disk.qcow2 qemu-img resize disk.qcow2 +10G","title":"Quick Commands"},{"location":"virtualization/#source-code","text":"Repository : qemu-project/qemu Documentation : qemu.org","title":"Source Code"},{"location":"virtualization/#kvm","text":"Kernel-based virtual machine providing hardware-assisted virtualization.","title":"KVM"},{"location":"virtualization/#architecture_1","text":"graph TB A[KVM Module] --> B[VMX/SVM Support] A --> C[KVM Kernel Modules] B --> D[QEMU Userspace] C --> E[Virtual Machine Monitor] E --> F[Guest VM]","title":"Architecture"},{"location":"virtualization/#key-features_1","text":"Hardware virtualization (Intel VT-x, AMD-V) Near-native performance EPT/NPT memory management Virtio I/O virtualization Live migration support","title":"Key Features"},{"location":"virtualization/#quick-commands_1","text":"# Check support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo # Load modules modprobe kvm-intel # Intel modprobe kvm-amd # AMD # Verify kvm-ok","title":"Quick Commands"},{"location":"virtualization/#source-code_1","text":"Location : virt/kvm/ in Linux kernel Repository : torvalds/linux Documentation : linux-kvm.org","title":"Source Code"},{"location":"virtualization/#libvirt","text":"Virtualization API and daemon for managing virtualization capabilities.","title":"libvirt"},{"location":"virtualization/#architecture_2","text":"graph TB A[libvirtd Daemon] --> B[QEMU/KVM] A --> C[LXC Containers] A --> D[Xen] E[Management Tools] --> F[virsh CLI] E --> G[virt-manager GUI] F --> A G --> A","title":"Architecture"},{"location":"virtualization/#key-features_2","text":"Unified API for multiple hypervisors XML-based VM configuration Network and storage management Live migration support Secure API with authentication","title":"Key Features"},{"location":"virtualization/#quick-commands_2","text":"# List VMs virsh list --all # VM operations virsh start vm1 virsh shutdown vm1 virsh destroy vm1 virsh dumpxml vm1 # Network management virsh net-list virsh net-define network.xml virsh net-start default","title":"Quick Commands"},{"location":"virtualization/#source-code_2","text":"Repository : libvirt/libvirt Documentation : libvirt.org","title":"Source Code"},{"location":"virtualization/#virsh","text":"Command-line interface for managing libvirt virtualization.","title":"virsh"},{"location":"virtualization/#quick-commands_3","text":"# VM Management virsh list # List running VMs virsh list --all # List all VMs virsh start <vm-name> # Start VM virsh shutdown <vm-name> # Shutdown VM virsh destroy <vm-name> # Force stop VM virsh undefine <vm-name> # Remove VM # VM Information virsh dominfo <vm-name> # Domain info virsh dumpxml <vm-name> # XML config virsh vcpuinfo <vm-name> # VCPU info virsh dommemstat <vm-name> # Memory stats # Snapshot Management virsh snapshot-create <vm-name> <snap-name> virsh snapshot-list <vm-name> virsh snapshot-revert <vm-name> <snap-name> # Console Access virsh console <vm-name> virsh qemu-monitor-command <vm-name> \"info status\"","title":"Quick Commands"},{"location":"virtualization/#open-vswitch","text":"Open source multilayer virtual switch for managing VM networking.","title":"Open vSwitch"},{"location":"virtualization/#architecture_3","text":"graph TB A[VM1 eth0] --> B[OVS Bridge br0] C[VM2 eth0] --> B D[VM3 eth0] --> B B --> E[OVS Database] E --> F[Controller] G[Host eth0] --> H[OVS Bridge br0] I[Management Tools] --> E","title":"Architecture"},{"location":"virtualization/#key-features_3","text":"Standard Linux bridge compatibility Flow-based switching VLAN isolation Bonding and trunking support Remote management protocol","title":"Key Features"},{"location":"virtualization/#quick-commands_4","text":"# Bridge Management ovs-vsctl add-br br0 ovs-vsctl del-br br0 ovs-vsctl list-br # Port Management ovs-vsctl add-port br0 eth0 ovs-vsctl del-port br0 eth0 ovs-vsctl list-ports br0 # Configuration ovs-vsctl show ovs-ofctl show br0 ovs-vsctls show","title":"Quick Commands"},{"location":"virtualization/#source-code_3","text":"Repository : openvswitch/ovs Documentation : openvswitch.org","title":"Source Code"},{"location":"virtualization/#deployment-workflow","text":"","title":"Deployment Workflow"},{"location":"virtualization/#1-hardware-verification","text":"# Check virtualization support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo","title":"1. Hardware Verification"},{"location":"virtualization/#2-software-installation","text":"# Install packages apt-get install qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch # Enable services systemctl enable libvirtd systemctl start libvirtd systemctl enable openvswitch systemctl start openvswitch # Verify virsh version ovs-vsctl show","title":"2. Software Installation"},{"location":"virtualization/#3-network-setup","text":"# Create OVS bridge ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 eth1","title":"3. Network Setup"},{"location":"virtualization/#4-create-vm","text":"# Using virt-install virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path = /var/lib/libvirt/images/vm1.qcow2,size = 20 \\ --network bridge = br0 # Or using virsh virsh define vm1.xml virsh start vm1","title":"4. Create VM"},{"location":"virtualization/#troubleshooting","text":"For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment .","title":"Troubleshooting"},{"location":"virtualization/#common-issues","text":"Issue Solution VM slow performance Enable KVM with -enable-kvm Network issues Check OVS bridges with ovs-vsctl show Storage issues Use virtio drivers for better I/O Resource constraints Adjust memory/CPU allocation","title":"Common Issues"},{"location":"virtualization/#security-considerations","text":"Use AppArmor/SELinux profiles Use virtio drivers for better isolation Enable seccomp filters Use IOMMU for device passthrough","title":"Security Considerations"},{"location":"virtualization/#source-code-references","text":"Technology Repository Documentation QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org Open vSwitch openvswitch/ovs openvswitch.org","title":"Source Code References"},{"location":"virtualization/kvm/","text":"Kernel-based virtual machine for Linux providing hardware-assisted virtualization. Architecture graph TB A[KVM Module] --> B[VMX/SVM Support] A --> C[KVM Kernel Modules] B --> D[QEMU Userspace] C --> E[Virtual Machine Monitor] E --> F[Guest VM] Key Features Hardware virtualization extensions (Intel VT-x, AMD-V) Near-native performance for guest code Memory management with EPT/NPT I/O virtualization with virtio Live migration support Source Code Location in Linux kernel: virt/kvm/ Repository: https://github.com/torvalds/linux/tree/master/virt/kvm Documentation: https://www.linux-kvm.org/","title":"KVM"},{"location":"virtualization/kvm/#architecture","text":"graph TB A[KVM Module] --> B[VMX/SVM Support] A --> C[KVM Kernel Modules] B --> D[QEMU Userspace] C --> E[Virtual Machine Monitor] E --> F[Guest VM]","title":"Architecture"},{"location":"virtualization/kvm/#key-features","text":"Hardware virtualization extensions (Intel VT-x, AMD-V) Near-native performance for guest code Memory management with EPT/NPT I/O virtualization with virtio Live migration support","title":"Key Features"},{"location":"virtualization/kvm/#source-code","text":"Location in Linux kernel: virt/kvm/ Repository: https://github.com/torvalds/linux/tree/master/virt/kvm Documentation: https://www.linux-kvm.org/","title":"Source Code"},{"location":"virtualization/libvirt/","text":"Virtualization API and daemon for managing virtualization capabilities. Architecture graph TB A[libvirtd Daemon] --> B[QEMU/KVM] A --> C[LXC Containers] A --> D[Xen] E[Management Tools] --> F[virsh CLI] E --> G[virt-manager GUI] E --> H[API Bindings] F --> A G --> A H --> A Key Features Unified API for multiple hypervisors XML-based VM configuration Network and storage management Secure API with authentication Live migration support Quick Commands # List VMs virsh list --all # VM operations virsh start vm1 virsh shutdown vm1 virsh dumpxml vm1 # Network management virsh net-list virsh net-define network.xml Source Code Repository: https://gitlab.com/libvirt/libvirt Documentation: https://libvirt.org/docs/","title":"libvirt"},{"location":"virtualization/libvirt/#architecture","text":"graph TB A[libvirtd Daemon] --> B[QEMU/KVM] A --> C[LXC Containers] A --> D[Xen] E[Management Tools] --> F[virsh CLI] E --> G[virt-manager GUI] E --> H[API Bindings] F --> A G --> A H --> A","title":"Architecture"},{"location":"virtualization/libvirt/#key-features","text":"Unified API for multiple hypervisors XML-based VM configuration Network and storage management Secure API with authentication Live migration support","title":"Key Features"},{"location":"virtualization/libvirt/#quick-commands","text":"# List VMs virsh list --all # VM operations virsh start vm1 virsh shutdown vm1 virsh dumpxml vm1 # Network management virsh net-list virsh net-define network.xml","title":"Quick Commands"},{"location":"virtualization/libvirt/#source-code","text":"Repository: https://gitlab.com/libvirt/libvirt Documentation: https://libvirt.org/docs/","title":"Source Code"},{"location":"virtualization/ovs/","text":"Open source multilayer virtual switch for managing VM networking. Architecture graph TB A[VM1 eth0] --> B[OVS Bridge br0] A[VM2 eth0] --> B A[VM3 eth0] --> B B --> C[OVS Database] B --> D[Controller] E[Host eth0] --> F[OVS Bridge br0] F --> B G[Host eth1] --> H[OVS Bridge br1] H --> B I[Management Tools] --> B style A fill:#e1f5ff style B fill:#c8e6c9 style E fill:#e1f5ff style G fill:#e1f5ff style I fill:#ffecb3 Key Features Standard Linux bridge compatibility Flow-based switching Network isolation with VLANs Bonding and trunking support Remote management protocol Quick Commands # Bridge Management ovs-vsctl add-br br0 ovs-vsctl del-br br0 ovs-vsctl list-br # Port Management ovs-vsctl add-port br0 eth0 ovs-vsctl del-port br0 eth0 ovs-vsctl list-ports br0 # Show Configuration ovs-vsctl show ovs-ofctl show br0 # Database Management ovs-vsctls show Nifty Behaviors Remote Management ovs-vsctl set-manager ptcp:127.0.0.1:6640 ovs-vsctl set-protocol ssl Nifty : Secure remote OVS management Bond Configuration ovs-vsctl add-bond bond0 eth0 eth1 ovs-vsctl set bond bond0 mode = active-backup Nifty : Network redundancy and load balancing Source Code Repository: https://github.com/openvswitch/ovs Documentation: https://docs.openvswitch.org/","title":"Open vSwitch"},{"location":"virtualization/ovs/#architecture","text":"graph TB A[VM1 eth0] --> B[OVS Bridge br0] A[VM2 eth0] --> B A[VM3 eth0] --> B B --> C[OVS Database] B --> D[Controller] E[Host eth0] --> F[OVS Bridge br0] F --> B G[Host eth1] --> H[OVS Bridge br1] H --> B I[Management Tools] --> B style A fill:#e1f5ff style B fill:#c8e6c9 style E fill:#e1f5ff style G fill:#e1f5ff style I fill:#ffecb3","title":"Architecture"},{"location":"virtualization/ovs/#key-features","text":"Standard Linux bridge compatibility Flow-based switching Network isolation with VLANs Bonding and trunking support Remote management protocol","title":"Key Features"},{"location":"virtualization/ovs/#quick-commands","text":"# Bridge Management ovs-vsctl add-br br0 ovs-vsctl del-br br0 ovs-vsctl list-br # Port Management ovs-vsctl add-port br0 eth0 ovs-vsctl del-port br0 eth0 ovs-vsctl list-ports br0 # Show Configuration ovs-vsctl show ovs-ofctl show br0 # Database Management ovs-vsctls show","title":"Quick Commands"},{"location":"virtualization/ovs/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"virtualization/ovs/#remote-management","text":"ovs-vsctl set-manager ptcp:127.0.0.1:6640 ovs-vsctl set-protocol ssl Nifty : Secure remote OVS management","title":"Remote Management"},{"location":"virtualization/ovs/#bond-configuration","text":"ovs-vsctl add-bond bond0 eth0 eth1 ovs-vsctl set bond bond0 mode = active-backup Nifty : Network redundancy and load balancing","title":"Bond Configuration"},{"location":"virtualization/ovs/#source-code","text":"Repository: https://github.com/openvswitch/ovs Documentation: https://docs.openvswitch.org/","title":"Source Code"},{"location":"virtualization/qemu/","text":"Generic and open source machine emulator and virtualizer providing full-system and user-mode emulation. Architecture graph TB A[QEMU Process] --> B[VCPU Threads] A --> C[IO Thread] A --> D[Timer Thread] B --> E[TCG Translator] B --> F[KVM Acceleration] E --> G[Dynamic Binary Translation] F --> H[VM Exit/Entry] Key Features Full-system emulation of various architectures User-mode emulation for cross-platform binaries TCG dynamic translation for performance KVM integration for hardware acceleration Device emulation (network, storage, graphics) Quick Commands # Create VM qemu-system-x86_64 -name vm1 -m 2048 -smp 2 \\ -drive file = disk.qcow2,format = qcow2 \\ -netdev user,id = net0,hostfwd = tcp::2222-:22 # Enable KVM qemu-system-x86_64 -enable-kvm -name vm1 -m 4096 -smp 4 Nifty Behaviors QMP Programmatic Control echo '{\"execute\":\"query-status\"}' | \\ socat UNIX-CONNECT:/var/run/libvirt/qemu/vm1.monitor Nifty : Programmatic control of running VMs Virtio Multiqueue <interface type= 'network' > <model type= 'virtio' /> <driver name= 'vhost' queues= '4' /> </interface> Nifty : Multi-queue network, better throughput Memory Ballooning # Attach balloon device virsh attach-device vm1 balloon.xml # Adjust memory virsh setmem vm1 2G Nifty : Dynamically adjust VM memory Source Code Repository: https://gitlab.com/qemu-project/qemu Documentation: https://www.qemu.org/documentation/","title":"QEMU"},{"location":"virtualization/qemu/#architecture","text":"graph TB A[QEMU Process] --> B[VCPU Threads] A --> C[IO Thread] A --> D[Timer Thread] B --> E[TCG Translator] B --> F[KVM Acceleration] E --> G[Dynamic Binary Translation] F --> H[VM Exit/Entry]","title":"Architecture"},{"location":"virtualization/qemu/#key-features","text":"Full-system emulation of various architectures User-mode emulation for cross-platform binaries TCG dynamic translation for performance KVM integration for hardware acceleration Device emulation (network, storage, graphics)","title":"Key Features"},{"location":"virtualization/qemu/#quick-commands","text":"# Create VM qemu-system-x86_64 -name vm1 -m 2048 -smp 2 \\ -drive file = disk.qcow2,format = qcow2 \\ -netdev user,id = net0,hostfwd = tcp::2222-:22 # Enable KVM qemu-system-x86_64 -enable-kvm -name vm1 -m 4096 -smp 4","title":"Quick Commands"},{"location":"virtualization/qemu/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"virtualization/qemu/#qmp-programmatic-control","text":"echo '{\"execute\":\"query-status\"}' | \\ socat UNIX-CONNECT:/var/run/libvirt/qemu/vm1.monitor Nifty : Programmatic control of running VMs","title":"QMP Programmatic Control"},{"location":"virtualization/qemu/#virtio-multiqueue","text":"<interface type= 'network' > <model type= 'virtio' /> <driver name= 'vhost' queues= '4' /> </interface> Nifty : Multi-queue network, better throughput","title":"Virtio Multiqueue"},{"location":"virtualization/qemu/#memory-ballooning","text":"# Attach balloon device virsh attach-device vm1 balloon.xml # Adjust memory virsh setmem vm1 2G Nifty : Dynamically adjust VM memory","title":"Memory Ballooning"},{"location":"virtualization/qemu/#source-code","text":"Repository: https://gitlab.com/qemu-project/qemu Documentation: https://www.qemu.org/documentation/","title":"Source Code"},{"location":"virtualization/virsh/","text":"Command-line interface for managing libvirt virtualization. Quick Commands # VM Management virsh list # List running VMs virsh list --all # List all VMs virsh start <vm-name> # Start VM virsh shutdown <vm-name> # Shutdown VM virsh destroy <vm-name> # Force stop VM virsh undefine <vm-name> # Remove VM # VM Information virsh dominfo <vm-name> # Domain information virsh dumpxml <vm-name> # XML configuration virsh vcpuinfo <vm-name> # VCPU information virsh dommemstat <vm-name> # Memory statistics # Snapshot Management virsh snapshot-create <vm-name> <snap-name> virsh snapshot-list <vm-name> virsh snapshot-revert <vm-name> <snap-name> # Console Access virsh console <vm-name> virsh qemu-monitor-command <vm-name> \"info status\"","title":"virsh"},{"location":"virtualization/virsh/#quick-commands","text":"# VM Management virsh list # List running VMs virsh list --all # List all VMs virsh start <vm-name> # Start VM virsh shutdown <vm-name> # Shutdown VM virsh destroy <vm-name> # Force stop VM virsh undefine <vm-name> # Remove VM # VM Information virsh dominfo <vm-name> # Domain information virsh dumpxml <vm-name> # XML configuration virsh vcpuinfo <vm-name> # VCPU information virsh dommemstat <vm-name> # Memory statistics # Snapshot Management virsh snapshot-create <vm-name> <snap-name> virsh snapshot-list <vm-name> virsh snapshot-revert <vm-name> <snap-name> # Console Access virsh console <vm-name> virsh qemu-monitor-command <vm-name> \"info status\"","title":"Quick Commands"}]}