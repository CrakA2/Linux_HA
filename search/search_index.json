{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Comprehensive course materials for Linux High Availability (HA) clustering and virtualization technologies including Corosync, Pacemaker, QEMU, KVM, CEPH, iSCSI, FC/NVMe-oF, GFS2, libvirt, virsh, and Open vSwitch. Table of Contents Core Technologies Cluster Technologies - Corosync, Pacemaker Virtualization - QEMU, KVM, libvirt, virsh, Open vSwitch Storage and File Systems - CEPH, iSCSI, NVMe-oF, GFS2 Reference Quick Reference - Command cheatsheets Deployment Workflows - Setup guides Troubleshooting - Code behavior diagnostics Additional Network and Switching - libvirt networking Resources - External links License - License information Quick Start # Clone repository git clone https://github.com/CrakA2/Linux_HA.git cd Linux_HA # View locally mkdocs serve # Open http://127.0.0.1:8000 Topics Covered Cluster Management Corosync messaging and quorum Pacemaker resource management STONITH and high availability configuration Cluster deployment and configuration Virtualization QEMU/KVM hypervisor setup libvirt VM management Open vSwitch networking Live migration and resource optimization Distributed Storage CEPH HCI deployment RBD block storage CephFS shared filesystem iSCSI/NVMe-oF gateways File Systems GFS2 clustered filesystem DLM lock management CLVM volume management Documentation Structure docs/ \u251c\u2500\u2500 cluster/ # Corosync, Pacemaker \u251c\u2500\u2500 virtualization/ # QEMU, KVM, libvirt, virsh, OVS \u251c\u2500\u2500 storage/ # CEPH, iSCSI, NVMe-oF, GFS2 \u251c\u2500\u2500 network/ # libvirt networking \u251c\u2500\u2500 reference.md # Quick command reference \u251c\u2500\u2500 deployment.md # Deployment guides \u2514\u2500\u2500 troubleshooting-code-behavior.md # Diagnostics Source Code References Technology Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org CEPH ceph/ceph docs.ceph.com Open vSwitch openvswitch/ovs openvswitch.org GFS2 torvalds/linux gfs2-utils Contributing This is a personal knowledge base for Linux HA and virtualization technologies. Corrections and additions are welcome via pull requests to the GitHub repository.","title":"Home"},{"location":"#table-of-contents","text":"","title":"Table of Contents"},{"location":"#core-technologies","text":"Cluster Technologies - Corosync, Pacemaker Virtualization - QEMU, KVM, libvirt, virsh, Open vSwitch Storage and File Systems - CEPH, iSCSI, NVMe-oF, GFS2","title":"Core Technologies"},{"location":"#reference","text":"Quick Reference - Command cheatsheets Deployment Workflows - Setup guides Troubleshooting - Code behavior diagnostics","title":"Reference"},{"location":"#additional","text":"Network and Switching - libvirt networking Resources - External links License - License information","title":"Additional"},{"location":"#quick-start","text":"# Clone repository git clone https://github.com/CrakA2/Linux_HA.git cd Linux_HA # View locally mkdocs serve # Open http://127.0.0.1:8000","title":"Quick Start"},{"location":"#topics-covered","text":"","title":"Topics Covered"},{"location":"#cluster-management","text":"Corosync messaging and quorum Pacemaker resource management STONITH and high availability configuration Cluster deployment and configuration","title":"Cluster Management"},{"location":"#virtualization","text":"QEMU/KVM hypervisor setup libvirt VM management Open vSwitch networking Live migration and resource optimization","title":"Virtualization"},{"location":"#distributed-storage","text":"CEPH HCI deployment RBD block storage CephFS shared filesystem iSCSI/NVMe-oF gateways","title":"Distributed Storage"},{"location":"#file-systems","text":"GFS2 clustered filesystem DLM lock management CLVM volume management","title":"File Systems"},{"location":"#documentation-structure","text":"docs/ \u251c\u2500\u2500 cluster/ # Corosync, Pacemaker \u251c\u2500\u2500 virtualization/ # QEMU, KVM, libvirt, virsh, OVS \u251c\u2500\u2500 storage/ # CEPH, iSCSI, NVMe-oF, GFS2 \u251c\u2500\u2500 network/ # libvirt networking \u251c\u2500\u2500 reference.md # Quick command reference \u251c\u2500\u2500 deployment.md # Deployment guides \u2514\u2500\u2500 troubleshooting-code-behavior.md # Diagnostics","title":"Documentation Structure"},{"location":"#source-code-references","text":"Technology Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org CEPH ceph/ceph docs.ceph.com Open vSwitch openvswitch/ovs openvswitch.org GFS2 torvalds/linux gfs2-utils","title":"Source Code References"},{"location":"#contributing","text":"This is a personal knowledge base for Linux HA and virtualization technologies. Corrections and additions are welcome via pull requests to the GitHub repository.","title":"Contributing"},{"location":"deployment/","text":"Comprehensive production deployment guides for Linux HA clusters, virtualization, and storage infrastructure. HA Cluster Deployment Prerequisites Hardware Requirements : - Minimum 3 nodes for production - 2x CPU cores per node minimum - 4GB RAM per node minimum - 2x network interfaces per node (public/cluster) - Dedicated storage for cluster data Network Requirements : - Public network: 192.168.1.0/24 - Cluster network: 10.0.1.0/24 - DNS resolution configured - NTP synchronized - Firewall rules configured 1. Base System Setup # Update system apt-get update && apt-get upgrade -y # Install required packages apt-get install -y corosync pacemaker pcs \\ openvswitch-switch ceph-common # Configure timezone timedatectl set-timezone UTC # Configure NTP apt-get install -y chrony systemctl enable chrony systemctl start chrony # Configure hostname hostnamectl set-hostname node1 echo \"192.168.1.10 node1\" >> /etc/hosts echo \"192.168.1.11 node2\" >> /etc/hosts echo \"192.168.1.12 node3\" >> /etc/hosts 2. Corosync Configuration # Generate authentication key corosync-keygen # Configure Corosync cat > /etc/corosync/corosync.conf <<EOF totem { version: 2 cluster_name: ha-cluster transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } interface { ringnumber: 1 bindnetaddr: 10.0.1.0 mcastport: 5406 } token: 5000 token_retransmit: 250 hold: 180 } nodelist { node { ring0_addr: 192.168.1.10 ring1_addr: 10.0.1.10 nodeid: 1 name: node1 } node { ring0_addr: 192.168.1.11 ring1_addr: 10.0.1.11 nodeid: 2 name: node2 } node { ring0_addr: 192.168.1.12 ring1_addr: 10.0.1.12 nodeid: 3 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 auto_tie_breaker: 1 last_man_standing: 1 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes debug: off } EOF # Distribute configuration and key scp /etc/corosync/corosync.conf node2:/etc/corosync/ scp /etc/corosync/corosync.conf node3:/etc/corosync/ scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ 3. Start Corosync # Enable and start Corosync on all nodes systemctl enable corosync systemctl start corosync # Verify cluster membership corosync-cfgtool -s # Verify quorum corosync-quorumtool -s 4. Pacemaker Configuration # Start Pacemaker systemctl enable pacemaker systemctl start pacemaker # Start pcsd systemctl enable pcsd systemctl start pcsd # Set hacluster password echo \"hacluster:ClusterPass123!\" | chpasswd # Configure cluster pcs cluster setup --name ha-cluster node1 node2 node3 pcs cluster start --all pcs cluster enable --all # Verify cluster pcs status 5. STONITH Configuration # Create IPMI fencing device pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 pcs stonith create fence-ipmi-node2 \\ fence_ipmilan \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 pcs stonith create fence-ipmi-node3 \\ fence_ipmilan \\ ipaddr=192.168.1.203 \\ login=admin \\ passwd=AdminPass123 # Enable STONITH pcs property set stonith-enabled=true # Set STONITH timeout pcs property set stonith-timeout=60s # Verify STONITH pcs stonith show 6. Resource Configuration # Create OVS bridge resource pcs resource create ovs-bridge \\ ocf:heartbeat:ovs-bridge \\ bridge_name=br0 # Create VIP resource pcs resource create vip \\ ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 \\ cidr_netmask=24 # Create Apache resource pcs resource create apache \\ ocf:heartbeat:apache \\ configfile=/etc/apache2/apache2.conf # Add constraints pcs constraint order start ovs-bridge then vip pcs constraint order start vip then apache pcs constraint colocation add vip with ovs-bridge pcs constraint colocation add apache with vip # Start resources pcs resource start ovs-bridge Virtualization Deployment Prerequisites Hardware Requirements : - Intel VT-x or AMD-V CPU - 8GB RAM minimum - 100GB storage minimum - 2x network interfaces Software Requirements : - Linux kernel 4.15+ - KVM support - libvirt installed 1. KVM/QEMU Installation # Install virtualization packages apt-get install -y qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch-switch # Verify KVM support lscpu | grep -i virtualization # Start libvirt systemctl enable libvirtd systemctl start libvirtd # Start OVS systemctl enable openvswitch-switch systemctl start openvswitch-switch 2. Network Configuration # Create OVS bridge ovs-vsctl add-br br0 # Add physical interface to bridge ovs-vsctl add-port br0 eth0 # Configure IP on bridge ip addr add 192.168.1.10/24 dev br0 ip link set br0 up # Create virtual network virsh net-define <(cat <<EOF <network> <name>default</name> <forward mode='nat'/> <bridge name='virbr0' stp='on' delay='0'/> <ip address='192.168.122.1' netmask='255.255.255.0'> <dhcp> <range start='192.168.122.2' end='192.168.122.254'/> </dhcp> </ip> </network> EOF ) # Start network virsh net-start default virsh net-autostart default 3. VM Deployment # Create VM storage pool virsh pool-define <(cat <<EOF <pool type='dir'> <name>vm-images</name> <target> <path>/var/lib/libvirt/images</path> </target> </pool> EOF ) # Build and start pool virsh pool-build vm-images virsh pool-start vm-images virsh pool-autostart vm-images # Create VM disk qemu-img create -f qcow2 /var/lib/libvirt/images/vm1.qcow2 20G # Define VM virt-install --name vm1 \\ --memory 2048 \\ --vcpus 2 \\ --disk path=/var/lib/libvirt/images/vm1.qcow2,format=qcow2 \\ --network network=default \\ --graphics vnc \\ --os-variant ubuntu20.04 \\ --cdrom /path/to/ubuntu.iso # Start VM virsh start vm1 CEPH HCI Deployment Prerequisites Hardware Requirements : - Minimum 3 OSD nodes - Dedicated storage drives per OSD - 1GB RAM per OSD - Network: 10Gbps recommended Network Requirements : - Public network: 192.168.1.0/24 - Cluster network: 10.0.1.0/24 - DNS resolution configured 1. CEPH Installation # Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm mv cephadm /usr/local/bin/ # Install podman or docker apt-get install -y podman # Bootstrap cluster cephadm bootstrap --mon-ip 192.168.1.10 # Copy SSH keys ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.1.11 ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.1.12 # Add hosts to cluster ceph orch host add node1 192.168.1.10 ceph orch host add node2 192.168.1.11 ceph orch host add node3 192.168.1.12 2. OSD Configuration # Create OSD specification cat > osd.yml <<EOF service_type: osd service_id: osd placement: hosts: - node1 - node2 - node3 spec: data_devices: all: true EOF # Apply OSD specification ceph orch apply -i osd.yml # Monitor OSD status ceph osd status ceph -s 3. Pool Creation # Create RBD pool ceph osd pool create rbd 64 64 ceph osd pool application enable rbd rbd # Create RBD image rbd create rbd/vm1 --size 50G --image-format 2 # Create CephFS ceph osd pool create cephfs_metadata 64 64 ceph osd pool create cephfs_data 128 128 ceph fs new myfs cephfs_metadata cephfs_data Full Stack Deployment Architecture graph TB subgraph \"Cluster Layer\" A[Corosync] B[Pacemaker] C[STONITH] end subgraph \"Storage Layer\" D[CEPH Cluster] E[RBD Images] F[CephFS] end subgraph \"Network Layer\" G[OVS] H[Network] I[VIP] end subgraph \"Virtualization Layer\" J[QEMU/KVM] K[libvirt] L[VMs] end A --> B B --> C B --> G B --> I D --> E D --> F G --> H H --> I K --> L L --> E L --> F B --> K style A fill:#c8e6c9 style D fill:#ffecb3 style G fill:#e1f5ff style J fill:#fff3e0 Deployment Steps Phase 1: Infrastructure # 1. Configure network # Public network ip addr add 192.168.1.10/24 dev eth0 ip link set eth0 up # Cluster network ip addr add 10.0.1.10/24 dev eth1 ip link set eth1 up # 2. Configure DNS echo \"nameserver 192.168.1.1\" > /etc/resolv.conf # 3. Configure NTP systemctl enable chrony systemctl start chrony # 4. Configure firewall firewall-cmd --permanent --add-port=6789/tcp firewall-cmd --permanent --add-port=6800-7300/tcp firewall-cmd --permanent --add-port=5405-5406/udp firewall-cmd --reload Phase 2: CEPH Setup # Bootstrap Ceph cephadm bootstrap --mon-ip 192.168.1.10 # Add OSDs ceph orch host add node1 192.168.1.10 ceph orch host add node2 192.168.1.11 ceph orch host add node3 192.168.1.12 ceph orch apply osd --all-available-devices # Create pools ceph osd pool create rbd 64 64 ceph osd pool application enable rbd rbd rbd create rbd/vm1 --size 50G Phase 3: Cluster Setup # Generate Corosync key corosync-keygen # Configure Corosync cat > /etc/corosync/corosync.conf <<EOF totem { version: 2 cluster_name: fullstack transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } } nodelist { node { ring0_addr: 192.168.1.10 nodeid: 1 name: node1 } node { ring0_addr: 192.168.1.11 nodeid: 2 name: node2 } node { ring0_addr: 192.168.1.12 nodeid: 3 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log } EOF # Start cluster systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd # Configure Pacemaker pcs cluster setup --name fullstack node1 node2 node3 pcs cluster start --all Phase 4: Virtualization Setup # Install virtualization apt-get install -y qemu-kvm libvirt-daemon-system \\ libvirt-clients openvswitch-switch # Start services systemctl enable libvirtd systemctl start libvirtd # Configure OVS ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 # Create VM from RBD rbd create rbd/vm1 --size 50G virt-install --name vm1 \\ --memory 4096 \\ --vcpus 2 \\ --disk path=rbd:rbd/vm1,format=raw \\ --network bridge=br0 \\ --graphics vnc Phase 5: Resource Configuration # Create OVS resources pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge bridge_name=br0 # Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 cidr_netmask=24 # Create VM resource pcs resource create vm1 systemd:libvirtd # Add constraints pcs constraint order start ovs-bridge then vip pcs constraint order start vip then vm1 pcs constraint colocation add vip with ovs-bridge pcs constraint colocation add vm1 with vip # Start resources pcs resource start ovs-bridge Verification and Testing Cluster Verification # Check Corosync corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker pcs status # Check OVS ovs-vsctl show # Check Ceph ceph -s ceph osd status Failover Testing # Test resource failover pcs resource move vm1 node2 # Verify failover pcs status # Test node failover systemctl stop pacemaker # Check cluster status pcs status Performance Testing # Test I/O performance fio --name=randread --ioengine=libaio \\ --iodepth=16 --rw=randread --bs=4k \\ --direct=1 --size=1G --numjobs=4 \\ --filename=/mnt/cephfs/test # Test network performance iperf3 -c 192.168.1.100 -t 60 # Test VM performance virt-top Production Checklist Pre-Deployment [ ] Hardware meets minimum requirements [ ] Network configured and tested [ ] DNS resolution working [ ] NTP synchronized [ ] Firewall rules configured [ ] Storage devices identified [ ] Backup strategy planned [ ] Monitoring system ready Deployment [ ] All nodes have consistent configuration [ ] Corosync quorum established [ ] Pacemaker cluster operational [ ] STONITH devices configured [ ] CEPH cluster healthy [ ] Virtualization configured [ ] Resources defined and started [ ] Constraints configured Post-Deployment [ ] Cluster status verified [ ] Failover tested [ ] Performance tested [ ] Monitoring configured [ ] Alerting configured [ ] Documentation updated [ ] Backup procedures tested [ ] Team trained Troubleshooting Cluster Issues # Check Corosync logs journalctl -u corosync -f # Check Pacemaker logs journalctl -u pacemaker -f # Check cluster status pcs status crm_mon # Check quorum corosync-quorumtool -s Storage Issues # Check Ceph status ceph -s # Check OSD status ceph osd status # Check pool status ceph osd lspools # Check logs ceph -w Network Issues # Check OVS ovs-vsctl show # Check network ip addr show ip route show # Check connectivity ping node1 ping node2 ping node3 Best Practices Plan thoroughly before deployment Test in staging first Document everything Monitor continuously Test failover regularly Keep backups current Review logs frequently Update regularly Train team members Have recovery plan ready","title":"Deployment"},{"location":"deployment/#ha-cluster-deployment","text":"","title":"HA Cluster Deployment"},{"location":"deployment/#prerequisites","text":"Hardware Requirements : - Minimum 3 nodes for production - 2x CPU cores per node minimum - 4GB RAM per node minimum - 2x network interfaces per node (public/cluster) - Dedicated storage for cluster data Network Requirements : - Public network: 192.168.1.0/24 - Cluster network: 10.0.1.0/24 - DNS resolution configured - NTP synchronized - Firewall rules configured","title":"Prerequisites"},{"location":"deployment/#1-base-system-setup","text":"# Update system apt-get update && apt-get upgrade -y # Install required packages apt-get install -y corosync pacemaker pcs \\ openvswitch-switch ceph-common # Configure timezone timedatectl set-timezone UTC # Configure NTP apt-get install -y chrony systemctl enable chrony systemctl start chrony # Configure hostname hostnamectl set-hostname node1 echo \"192.168.1.10 node1\" >> /etc/hosts echo \"192.168.1.11 node2\" >> /etc/hosts echo \"192.168.1.12 node3\" >> /etc/hosts","title":"1. Base System Setup"},{"location":"deployment/#2-corosync-configuration","text":"# Generate authentication key corosync-keygen # Configure Corosync cat > /etc/corosync/corosync.conf <<EOF totem { version: 2 cluster_name: ha-cluster transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } interface { ringnumber: 1 bindnetaddr: 10.0.1.0 mcastport: 5406 } token: 5000 token_retransmit: 250 hold: 180 } nodelist { node { ring0_addr: 192.168.1.10 ring1_addr: 10.0.1.10 nodeid: 1 name: node1 } node { ring0_addr: 192.168.1.11 ring1_addr: 10.0.1.11 nodeid: 2 name: node2 } node { ring0_addr: 192.168.1.12 ring1_addr: 10.0.1.12 nodeid: 3 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 auto_tie_breaker: 1 last_man_standing: 1 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes debug: off } EOF # Distribute configuration and key scp /etc/corosync/corosync.conf node2:/etc/corosync/ scp /etc/corosync/corosync.conf node3:/etc/corosync/ scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/","title":"2. Corosync Configuration"},{"location":"deployment/#3-start-corosync","text":"# Enable and start Corosync on all nodes systemctl enable corosync systemctl start corosync # Verify cluster membership corosync-cfgtool -s # Verify quorum corosync-quorumtool -s","title":"3. Start Corosync"},{"location":"deployment/#4-pacemaker-configuration","text":"# Start Pacemaker systemctl enable pacemaker systemctl start pacemaker # Start pcsd systemctl enable pcsd systemctl start pcsd # Set hacluster password echo \"hacluster:ClusterPass123!\" | chpasswd # Configure cluster pcs cluster setup --name ha-cluster node1 node2 node3 pcs cluster start --all pcs cluster enable --all # Verify cluster pcs status","title":"4. Pacemaker Configuration"},{"location":"deployment/#5-stonith-configuration","text":"# Create IPMI fencing device pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 pcs stonith create fence-ipmi-node2 \\ fence_ipmilan \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 pcs stonith create fence-ipmi-node3 \\ fence_ipmilan \\ ipaddr=192.168.1.203 \\ login=admin \\ passwd=AdminPass123 # Enable STONITH pcs property set stonith-enabled=true # Set STONITH timeout pcs property set stonith-timeout=60s # Verify STONITH pcs stonith show","title":"5. STONITH Configuration"},{"location":"deployment/#6-resource-configuration","text":"# Create OVS bridge resource pcs resource create ovs-bridge \\ ocf:heartbeat:ovs-bridge \\ bridge_name=br0 # Create VIP resource pcs resource create vip \\ ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 \\ cidr_netmask=24 # Create Apache resource pcs resource create apache \\ ocf:heartbeat:apache \\ configfile=/etc/apache2/apache2.conf # Add constraints pcs constraint order start ovs-bridge then vip pcs constraint order start vip then apache pcs constraint colocation add vip with ovs-bridge pcs constraint colocation add apache with vip # Start resources pcs resource start ovs-bridge","title":"6. Resource Configuration"},{"location":"deployment/#virtualization-deployment","text":"","title":"Virtualization Deployment"},{"location":"deployment/#prerequisites_1","text":"Hardware Requirements : - Intel VT-x or AMD-V CPU - 8GB RAM minimum - 100GB storage minimum - 2x network interfaces Software Requirements : - Linux kernel 4.15+ - KVM support - libvirt installed","title":"Prerequisites"},{"location":"deployment/#1-kvmqemu-installation","text":"# Install virtualization packages apt-get install -y qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch-switch # Verify KVM support lscpu | grep -i virtualization # Start libvirt systemctl enable libvirtd systemctl start libvirtd # Start OVS systemctl enable openvswitch-switch systemctl start openvswitch-switch","title":"1. KVM/QEMU Installation"},{"location":"deployment/#2-network-configuration","text":"# Create OVS bridge ovs-vsctl add-br br0 # Add physical interface to bridge ovs-vsctl add-port br0 eth0 # Configure IP on bridge ip addr add 192.168.1.10/24 dev br0 ip link set br0 up # Create virtual network virsh net-define <(cat <<EOF <network> <name>default</name> <forward mode='nat'/> <bridge name='virbr0' stp='on' delay='0'/> <ip address='192.168.122.1' netmask='255.255.255.0'> <dhcp> <range start='192.168.122.2' end='192.168.122.254'/> </dhcp> </ip> </network> EOF ) # Start network virsh net-start default virsh net-autostart default","title":"2. Network Configuration"},{"location":"deployment/#3-vm-deployment","text":"# Create VM storage pool virsh pool-define <(cat <<EOF <pool type='dir'> <name>vm-images</name> <target> <path>/var/lib/libvirt/images</path> </target> </pool> EOF ) # Build and start pool virsh pool-build vm-images virsh pool-start vm-images virsh pool-autostart vm-images # Create VM disk qemu-img create -f qcow2 /var/lib/libvirt/images/vm1.qcow2 20G # Define VM virt-install --name vm1 \\ --memory 2048 \\ --vcpus 2 \\ --disk path=/var/lib/libvirt/images/vm1.qcow2,format=qcow2 \\ --network network=default \\ --graphics vnc \\ --os-variant ubuntu20.04 \\ --cdrom /path/to/ubuntu.iso # Start VM virsh start vm1","title":"3. VM Deployment"},{"location":"deployment/#ceph-hci-deployment","text":"","title":"CEPH HCI Deployment"},{"location":"deployment/#prerequisites_2","text":"Hardware Requirements : - Minimum 3 OSD nodes - Dedicated storage drives per OSD - 1GB RAM per OSD - Network: 10Gbps recommended Network Requirements : - Public network: 192.168.1.0/24 - Cluster network: 10.0.1.0/24 - DNS resolution configured","title":"Prerequisites"},{"location":"deployment/#1-ceph-installation","text":"# Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm mv cephadm /usr/local/bin/ # Install podman or docker apt-get install -y podman # Bootstrap cluster cephadm bootstrap --mon-ip 192.168.1.10 # Copy SSH keys ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.1.11 ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.1.12 # Add hosts to cluster ceph orch host add node1 192.168.1.10 ceph orch host add node2 192.168.1.11 ceph orch host add node3 192.168.1.12","title":"1. CEPH Installation"},{"location":"deployment/#2-osd-configuration","text":"# Create OSD specification cat > osd.yml <<EOF service_type: osd service_id: osd placement: hosts: - node1 - node2 - node3 spec: data_devices: all: true EOF # Apply OSD specification ceph orch apply -i osd.yml # Monitor OSD status ceph osd status ceph -s","title":"2. OSD Configuration"},{"location":"deployment/#3-pool-creation","text":"# Create RBD pool ceph osd pool create rbd 64 64 ceph osd pool application enable rbd rbd # Create RBD image rbd create rbd/vm1 --size 50G --image-format 2 # Create CephFS ceph osd pool create cephfs_metadata 64 64 ceph osd pool create cephfs_data 128 128 ceph fs new myfs cephfs_metadata cephfs_data","title":"3. Pool Creation"},{"location":"deployment/#full-stack-deployment","text":"","title":"Full Stack Deployment"},{"location":"deployment/#architecture","text":"graph TB subgraph \"Cluster Layer\" A[Corosync] B[Pacemaker] C[STONITH] end subgraph \"Storage Layer\" D[CEPH Cluster] E[RBD Images] F[CephFS] end subgraph \"Network Layer\" G[OVS] H[Network] I[VIP] end subgraph \"Virtualization Layer\" J[QEMU/KVM] K[libvirt] L[VMs] end A --> B B --> C B --> G B --> I D --> E D --> F G --> H H --> I K --> L L --> E L --> F B --> K style A fill:#c8e6c9 style D fill:#ffecb3 style G fill:#e1f5ff style J fill:#fff3e0","title":"Architecture"},{"location":"deployment/#deployment-steps","text":"","title":"Deployment Steps"},{"location":"deployment/#phase-1-infrastructure","text":"# 1. Configure network # Public network ip addr add 192.168.1.10/24 dev eth0 ip link set eth0 up # Cluster network ip addr add 10.0.1.10/24 dev eth1 ip link set eth1 up # 2. Configure DNS echo \"nameserver 192.168.1.1\" > /etc/resolv.conf # 3. Configure NTP systemctl enable chrony systemctl start chrony # 4. Configure firewall firewall-cmd --permanent --add-port=6789/tcp firewall-cmd --permanent --add-port=6800-7300/tcp firewall-cmd --permanent --add-port=5405-5406/udp firewall-cmd --reload","title":"Phase 1: Infrastructure"},{"location":"deployment/#phase-2-ceph-setup","text":"# Bootstrap Ceph cephadm bootstrap --mon-ip 192.168.1.10 # Add OSDs ceph orch host add node1 192.168.1.10 ceph orch host add node2 192.168.1.11 ceph orch host add node3 192.168.1.12 ceph orch apply osd --all-available-devices # Create pools ceph osd pool create rbd 64 64 ceph osd pool application enable rbd rbd rbd create rbd/vm1 --size 50G","title":"Phase 2: CEPH Setup"},{"location":"deployment/#phase-3-cluster-setup","text":"# Generate Corosync key corosync-keygen # Configure Corosync cat > /etc/corosync/corosync.conf <<EOF totem { version: 2 cluster_name: fullstack transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } } nodelist { node { ring0_addr: 192.168.1.10 nodeid: 1 name: node1 } node { ring0_addr: 192.168.1.11 nodeid: 2 name: node2 } node { ring0_addr: 192.168.1.12 nodeid: 3 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log } EOF # Start cluster systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd # Configure Pacemaker pcs cluster setup --name fullstack node1 node2 node3 pcs cluster start --all","title":"Phase 3: Cluster Setup"},{"location":"deployment/#phase-4-virtualization-setup","text":"# Install virtualization apt-get install -y qemu-kvm libvirt-daemon-system \\ libvirt-clients openvswitch-switch # Start services systemctl enable libvirtd systemctl start libvirtd # Configure OVS ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 # Create VM from RBD rbd create rbd/vm1 --size 50G virt-install --name vm1 \\ --memory 4096 \\ --vcpus 2 \\ --disk path=rbd:rbd/vm1,format=raw \\ --network bridge=br0 \\ --graphics vnc","title":"Phase 4: Virtualization Setup"},{"location":"deployment/#phase-5-resource-configuration","text":"# Create OVS resources pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge bridge_name=br0 # Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 cidr_netmask=24 # Create VM resource pcs resource create vm1 systemd:libvirtd # Add constraints pcs constraint order start ovs-bridge then vip pcs constraint order start vip then vm1 pcs constraint colocation add vip with ovs-bridge pcs constraint colocation add vm1 with vip # Start resources pcs resource start ovs-bridge","title":"Phase 5: Resource Configuration"},{"location":"deployment/#verification-and-testing","text":"","title":"Verification and Testing"},{"location":"deployment/#cluster-verification","text":"# Check Corosync corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker pcs status # Check OVS ovs-vsctl show # Check Ceph ceph -s ceph osd status","title":"Cluster Verification"},{"location":"deployment/#failover-testing","text":"# Test resource failover pcs resource move vm1 node2 # Verify failover pcs status # Test node failover systemctl stop pacemaker # Check cluster status pcs status","title":"Failover Testing"},{"location":"deployment/#performance-testing","text":"# Test I/O performance fio --name=randread --ioengine=libaio \\ --iodepth=16 --rw=randread --bs=4k \\ --direct=1 --size=1G --numjobs=4 \\ --filename=/mnt/cephfs/test # Test network performance iperf3 -c 192.168.1.100 -t 60 # Test VM performance virt-top","title":"Performance Testing"},{"location":"deployment/#production-checklist","text":"","title":"Production Checklist"},{"location":"deployment/#pre-deployment","text":"[ ] Hardware meets minimum requirements [ ] Network configured and tested [ ] DNS resolution working [ ] NTP synchronized [ ] Firewall rules configured [ ] Storage devices identified [ ] Backup strategy planned [ ] Monitoring system ready","title":"Pre-Deployment"},{"location":"deployment/#deployment","text":"[ ] All nodes have consistent configuration [ ] Corosync quorum established [ ] Pacemaker cluster operational [ ] STONITH devices configured [ ] CEPH cluster healthy [ ] Virtualization configured [ ] Resources defined and started [ ] Constraints configured","title":"Deployment"},{"location":"deployment/#post-deployment","text":"[ ] Cluster status verified [ ] Failover tested [ ] Performance tested [ ] Monitoring configured [ ] Alerting configured [ ] Documentation updated [ ] Backup procedures tested [ ] Team trained","title":"Post-Deployment"},{"location":"deployment/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"deployment/#cluster-issues","text":"# Check Corosync logs journalctl -u corosync -f # Check Pacemaker logs journalctl -u pacemaker -f # Check cluster status pcs status crm_mon # Check quorum corosync-quorumtool -s","title":"Cluster Issues"},{"location":"deployment/#storage-issues","text":"# Check Ceph status ceph -s # Check OSD status ceph osd status # Check pool status ceph osd lspools # Check logs ceph -w","title":"Storage Issues"},{"location":"deployment/#network-issues","text":"# Check OVS ovs-vsctl show # Check network ip addr show ip route show # Check connectivity ping node1 ping node2 ping node3","title":"Network Issues"},{"location":"deployment/#best-practices","text":"Plan thoroughly before deployment Test in staging first Document everything Monitor continuously Test failover regularly Keep backups current Review logs frequently Update regularly Train team members Have recovery plan ready","title":"Best Practices"},{"location":"license/","text":"Course materials follow documentation licenses of their respective projects: Corosync: BSD License Pacemaker: GPL-2.0 and LGPL-2.1 QEMU/KVM: GPL-2.0 CEPH: GPL-2.0, LGPL-2.1, LGPL-3.0 GFS2: GPL-2.0 libvirt: LGPL-2.1 Open vSwitch: Apache 2.0","title":"License"},{"location":"netplan/","text":"Network configuration abstraction layer for modern Linux distributions, replacing traditional networking tools. Overview Netplan reads YAML configuration files and generates configuration for network renderers (NetworkManager or systemd-networkd). graph TB A[Netplan YAML Files] --> B[netplan generate] B --> C[Configuration Backend] C --> D[NetworkManager] C --> E[systemd-networkd] D --> F[Network Interfaces] E --> F F --> G[Network State] style A fill:#c8e6c9 style B fill:#ffecb3 style C fill:#e1f5ff style F fill:#fff3e0 Configuration Files File Locations Location Purpose /etc/netplan/*.yaml Main configuration files /run/netplan/*.yaml Runtime configuration /lib/netplan/*.yaml Fallback configuration File Priority Files are processed in alphanumeric order: 1. /run/netplan/*.yaml (highest priority) 2. /etc/netplan/*.yaml 3. /lib/netplan/*.yaml (lowest priority) YAML Structure Basic Syntax network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true Version 2 Schema network: version: 2 # Required: schema version renderer: NetworkManager # Optional: NetworkManager or networkd renderer: systemd-networkd # Optional: NetworkManager or networkd ethernets: # Ethernet interfaces wifis: # Wireless interfaces bridges: # Bridge interfaces bonds: # Bond interfaces vlans: # VLAN interfaces tunnels: # Tunnel interfaces vrf: # VRF interfaces Ethernet Configuration Static IP Configuration network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false dhcp6: false addresses: - 192.168.1.10/24 - 192.168.2.10/24 gateway4: 192.168.1.1 gateway6: fe80::1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: - example.com - localdomain routes: - to: 10.0.0.0/8 via: 192.168.1.1 metric: 100 - to: 172.16.0.0/12 via: 192.168.2.1 metric: 200 DHCP Configuration network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true dhcp4-overrides: use-dns: true use-hostname: false use-mtu: true send-hostname: true hostname: myhost dhcp6: true dhcp6-overrides: use-dns: true use-hostname: false Link Local Addresses network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false dhcp6: false link-local: - ipv4 - ipv6 MTU Configuration network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true mtu: 9000 MAC Address Spoofing network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true macaddress: 00:11:22:33:44:55 Bridge Configuration Basic Bridge network: version: 2 renderer: NetworkManager bridges: br0: dhcp4: true interfaces: - eth0 - eth1 Bridge with Static IP network: version: 2 renderer: NetworkManager bridges: br0: dhcp4: false addresses: - 192.168.1.100/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8 interfaces: - eth0 - eth1 parameters: stp: true priority: 32768 forward-delay: 4 hello-time: 2 max-age: 20 ageing-time: 300 Bridge Parameters Parameter Description Default stp Spanning Tree Protocol false priority Bridge priority 32768 forward-delay Forward delay in seconds 4 hello-time Hello time in seconds 2 max-age Max age in seconds 20 ageing-time Ageing time in seconds 300 Bond Configuration Active-Backup Bond network: version: 2 renderer: NetworkManager bonds: bond0: dhcp4: true interfaces: - eth0 - eth1 parameters: mode: active-backup mii-monitor-interval: 100 up-delay: 200 down-delay: 200 LACP Bond network: version: 2 renderer: NetworkManager bonds: bond0: dhcp4: true interfaces: - eth0 - eth1 parameters: mode: 802.3ad lacp-rate: fast mii-monitor-interval: 100 xmit-hash-policy: layer3+4 Bond Modes Mode Description Requires Switch balance-rr Round-robin load balancing No active-backup Active/backup failover No balance-xor XOR load balancing Yes broadcast Broadcast to all slaves No 802.3ad LACP aggregation Yes balance-tlb Adaptive transmit load balancing No balance-alb Adaptive load balancing No Bond Parameters Parameter Description Default mode Bond mode active-backup lacp-rate LACP rate (slow/fast) slow mii-monitor-interval MII monitoring interval (ms) 100 up-delay Up delay (ms) 0 down-delay Down delay (ms) 0 xmit-hash-policy Hash policy layer2 arp-interval ARP interval (ms) 0 arp-ip-targets ARP target IPs - primary Primary slave - miimon MII monitoring interval (ms) 100 VLAN Configuration Basic VLAN network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false vlans: eth0.10: id: 10 link: eth0 dhcp4: true eth0.20: id: 20 link: eth0 dhcp4: true VLAN with Static IP network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false vlans: vlan10: id: 10 link: eth0 dhcp4: false addresses: - 192.168.10.10/24 Tunnel Configuration GRE Tunnel network: version: 2 renderer: NetworkManager tunnels: gre0: mode: gre local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 key: 1234 dhcp4: false addresses: - 10.0.0.1/30 VXLAN Tunnel network: version: 2 renderer: NetworkManager tunnels: vxlan0: mode: vxlan id: 100 local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 dhcp4: false addresses: - 10.0.0.1/24 IPIP Tunnel network: version: 2 renderer: NetworkManager tunnels: ipip0: mode: ipip local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 dhcp4: false addresses: - 10.0.0.1/30 SIT Tunnel network: version: 2 renderer: NetworkManager tunnels: sit0: mode: sit local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 dhcp4: false addresses: - 2001:db8::1/64 Wireless Configuration Basic Wireless network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: true access-points: \"MyNetwork\": password: \"mypassword\" Multiple Networks network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: true access-points: \"Network1\": password: \"password1\" \"Network2\": password: \"password2\" Wireless with Static IP network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: false addresses: - 192.168.1.100/24 gateway4: 192.168.1.1 access-points: \"MyNetwork\": password: \"mypassword\" Hidden Network network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: true access-points: \"HiddenNetwork\": hidden: true password: \"mypassword\" Advanced Configuration Multi-Homing network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true dhcp4-overrides: use-routes: false eth1: dhcp4: true dhcp4-overrides: use-routes: false addresses: - 192.168.2.10/24 routes: - to: 0.0.0.0/0 via: 192.168.2.1 table: 101 routing-policy: - from: 192.168.2.10/24 table: 101 Policy-Based Routing network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true routes: - to: 10.0.0.0/8 via: 192.168.1.1 table: 100 - to: 172.16.0.0/12 via: 192.168.1.1 table: 101 routing-policy: - from: 10.0.0.0/8 table: 100 - from: 172.16.0.0/12 table: 101 IPv6 Configuration network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp6: true dhcp6-overrides: use-dns: true accept-ra: true addresses: - 2001:db8::1/64 gateway6: fe80::1 nameservers: addresses: - 2001:4860:4860::8888 - 2001:4860:4860::8844 Link Aggregation with LACP network: version: 2 renderer: NetworkManager bonds: bond0: interfaces: - eth0 - eth1 parameters: mode: 802.3ad lacp-rate: fast xmit-hash-policy: layer3+4 addresses: - 192.168.1.100/24 routes: - to: 0.0.0.0/0 via: 192.168.1.1 Commands Generate Configuration # Generate configuration sudo netplan generate # Generate with debug output sudo netplan generate --debug # Generate for specific renderer sudo netplan generate --renderer NetworkManager sudo netplan generate --renderer networkd Apply Configuration # Apply configuration sudo netplan apply # Apply with debug output sudo netplan apply --debug # Apply configuration without stopping interfaces sudo netplan apply --no-flush Test Configuration # Try configuration (temporary) sudo netplan try # Try with timeout (default 120 seconds) sudo netplan try --timeout 180 Debug Configuration # Enable debug mode sudo netplan --debug generate # Check generated files cat /run/systemd/network/10-netplan-*.network # Check NetworkManager connections nmcli connection show Netplan Commands Reference Command Description netplan generate Generate configuration files netplan apply Apply configuration netplan try Test configuration netplan info Show netplan version netplan help Show help Configuration Validation Syntax Check # Validate YAML syntax python3 -c \"import yaml; yaml.safe_load(open('/etc/netplan/01-netcfg.yaml'))\" # Or use yamllint yamllint /etc/netplan/01-netcfg.yaml Dry Run # Generate without applying sudo netplan generate # Check generated configuration cat /run/systemd/network/10-netplan-*.network Troubleshooting Common Issues Network not starting : # Check configuration sudo netplan --debug generate # Check systemd-networkd status sudo systemctl status systemd-networkd # Check NetworkManager status sudo systemctl status NetworkManager # Check network configuration ip addr show ip route show Invalid YAML : # Validate YAML python3 -c \"import yaml; yaml.safe_load(open('/etc/netplan/01-netcfg.yaml'))\" # Check for tabs (YAML doesn't support tabs) cat -A /etc/netplan/01-netcfg.yaml # Fix indentation (use 2 spaces) Configuration not applying : # Stop interfaces sudo ip link set eth0 down # Generate configuration sudo netplan generate # Apply configuration sudo netplan apply # Check status systemctl status systemd-networkd systemctl status NetworkManager Debug Logging # Enable debug logging sudo journalctl -u systemd-networkd -f # Enable NetworkManager debug sudo journalctl -u NetworkManager -f # Check netplan debug sudo netplan --debug generate Best Practices Always use 2 spaces for indentation (no tabs) Test configuration with netplan try before applying Back up configuration before making changes Use version 2 schema Specify renderer explicitly Test network connectivity after applying Use netplan try for production changes Monitor logs for errors Document custom configurations Use appropriate MTU for network type Example Configurations Cluster Node Configuration network: version: 2 renderer: NetworkManager ethernets: # Public network eth0: dhcp4: false addresses: - 192.168.1.10/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: - cluster.local - example.com # Cluster network eth1: dhcp4: false addresses: - 10.0.1.10/24 mtu: 9000 # Storage network eth2: dhcp4: false addresses: - 10.0.2.10/24 mtu: 9000 HA Configuration with Bonding network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false eth1: dhcp4: false bonds: bond0: dhcp4: false interfaces: - eth0 - eth1 parameters: mode: active-backup mii-monitor-interval: 100 up-delay: 200 down-delay: 200 addresses: - 192.168.1.10/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8 VM Host Configuration network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false addresses: - 192.168.1.10/24 gateway4: 192.168.1.1 bridges: br0: dhcp4: false addresses: - 192.168.100.1/24 interfaces: - eth1 parameters: stp: true priority: 32768 vlans: vlan10: id: 10 link: br0 dhcp4: false addresses: - 192.168.10.1/24 NetworkManager vs systemd-networkd NetworkManager Advantages : - GUI support - Dynamic configuration - VPN integration - Wi-Fi support - Connection profiles Best for : - Desktop systems - Laptops - Systems with Wi-Fi - Complex networking systemd-networkd Advantages : - Lightweight - Fast startup - Simple configuration - Systemd integration Best for : - Servers - Containers - Simple networking - Fast boot times Source Code Repository : https://github.com/canonical/netplan Documentation : https://netplan.readthedocs.io/ YAML Schema : https://netplan.readthedocs.io/en/stable/netplan-yaml/","title":"Netplan"},{"location":"netplan/#overview","text":"Netplan reads YAML configuration files and generates configuration for network renderers (NetworkManager or systemd-networkd). graph TB A[Netplan YAML Files] --> B[netplan generate] B --> C[Configuration Backend] C --> D[NetworkManager] C --> E[systemd-networkd] D --> F[Network Interfaces] E --> F F --> G[Network State] style A fill:#c8e6c9 style B fill:#ffecb3 style C fill:#e1f5ff style F fill:#fff3e0","title":"Overview"},{"location":"netplan/#configuration-files","text":"","title":"Configuration Files"},{"location":"netplan/#file-locations","text":"Location Purpose /etc/netplan/*.yaml Main configuration files /run/netplan/*.yaml Runtime configuration /lib/netplan/*.yaml Fallback configuration","title":"File Locations"},{"location":"netplan/#file-priority","text":"Files are processed in alphanumeric order: 1. /run/netplan/*.yaml (highest priority) 2. /etc/netplan/*.yaml 3. /lib/netplan/*.yaml (lowest priority)","title":"File Priority"},{"location":"netplan/#yaml-structure","text":"","title":"YAML Structure"},{"location":"netplan/#basic-syntax","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true","title":"Basic Syntax"},{"location":"netplan/#version-2-schema","text":"network: version: 2 # Required: schema version renderer: NetworkManager # Optional: NetworkManager or networkd renderer: systemd-networkd # Optional: NetworkManager or networkd ethernets: # Ethernet interfaces wifis: # Wireless interfaces bridges: # Bridge interfaces bonds: # Bond interfaces vlans: # VLAN interfaces tunnels: # Tunnel interfaces vrf: # VRF interfaces","title":"Version 2 Schema"},{"location":"netplan/#ethernet-configuration","text":"","title":"Ethernet Configuration"},{"location":"netplan/#static-ip-configuration","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false dhcp6: false addresses: - 192.168.1.10/24 - 192.168.2.10/24 gateway4: 192.168.1.1 gateway6: fe80::1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: - example.com - localdomain routes: - to: 10.0.0.0/8 via: 192.168.1.1 metric: 100 - to: 172.16.0.0/12 via: 192.168.2.1 metric: 200","title":"Static IP Configuration"},{"location":"netplan/#dhcp-configuration","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true dhcp4-overrides: use-dns: true use-hostname: false use-mtu: true send-hostname: true hostname: myhost dhcp6: true dhcp6-overrides: use-dns: true use-hostname: false","title":"DHCP Configuration"},{"location":"netplan/#link-local-addresses","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false dhcp6: false link-local: - ipv4 - ipv6","title":"Link Local Addresses"},{"location":"netplan/#mtu-configuration","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true mtu: 9000","title":"MTU Configuration"},{"location":"netplan/#mac-address-spoofing","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true macaddress: 00:11:22:33:44:55","title":"MAC Address Spoofing"},{"location":"netplan/#bridge-configuration","text":"","title":"Bridge Configuration"},{"location":"netplan/#basic-bridge","text":"network: version: 2 renderer: NetworkManager bridges: br0: dhcp4: true interfaces: - eth0 - eth1","title":"Basic Bridge"},{"location":"netplan/#bridge-with-static-ip","text":"network: version: 2 renderer: NetworkManager bridges: br0: dhcp4: false addresses: - 192.168.1.100/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8 interfaces: - eth0 - eth1 parameters: stp: true priority: 32768 forward-delay: 4 hello-time: 2 max-age: 20 ageing-time: 300","title":"Bridge with Static IP"},{"location":"netplan/#bridge-parameters","text":"Parameter Description Default stp Spanning Tree Protocol false priority Bridge priority 32768 forward-delay Forward delay in seconds 4 hello-time Hello time in seconds 2 max-age Max age in seconds 20 ageing-time Ageing time in seconds 300","title":"Bridge Parameters"},{"location":"netplan/#bond-configuration","text":"","title":"Bond Configuration"},{"location":"netplan/#active-backup-bond","text":"network: version: 2 renderer: NetworkManager bonds: bond0: dhcp4: true interfaces: - eth0 - eth1 parameters: mode: active-backup mii-monitor-interval: 100 up-delay: 200 down-delay: 200","title":"Active-Backup Bond"},{"location":"netplan/#lacp-bond","text":"network: version: 2 renderer: NetworkManager bonds: bond0: dhcp4: true interfaces: - eth0 - eth1 parameters: mode: 802.3ad lacp-rate: fast mii-monitor-interval: 100 xmit-hash-policy: layer3+4","title":"LACP Bond"},{"location":"netplan/#bond-modes","text":"Mode Description Requires Switch balance-rr Round-robin load balancing No active-backup Active/backup failover No balance-xor XOR load balancing Yes broadcast Broadcast to all slaves No 802.3ad LACP aggregation Yes balance-tlb Adaptive transmit load balancing No balance-alb Adaptive load balancing No","title":"Bond Modes"},{"location":"netplan/#bond-parameters","text":"Parameter Description Default mode Bond mode active-backup lacp-rate LACP rate (slow/fast) slow mii-monitor-interval MII monitoring interval (ms) 100 up-delay Up delay (ms) 0 down-delay Down delay (ms) 0 xmit-hash-policy Hash policy layer2 arp-interval ARP interval (ms) 0 arp-ip-targets ARP target IPs - primary Primary slave - miimon MII monitoring interval (ms) 100","title":"Bond Parameters"},{"location":"netplan/#vlan-configuration","text":"","title":"VLAN Configuration"},{"location":"netplan/#basic-vlan","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false vlans: eth0.10: id: 10 link: eth0 dhcp4: true eth0.20: id: 20 link: eth0 dhcp4: true","title":"Basic VLAN"},{"location":"netplan/#vlan-with-static-ip","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false vlans: vlan10: id: 10 link: eth0 dhcp4: false addresses: - 192.168.10.10/24","title":"VLAN with Static IP"},{"location":"netplan/#tunnel-configuration","text":"","title":"Tunnel Configuration"},{"location":"netplan/#gre-tunnel","text":"network: version: 2 renderer: NetworkManager tunnels: gre0: mode: gre local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 key: 1234 dhcp4: false addresses: - 10.0.0.1/30","title":"GRE Tunnel"},{"location":"netplan/#vxlan-tunnel","text":"network: version: 2 renderer: NetworkManager tunnels: vxlan0: mode: vxlan id: 100 local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 dhcp4: false addresses: - 10.0.0.1/24","title":"VXLAN Tunnel"},{"location":"netplan/#ipip-tunnel","text":"network: version: 2 renderer: NetworkManager tunnels: ipip0: mode: ipip local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 dhcp4: false addresses: - 10.0.0.1/30","title":"IPIP Tunnel"},{"location":"netplan/#sit-tunnel","text":"network: version: 2 renderer: NetworkManager tunnels: sit0: mode: sit local: 192.168.1.10 remote: 192.168.1.20 ttl: 255 dhcp4: false addresses: - 2001:db8::1/64","title":"SIT Tunnel"},{"location":"netplan/#wireless-configuration","text":"","title":"Wireless Configuration"},{"location":"netplan/#basic-wireless","text":"network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: true access-points: \"MyNetwork\": password: \"mypassword\"","title":"Basic Wireless"},{"location":"netplan/#multiple-networks","text":"network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: true access-points: \"Network1\": password: \"password1\" \"Network2\": password: \"password2\"","title":"Multiple Networks"},{"location":"netplan/#wireless-with-static-ip","text":"network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: false addresses: - 192.168.1.100/24 gateway4: 192.168.1.1 access-points: \"MyNetwork\": password: \"mypassword\"","title":"Wireless with Static IP"},{"location":"netplan/#hidden-network","text":"network: version: 2 renderer: NetworkManager wifis: wlan0: dhcp4: true access-points: \"HiddenNetwork\": hidden: true password: \"mypassword\"","title":"Hidden Network"},{"location":"netplan/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"netplan/#multi-homing","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true dhcp4-overrides: use-routes: false eth1: dhcp4: true dhcp4-overrides: use-routes: false addresses: - 192.168.2.10/24 routes: - to: 0.0.0.0/0 via: 192.168.2.1 table: 101 routing-policy: - from: 192.168.2.10/24 table: 101","title":"Multi-Homing"},{"location":"netplan/#policy-based-routing","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true routes: - to: 10.0.0.0/8 via: 192.168.1.1 table: 100 - to: 172.16.0.0/12 via: 192.168.1.1 table: 101 routing-policy: - from: 10.0.0.0/8 table: 100 - from: 172.16.0.0/12 table: 101","title":"Policy-Based Routing"},{"location":"netplan/#ipv6-configuration","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp6: true dhcp6-overrides: use-dns: true accept-ra: true addresses: - 2001:db8::1/64 gateway6: fe80::1 nameservers: addresses: - 2001:4860:4860::8888 - 2001:4860:4860::8844","title":"IPv6 Configuration"},{"location":"netplan/#link-aggregation-with-lacp","text":"network: version: 2 renderer: NetworkManager bonds: bond0: interfaces: - eth0 - eth1 parameters: mode: 802.3ad lacp-rate: fast xmit-hash-policy: layer3+4 addresses: - 192.168.1.100/24 routes: - to: 0.0.0.0/0 via: 192.168.1.1","title":"Link Aggregation with LACP"},{"location":"netplan/#commands","text":"","title":"Commands"},{"location":"netplan/#generate-configuration","text":"# Generate configuration sudo netplan generate # Generate with debug output sudo netplan generate --debug # Generate for specific renderer sudo netplan generate --renderer NetworkManager sudo netplan generate --renderer networkd","title":"Generate Configuration"},{"location":"netplan/#apply-configuration","text":"# Apply configuration sudo netplan apply # Apply with debug output sudo netplan apply --debug # Apply configuration without stopping interfaces sudo netplan apply --no-flush","title":"Apply Configuration"},{"location":"netplan/#test-configuration","text":"# Try configuration (temporary) sudo netplan try # Try with timeout (default 120 seconds) sudo netplan try --timeout 180","title":"Test Configuration"},{"location":"netplan/#debug-configuration","text":"# Enable debug mode sudo netplan --debug generate # Check generated files cat /run/systemd/network/10-netplan-*.network # Check NetworkManager connections nmcli connection show","title":"Debug Configuration"},{"location":"netplan/#netplan-commands-reference","text":"Command Description netplan generate Generate configuration files netplan apply Apply configuration netplan try Test configuration netplan info Show netplan version netplan help Show help","title":"Netplan Commands Reference"},{"location":"netplan/#configuration-validation","text":"","title":"Configuration Validation"},{"location":"netplan/#syntax-check","text":"# Validate YAML syntax python3 -c \"import yaml; yaml.safe_load(open('/etc/netplan/01-netcfg.yaml'))\" # Or use yamllint yamllint /etc/netplan/01-netcfg.yaml","title":"Syntax Check"},{"location":"netplan/#dry-run","text":"# Generate without applying sudo netplan generate # Check generated configuration cat /run/systemd/network/10-netplan-*.network","title":"Dry Run"},{"location":"netplan/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"netplan/#common-issues","text":"Network not starting : # Check configuration sudo netplan --debug generate # Check systemd-networkd status sudo systemctl status systemd-networkd # Check NetworkManager status sudo systemctl status NetworkManager # Check network configuration ip addr show ip route show Invalid YAML : # Validate YAML python3 -c \"import yaml; yaml.safe_load(open('/etc/netplan/01-netcfg.yaml'))\" # Check for tabs (YAML doesn't support tabs) cat -A /etc/netplan/01-netcfg.yaml # Fix indentation (use 2 spaces) Configuration not applying : # Stop interfaces sudo ip link set eth0 down # Generate configuration sudo netplan generate # Apply configuration sudo netplan apply # Check status systemctl status systemd-networkd systemctl status NetworkManager","title":"Common Issues"},{"location":"netplan/#debug-logging","text":"# Enable debug logging sudo journalctl -u systemd-networkd -f # Enable NetworkManager debug sudo journalctl -u NetworkManager -f # Check netplan debug sudo netplan --debug generate","title":"Debug Logging"},{"location":"netplan/#best-practices","text":"Always use 2 spaces for indentation (no tabs) Test configuration with netplan try before applying Back up configuration before making changes Use version 2 schema Specify renderer explicitly Test network connectivity after applying Use netplan try for production changes Monitor logs for errors Document custom configurations Use appropriate MTU for network type","title":"Best Practices"},{"location":"netplan/#example-configurations","text":"","title":"Example Configurations"},{"location":"netplan/#cluster-node-configuration","text":"network: version: 2 renderer: NetworkManager ethernets: # Public network eth0: dhcp4: false addresses: - 192.168.1.10/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: - cluster.local - example.com # Cluster network eth1: dhcp4: false addresses: - 10.0.1.10/24 mtu: 9000 # Storage network eth2: dhcp4: false addresses: - 10.0.2.10/24 mtu: 9000","title":"Cluster Node Configuration"},{"location":"netplan/#ha-configuration-with-bonding","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false eth1: dhcp4: false bonds: bond0: dhcp4: false interfaces: - eth0 - eth1 parameters: mode: active-backup mii-monitor-interval: 100 up-delay: 200 down-delay: 200 addresses: - 192.168.1.10/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8","title":"HA Configuration with Bonding"},{"location":"netplan/#vm-host-configuration","text":"network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: false addresses: - 192.168.1.10/24 gateway4: 192.168.1.1 bridges: br0: dhcp4: false addresses: - 192.168.100.1/24 interfaces: - eth1 parameters: stp: true priority: 32768 vlans: vlan10: id: 10 link: br0 dhcp4: false addresses: - 192.168.10.1/24","title":"VM Host Configuration"},{"location":"netplan/#networkmanager-vs-systemd-networkd","text":"","title":"NetworkManager vs systemd-networkd"},{"location":"netplan/#networkmanager","text":"Advantages : - GUI support - Dynamic configuration - VPN integration - Wi-Fi support - Connection profiles Best for : - Desktop systems - Laptops - Systems with Wi-Fi - Complex networking","title":"NetworkManager"},{"location":"netplan/#systemd-networkd","text":"Advantages : - Lightweight - Fast startup - Simple configuration - Systemd integration Best for : - Servers - Containers - Simple networking - Fast boot times","title":"systemd-networkd"},{"location":"netplan/#source-code","text":"Repository : https://github.com/canonical/netplan Documentation : https://netplan.readthedocs.io/ YAML Schema : https://netplan.readthedocs.io/en/stable/netplan-yaml/","title":"Source Code"},{"location":"reference/","text":"Corosync Command Description corosync-cfgtool -s Show cluster status corosync-quorumtool -s Show quorum status corosync-cmapctl Query configuration corosync-keygen Generate auth key Pacemaker Command Description pcs status Display cluster status pcs resource create Create resource pcs constraint order Add ordering pcs constraint colocation Add colocation pcs stonith create Create STONITH QEMU/KVM Command Description qemu-system-x86_64 Run QEMU VM virsh list List VMs virsh start Start VM virsh shutdown Shutdown VM kvm-ok Check KVM support CEPH Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image ceph orch apply Apply service iSCSI Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets rbd showmapped Show mapped RBDs NVMe-oF Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems nvme show-ctrl Show controller GFS2 Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes dlm_tool dump Dump locks Open vSwitch Command Description ovs-vsctl add-br Add bridge ovs-vsctl del-br Delete bridge ovs-vsctl show Show config ovs-ofctl show Show OpenFlow rules","title":"Reference"},{"location":"reference/#corosync","text":"Command Description corosync-cfgtool -s Show cluster status corosync-quorumtool -s Show quorum status corosync-cmapctl Query configuration corosync-keygen Generate auth key","title":"Corosync"},{"location":"reference/#pacemaker","text":"Command Description pcs status Display cluster status pcs resource create Create resource pcs constraint order Add ordering pcs constraint colocation Add colocation pcs stonith create Create STONITH","title":"Pacemaker"},{"location":"reference/#qemukvm","text":"Command Description qemu-system-x86_64 Run QEMU VM virsh list List VMs virsh start Start VM virsh shutdown Shutdown VM kvm-ok Check KVM support","title":"QEMU/KVM"},{"location":"reference/#ceph","text":"Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image ceph orch apply Apply service","title":"CEPH"},{"location":"reference/#iscsi","text":"Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets rbd showmapped Show mapped RBDs","title":"iSCSI"},{"location":"reference/#nvme-of","text":"Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems nvme show-ctrl Show controller","title":"NVMe-oF"},{"location":"reference/#gfs2","text":"Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes dlm_tool dump Dump locks","title":"GFS2"},{"location":"reference/#open-vswitch","text":"Command Description ovs-vsctl add-br Add bridge ovs-vsctl del-br Delete bridge ovs-vsctl show Show config ovs-ofctl show Show OpenFlow rules","title":"Open vSwitch"},{"location":"resources/","text":"Cluster Technologies Technology Official Site Source Code Corosync https://corosync.github.io/corosync/ https://github.com/corosync/corosync Pacemaker https://www.clusterlabs.org/pacemaker/ https://github.com/ClusterLabs/pacemaker Virtualization Technology Official Site Source Code QEMU https://www.qemu.org/ https://gitlab.com/qemu-project/qemu KVM https://www.linux-kvm.org/ https://github.com/torvalds/linux/tree/master/virt/kvm libvirt https://libvirt.org/ https://gitlab.com/libvirt/libvirt Open vSwitch https://www.openvswitch.org/ https://github.com/openvswitch/ovs Storage Technology Official Site Source Code CEPH https://docs.ceph.com/ https://github.com/ceph/ceph iSCSI https://linux-iscsi.org/ https://github.com/ceph/ceph NVMe-oF https://nvmexpress.org/ https://github.com/ceph/ceph GFS2 /usr/share/doc/gfs2-utils/ https://github.com/torvalds/linux/tree/master/fs/gfs2 Community and Support CEPH: https://ceph.io/community/ ClusterLabs: https://clusterlabs.org/community/ QEMU: qemu-devel@nongnu.org KVM: kvm@vger.kernel.org Bug Trackers Corosync: https://github.com/corosync/corosync/issues Pacemaker: https://bugs.clusterlabs.org/ CEPH: https://tracker.ceph.com/projects/ceph QEMU: https://gitlab.com/qemu-project/qemu/-/issues libvirt: https://gitlab.com/libvirt/libvirt/-/issues Open vSwitch: https://bugs.openvswitch.org/","title":"Resources"},{"location":"resources/#cluster-technologies","text":"Technology Official Site Source Code Corosync https://corosync.github.io/corosync/ https://github.com/corosync/corosync Pacemaker https://www.clusterlabs.org/pacemaker/ https://github.com/ClusterLabs/pacemaker","title":"Cluster Technologies"},{"location":"resources/#virtualization","text":"Technology Official Site Source Code QEMU https://www.qemu.org/ https://gitlab.com/qemu-project/qemu KVM https://www.linux-kvm.org/ https://github.com/torvalds/linux/tree/master/virt/kvm libvirt https://libvirt.org/ https://gitlab.com/libvirt/libvirt Open vSwitch https://www.openvswitch.org/ https://github.com/openvswitch/ovs","title":"Virtualization"},{"location":"resources/#storage","text":"Technology Official Site Source Code CEPH https://docs.ceph.com/ https://github.com/ceph/ceph iSCSI https://linux-iscsi.org/ https://github.com/ceph/ceph NVMe-oF https://nvmexpress.org/ https://github.com/ceph/ceph GFS2 /usr/share/doc/gfs2-utils/ https://github.com/torvalds/linux/tree/master/fs/gfs2","title":"Storage"},{"location":"resources/#community-and-support","text":"CEPH: https://ceph.io/community/ ClusterLabs: https://clusterlabs.org/community/ QEMU: qemu-devel@nongnu.org KVM: kvm@vger.kernel.org","title":"Community and Support"},{"location":"resources/#bug-trackers","text":"Corosync: https://github.com/corosync/corosync/issues Pacemaker: https://bugs.clusterlabs.org/ CEPH: https://tracker.ceph.com/projects/ceph QEMU: https://gitlab.com/qemu-project/qemu/-/issues libvirt: https://gitlab.com/libvirt/libvirt/-/issues Open vSwitch: https://bugs.openvswitch.org/","title":"Bug Trackers"},{"location":"stonith/","text":"Comprehensive guide to STONITH (Shoot The Other Node In The Head) and fencing for Linux HA clusters. Overview STONITH is the fencing mechanism used by Pacemaker to prevent split-brain scenarios by ensuring failed nodes are properly fenced. graph TB subgraph \"Cluster Layer\" A[Pacemaker] B[STONITH Resources] C[Quorum] end subgraph \"Fencing Layer\" D[Fence Agents] E[Fence Device] F[Network] end subgraph \"Hardware Layer\" G[IPMI/iLO/DRAC] H[Network Switches] I[PDU - Power Distribution Units] end subgraph \"Cloud Layer\" J[AWS EC2] K[OpenStack Nova] L[Azure VM] end A --> B A --> C B --> D D --> E E --> F E --> G E --> H E --> I E --> J E --> K E --> L style B fill:#c8e6c9 style D fill:#ffecb3 style G fill:#e1f5ff style J fill:#fff3e0 STONITH Architecture Fencing Process sequenceDiagram participant C as Cluster participant S as STONITH Daemon participant F as Fence Agent participant H as Hardware C->>S: Node failure detected S->>F: Initiate fence F->>H: Send fence command H->>H: Execute fence (reboot/poweroff) H->>F: Fence complete F->>S: Fence success S->>C: Node fenced C->>C: Recover resources style S fill:#c8e6c9 style F fill:#ffecb3 style H fill:#e1f5ff STONITH Components Component Description STONITH Daemon Manages fencing operations Fence Agents Scripts for fencing devices Fence Resources Cluster resources for fencing Fence Levels Primary/secondary fencing Fence Methods Multiple fencing strategies Fence Agent Types IPMI Fence Agents fence_ipmilan : Intel IPMI LAN interface # Create IPMI fence device pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 \\ lanplus=1 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | IPMI IP address | - | | login | IPMI username | - | | passwd | IPMI password | - | | port | IPMI port | 623 | | lanplus | Use IPMIv2.0 | 0 | | action | Fencing action (reboot/off/on) | reboot | | timeout | Timeout in seconds | 20 | | cipher | Cipher suite (0-15) | 17 | iLO Fence Agents fence_ilo : HP iLO fencing # Create iLO fence device pcs stonith create fence-ilo-node1 \\ fence_ilo \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | iLO IP address | - | | login | iLO username | - | | passwd | iLO password | - | | ssl | Use SSL | 1 | | port | iLO port | 443 | | action | Fencing action | reboot | | timeout | Timeout in seconds | 20 | DRAC Fence Agents fence_drac5 : Dell DRAC 5 fencing # Create DRAC fence device pcs stonith create fence-drac-node1 \\ fence_drac5 \\ ipaddr=192.168.1.203 \\ login=root \\ passwd=AdminPass123 \\ module=1 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | DRAC IP address | - | | login | DRAC username | - | | passwd | DRAC password | - | | module | DRAC module number | 1 | | cmd_prompt | Command prompt | ([A-Za-z0-9])+\\s*>? | | action | Fencing action | reboot | VMware Fence Agents fence_vmware_soap : VMware vSphere fencing # Create VMware fence device pcs stonith create fence-vmware \\ fence_vmware_soap \\ ipaddr=192.168.1.210 \\ login=administrator \\ passwd=AdminPass123 \\ ssl=1 \\ ipport=443 \\ action=reboot \\ pcmk_host_list=\"node1 node2 node3\" Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | vCenter IP address | - | | login | vCenter username | - | | passwd | vCenter password | - | | ssl | Use SSL | 1 | | ipport | vCenter port | 443 | | vm_name | VM name pattern | - | | action | Fencing action | reboot | AWS Fence Agents fence_aws : Amazon AWS fencing # Create AWS fence device pcs stonith create fence-aws \\ fence_aws \\ region=us-east-1 \\ access-key=AKIAIOSFODNN7EXAMPLE \\ secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\ action=reboot Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | region | AWS region | - | | access-key | AWS access key | - | | secret-key | AWS secret key | - | | profile | AWS profile | - | | action | Fencing action | reboot | PDU Fence Agents fence_apc : APC PDU fencing # Create APC PDU fence device pcs stonith create fence-apc-pdu \\ fence_apc \\ ipaddr=192.168.1.220 \\ login=admin \\ passwd=AdminPass123 \\ port=1 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | PDU IP address | - | | login | PDU username | - | | passwd | PDU password | - | | port | Outlet number | - | | switch | Switch number | 1 | | action | Fencing action | reboot | Switch Fence Agents fence_ifmib : SNMP-based fencing # Create SNMP fence device pcs stonith create fence-switch \\ fence_ifmib \\ ipaddr=192.168.1.230 \\ community=public \\ action=reboot Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | Switch IP address | - | | community | SNMP community | public | | port | Port number | - | | action | Fencing action | reboot | STONITH Configuration Basic Configuration # Enable STONITH pcs property set stonith-enabled=true # Set STONITH timeout pcs property set stonith-timeout=60s # Set STONITH action pcs property set stonith-action=reboot # Set STONITH failure handling pcs property set stonith-failure-is-fatal=true Per-Node Fencing # IPMI fencing for node1 pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node1 # IPMI fencing for node2 pcs stonith create fence-ipmi-node2 \\ fence_ipmilan \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node2 # IPMI fencing for node3 pcs stonith create fence-ipmi-node3 \\ fence_ipmilan \\ ipaddr=192.168.1.203 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node3 Shared Fencing Device # Single fence device for all nodes pcs stonith create fence-vmware \\ fence_vmware_soap \\ ipaddr=192.168.1.210 \\ login=administrator \\ passwd=AdminPass123 \\ pcmk_host_list=\"node1 node2 node3\" Fence Levels # Primary fencing level (IPMI) pcs stonith level add 1 node1 fence-ipmi-node1 # Secondary fencing level (VMware) pcs stonith level add 2 node1 fence-vmware # View fence levels pcs stonith level # Remove fence level pcs stonith level remove 1 node1 Fence Methods # Define fence method pcs stonith create fence-ipmi-ilo \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node1 \\ op monitor interval=60s pcs stonith create fence-ilo-fallback \\ fence_ilo \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node1 STONITH Operations Testing Fencing # Test fencing manually fence_ipmilan -a 192.168.1.201 -l admin -p AdminPass123 \\ -o status # Test fencing via pcs pcs stonith fence node1 # Test fencing with verbose output pcs stonith fence node1 --verbose Monitoring Fencing # Check STONITH status pcs stonith status # Check STONITH devices pcs stonith show # Check fence device details pcs stonith show fence-ipmi-node1 # Check STONITH logs journalctl -u pacemaker -f | grep stonith Fence Device Management # Start fence device pcs resource start fence-ipmi-node1 # Stop fence device pcs resource stop fence-ipmi-node1 # Enable fence device pcs resource enable fence-ipmi-node1 # Disable fence device pcs resource disable fence-ipmi-node1 # Delete fence device pcs resource delete fence-ipmi-node1 Advanced Configuration STONITH Properties Property Description Default stonith-enabled Enable/disable STONITH false stonith-timeout Timeout for fencing 60s stonith-action Default fencing action reboot stonith-failure-is-fatal Stop cluster on fence failure true stonith-watchdog-timeout Watchdog timeout 0 stonith-max-attempts Max fencing attempts 2 Fence Device Attributes # Set timeout pcs resource update fence-ipmi-node1 \\ meta target-role=Started # Set monitoring interval pcs resource op add fence-ipmi-node1 \\ monitor interval=60s timeout=90s # Set resource stickiness pcs resource update fence-ipmi-node1 \\ meta resource-stickiness=0 Fence Device Constraints # Colocate fencing with node pcs constraint colocation add fence-ipmi-node1 with node1 # Order fencing before resources pcs constraint order start fence-ipmi-node1 then apache # Location constraint pcs constraint location fence-ipmi-node1 prefers node1 Troubleshooting Fencing Failures Check fence device status : # Check STONITH status pcs stonith show # Check resource status pcs status resources # Check cluster status pcs status Test fence device manually : # Test IPMI fencing fence_ipmilan -a 192.168.1.201 -l admin -p AdminPass123 \\ -o status # Test iLO fencing fence_ilo -a 192.168.1.202 -l admin -p AdminPass123 \\ -o status # Test DRAC fencing fence_drac5 -a 192.168.1.203 -l root -p AdminPass123 \\ -o status Check logs : # Check Pacemaker logs journalctl -u pacemaker -f # Check STONITH logs grep stonith /var/log/pacemaker/pacemaker.log # Check fence agent logs journalctl -f | grep fence Network Issues Check connectivity : # Ping fence device ping -c 3 192.168.1.201 # Check port accessibility telnet 192.168.1.201 623 nc -zv 192.168.1.201 623 Check firewall : # Check firewall rules iptables -L -n # Open IPMI port iptables -A INPUT -p tcp --dport 623 -j ACCEPT iptables -A INPUT -p udp --dport 623 -j ACCEPT # Check UFW (Ubuntu) ufw allow 623/tcp ufw allow 623/udp Authentication Issues Verify credentials : # Test IPMI credentials ipmitool -I lanplus -H 192.168.1.201 \\ -U admin -P AdminPass123 chassis status # Test iLO credentials curl -k https://192.168.1.202/ \\ --user admin:AdminPass123 # Test DRAC credentials racadm -r 192.168.1.203 -u root -p AdminPass123 \\ getinfo Check fence device configuration : # View fence device configuration pcs stonith show fence-ipmi-node1 # Check fence device status pcs resource show fence-ipmi-node1 Cloud Fencing AWS EC2 Fencing # Create AWS fence device pcs stonith create fence-aws \\ fence_aws \\ region=us-east-1 \\ access-key=AKIAIOSFODNN7EXAMPLE \\ secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Map instances to fence device pcs stonith create fence-aws-node1 \\ fence_aws \\ region=us-east-1 \\ access-key=AKIAIOSFODNN7EXAMPLE \\ secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\ pcmk_host_map=\"node1:i-1234567890abcdef0\" OpenStack Nova Fencing # Create OpenStack fence device pcs stonith create fence-openstack \\ fence_openstack \\ auth-url=http://192.168.1.200:5000/v3 \\ username=admin \\ password=AdminPass123 \\ project-id=service \\ domain-id=default \\ region-name=RegionOne # Map instances pcs stonith create fence-openstack-node1 \\ fence_openstack \\ auth-url=http://192.168.1.200:5000/v3 \\ username=admin \\ password=AdminPass123 \\ project-id=service \\ domain-id=default \\ region-name=RegionOne \\ pcmk_host_map=\"node1:server-1-uuid\" Best Practices Always enable STONITH in production Use redundant fencing with multiple levels Test fencing devices regularly Monitor fencing operations Use appropriate timeout values Secure fence credentials Document fencing configuration Use fence device monitoring Test fencing scenarios Have manual fencing procedure ready Fencing Strategies IPMI + VMware # Primary: IPMI (fast) pcs stonith level add 1 node1 fence-ipmi-node1 # Secondary: VMware (reliable) pcs stonith level add 2 node1 fence-vmware iLO + DRAC # Primary: iLO pcs stonith level add 1 node1 fence-ilo-node1 # Secondary: DRAC pcs stonith level add 2 node1 fence-drac-node1 PDU + Network # Primary: PDU (power off) pcs stonith level add 1 node1 fence-apc-pdu # Secondary: Network switch (disconnect) pcs stonith level add 2 node1 fence-switch Security Secure Fence Credentials # Use Pacemaker CIB secrets pcs resource secret set fence-ipmi-node1 passwd # Or use environment variables export FENCE_PASSWD=\"AdminPass123\" pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=\"${FENCE_PASSWD}\" Network Security # Use dedicated network for fencing ip route add 192.168.100.0/24 dev eth1 # Use firewall rules iptables -A INPUT -s 192.168.100.0/24 -p tcp --dport 623 -j ACCEPT iptables -A INPUT -p tcp --dport 623 -j DROP # Use IPsec for fencing traffic ip xfrm state add src 192.168.100.0/24 dst 192.168.1.0/24 proto esp spi 0x200 mode transport reqid 1000 Monitoring Fencing Alerts # Monitor fencing events crm_mon --one-shot --as-xml | grep stonith # Monitor fence device status pcs stonith show --full # Monitor fencing logs tail -f /var/log/pacemaker/pacemaker.log | grep stonith Fencing Metrics # Check fence device performance pcs resource show fence-ipmi-node1 # Check fencing history crm_history --show fence-ipmi-node1 # Check fencing statistics pcs status resources | grep fence Fencing Simulation # Simulate fencing (without actually fencing) pcs stonith fence node1 --dry-run # Simulate fencing failure pcs stonith fence node1 --timeout 1 # Simulate network partition iptables -A INPUT -s 192.168.1.0/24 -j DROP Recovery Procedures Manual Fencing # Manually fence node via IPMI ipmitool -I lanplus -H 192.168.1.201 -U admin -P AdminPass123 \\ power cycle # Manually fence node via iLO curl -k -X POST https://192.168.1.202/rest/v1/Systems/1/Actions/ComputerSystem.Reset \\ -d '{\"ResetType\":\"ForceRestart\"}' \\ --user admin:AdminPass123 # Manually fence node via DRAC racadm -r 192.168.1.203 -u root -p AdminPass123 serveraction graceful Recovery After Fencing # Wait for node to reboot sleep 60 # Check cluster status pcs status # Verify node rejoined cluster pcs status nodes # Start resources on node pcs resource move apache node1 Source Code Fence Agents : https://github.com/ClusterLabs/fence-agents Pacemaker : https://github.com/ClusterLabs/pacemaker STONITH Documentation : https://www.clusterlabs.org/pacemaker/doc/2.0/Pacemaker_Explained/ Key Fence Agent Locations Agent Location Description fence_ipmilan agents/ipmilan/fence_ipmilan.py IPMI LAN fence_ilo agents/ilo/fence_ilo.py HP iLO fence_drac5 agents/drac5/fence_drac5.py Dell DRAC fence_vmware_soap agents/vmware_soap/fence_vmware_soap.py VMware vSphere fence_aws agents/aws/fence_aws.py Amazon AWS fence_apc agents/apc/fence_apc.py APC PDU","title":"Stonith"},{"location":"stonith/#overview","text":"STONITH is the fencing mechanism used by Pacemaker to prevent split-brain scenarios by ensuring failed nodes are properly fenced. graph TB subgraph \"Cluster Layer\" A[Pacemaker] B[STONITH Resources] C[Quorum] end subgraph \"Fencing Layer\" D[Fence Agents] E[Fence Device] F[Network] end subgraph \"Hardware Layer\" G[IPMI/iLO/DRAC] H[Network Switches] I[PDU - Power Distribution Units] end subgraph \"Cloud Layer\" J[AWS EC2] K[OpenStack Nova] L[Azure VM] end A --> B A --> C B --> D D --> E E --> F E --> G E --> H E --> I E --> J E --> K E --> L style B fill:#c8e6c9 style D fill:#ffecb3 style G fill:#e1f5ff style J fill:#fff3e0","title":"Overview"},{"location":"stonith/#stonith-architecture","text":"","title":"STONITH Architecture"},{"location":"stonith/#fencing-process","text":"sequenceDiagram participant C as Cluster participant S as STONITH Daemon participant F as Fence Agent participant H as Hardware C->>S: Node failure detected S->>F: Initiate fence F->>H: Send fence command H->>H: Execute fence (reboot/poweroff) H->>F: Fence complete F->>S: Fence success S->>C: Node fenced C->>C: Recover resources style S fill:#c8e6c9 style F fill:#ffecb3 style H fill:#e1f5ff","title":"Fencing Process"},{"location":"stonith/#stonith-components","text":"Component Description STONITH Daemon Manages fencing operations Fence Agents Scripts for fencing devices Fence Resources Cluster resources for fencing Fence Levels Primary/secondary fencing Fence Methods Multiple fencing strategies","title":"STONITH Components"},{"location":"stonith/#fence-agent-types","text":"","title":"Fence Agent Types"},{"location":"stonith/#ipmi-fence-agents","text":"fence_ipmilan : Intel IPMI LAN interface # Create IPMI fence device pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 \\ lanplus=1 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | IPMI IP address | - | | login | IPMI username | - | | passwd | IPMI password | - | | port | IPMI port | 623 | | lanplus | Use IPMIv2.0 | 0 | | action | Fencing action (reboot/off/on) | reboot | | timeout | Timeout in seconds | 20 | | cipher | Cipher suite (0-15) | 17 |","title":"IPMI Fence Agents"},{"location":"stonith/#ilo-fence-agents","text":"fence_ilo : HP iLO fencing # Create iLO fence device pcs stonith create fence-ilo-node1 \\ fence_ilo \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | iLO IP address | - | | login | iLO username | - | | passwd | iLO password | - | | ssl | Use SSL | 1 | | port | iLO port | 443 | | action | Fencing action | reboot | | timeout | Timeout in seconds | 20 |","title":"iLO Fence Agents"},{"location":"stonith/#drac-fence-agents","text":"fence_drac5 : Dell DRAC 5 fencing # Create DRAC fence device pcs stonith create fence-drac-node1 \\ fence_drac5 \\ ipaddr=192.168.1.203 \\ login=root \\ passwd=AdminPass123 \\ module=1 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | DRAC IP address | - | | login | DRAC username | - | | passwd | DRAC password | - | | module | DRAC module number | 1 | | cmd_prompt | Command prompt | ([A-Za-z0-9])+\\s*>? | | action | Fencing action | reboot |","title":"DRAC Fence Agents"},{"location":"stonith/#vmware-fence-agents","text":"fence_vmware_soap : VMware vSphere fencing # Create VMware fence device pcs stonith create fence-vmware \\ fence_vmware_soap \\ ipaddr=192.168.1.210 \\ login=administrator \\ passwd=AdminPass123 \\ ssl=1 \\ ipport=443 \\ action=reboot \\ pcmk_host_list=\"node1 node2 node3\" Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | vCenter IP address | - | | login | vCenter username | - | | passwd | vCenter password | - | | ssl | Use SSL | 1 | | ipport | vCenter port | 443 | | vm_name | VM name pattern | - | | action | Fencing action | reboot |","title":"VMware Fence Agents"},{"location":"stonith/#aws-fence-agents","text":"fence_aws : Amazon AWS fencing # Create AWS fence device pcs stonith create fence-aws \\ fence_aws \\ region=us-east-1 \\ access-key=AKIAIOSFODNN7EXAMPLE \\ secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\ action=reboot Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | region | AWS region | - | | access-key | AWS access key | - | | secret-key | AWS secret key | - | | profile | AWS profile | - | | action | Fencing action | reboot |","title":"AWS Fence Agents"},{"location":"stonith/#pdu-fence-agents","text":"fence_apc : APC PDU fencing # Create APC PDU fence device pcs stonith create fence-apc-pdu \\ fence_apc \\ ipaddr=192.168.1.220 \\ login=admin \\ passwd=AdminPass123 \\ port=1 \\ action=reboot \\ pcmk_host_list=node1 Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | PDU IP address | - | | login | PDU username | - | | passwd | PDU password | - | | port | Outlet number | - | | switch | Switch number | 1 | | action | Fencing action | reboot |","title":"PDU Fence Agents"},{"location":"stonith/#switch-fence-agents","text":"fence_ifmib : SNMP-based fencing # Create SNMP fence device pcs stonith create fence-switch \\ fence_ifmib \\ ipaddr=192.168.1.230 \\ community=public \\ action=reboot Parameters : | Parameter | Description | Default | |-----------|-------------|---------| | ipaddr | Switch IP address | - | | community | SNMP community | public | | port | Port number | - | | action | Fencing action | reboot |","title":"Switch Fence Agents"},{"location":"stonith/#stonith-configuration","text":"","title":"STONITH Configuration"},{"location":"stonith/#basic-configuration","text":"# Enable STONITH pcs property set stonith-enabled=true # Set STONITH timeout pcs property set stonith-timeout=60s # Set STONITH action pcs property set stonith-action=reboot # Set STONITH failure handling pcs property set stonith-failure-is-fatal=true","title":"Basic Configuration"},{"location":"stonith/#per-node-fencing","text":"# IPMI fencing for node1 pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node1 # IPMI fencing for node2 pcs stonith create fence-ipmi-node2 \\ fence_ipmilan \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node2 # IPMI fencing for node3 pcs stonith create fence-ipmi-node3 \\ fence_ipmilan \\ ipaddr=192.168.1.203 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node3","title":"Per-Node Fencing"},{"location":"stonith/#shared-fencing-device","text":"# Single fence device for all nodes pcs stonith create fence-vmware \\ fence_vmware_soap \\ ipaddr=192.168.1.210 \\ login=administrator \\ passwd=AdminPass123 \\ pcmk_host_list=\"node1 node2 node3\"","title":"Shared Fencing Device"},{"location":"stonith/#fence-levels","text":"# Primary fencing level (IPMI) pcs stonith level add 1 node1 fence-ipmi-node1 # Secondary fencing level (VMware) pcs stonith level add 2 node1 fence-vmware # View fence levels pcs stonith level # Remove fence level pcs stonith level remove 1 node1","title":"Fence Levels"},{"location":"stonith/#fence-methods","text":"# Define fence method pcs stonith create fence-ipmi-ilo \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node1 \\ op monitor interval=60s pcs stonith create fence-ilo-fallback \\ fence_ilo \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=AdminPass123 \\ pcmk_host_list=node1","title":"Fence Methods"},{"location":"stonith/#stonith-operations","text":"","title":"STONITH Operations"},{"location":"stonith/#testing-fencing","text":"# Test fencing manually fence_ipmilan -a 192.168.1.201 -l admin -p AdminPass123 \\ -o status # Test fencing via pcs pcs stonith fence node1 # Test fencing with verbose output pcs stonith fence node1 --verbose","title":"Testing Fencing"},{"location":"stonith/#monitoring-fencing","text":"# Check STONITH status pcs stonith status # Check STONITH devices pcs stonith show # Check fence device details pcs stonith show fence-ipmi-node1 # Check STONITH logs journalctl -u pacemaker -f | grep stonith","title":"Monitoring Fencing"},{"location":"stonith/#fence-device-management","text":"# Start fence device pcs resource start fence-ipmi-node1 # Stop fence device pcs resource stop fence-ipmi-node1 # Enable fence device pcs resource enable fence-ipmi-node1 # Disable fence device pcs resource disable fence-ipmi-node1 # Delete fence device pcs resource delete fence-ipmi-node1","title":"Fence Device Management"},{"location":"stonith/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"stonith/#stonith-properties","text":"Property Description Default stonith-enabled Enable/disable STONITH false stonith-timeout Timeout for fencing 60s stonith-action Default fencing action reboot stonith-failure-is-fatal Stop cluster on fence failure true stonith-watchdog-timeout Watchdog timeout 0 stonith-max-attempts Max fencing attempts 2","title":"STONITH Properties"},{"location":"stonith/#fence-device-attributes","text":"# Set timeout pcs resource update fence-ipmi-node1 \\ meta target-role=Started # Set monitoring interval pcs resource op add fence-ipmi-node1 \\ monitor interval=60s timeout=90s # Set resource stickiness pcs resource update fence-ipmi-node1 \\ meta resource-stickiness=0","title":"Fence Device Attributes"},{"location":"stonith/#fence-device-constraints","text":"# Colocate fencing with node pcs constraint colocation add fence-ipmi-node1 with node1 # Order fencing before resources pcs constraint order start fence-ipmi-node1 then apache # Location constraint pcs constraint location fence-ipmi-node1 prefers node1","title":"Fence Device Constraints"},{"location":"stonith/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"stonith/#fencing-failures","text":"Check fence device status : # Check STONITH status pcs stonith show # Check resource status pcs status resources # Check cluster status pcs status Test fence device manually : # Test IPMI fencing fence_ipmilan -a 192.168.1.201 -l admin -p AdminPass123 \\ -o status # Test iLO fencing fence_ilo -a 192.168.1.202 -l admin -p AdminPass123 \\ -o status # Test DRAC fencing fence_drac5 -a 192.168.1.203 -l root -p AdminPass123 \\ -o status Check logs : # Check Pacemaker logs journalctl -u pacemaker -f # Check STONITH logs grep stonith /var/log/pacemaker/pacemaker.log # Check fence agent logs journalctl -f | grep fence","title":"Fencing Failures"},{"location":"stonith/#network-issues","text":"Check connectivity : # Ping fence device ping -c 3 192.168.1.201 # Check port accessibility telnet 192.168.1.201 623 nc -zv 192.168.1.201 623 Check firewall : # Check firewall rules iptables -L -n # Open IPMI port iptables -A INPUT -p tcp --dport 623 -j ACCEPT iptables -A INPUT -p udp --dport 623 -j ACCEPT # Check UFW (Ubuntu) ufw allow 623/tcp ufw allow 623/udp","title":"Network Issues"},{"location":"stonith/#authentication-issues","text":"Verify credentials : # Test IPMI credentials ipmitool -I lanplus -H 192.168.1.201 \\ -U admin -P AdminPass123 chassis status # Test iLO credentials curl -k https://192.168.1.202/ \\ --user admin:AdminPass123 # Test DRAC credentials racadm -r 192.168.1.203 -u root -p AdminPass123 \\ getinfo Check fence device configuration : # View fence device configuration pcs stonith show fence-ipmi-node1 # Check fence device status pcs resource show fence-ipmi-node1","title":"Authentication Issues"},{"location":"stonith/#cloud-fencing","text":"","title":"Cloud Fencing"},{"location":"stonith/#aws-ec2-fencing","text":"# Create AWS fence device pcs stonith create fence-aws \\ fence_aws \\ region=us-east-1 \\ access-key=AKIAIOSFODNN7EXAMPLE \\ secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # Map instances to fence device pcs stonith create fence-aws-node1 \\ fence_aws \\ region=us-east-1 \\ access-key=AKIAIOSFODNN7EXAMPLE \\ secret-key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\ pcmk_host_map=\"node1:i-1234567890abcdef0\"","title":"AWS EC2 Fencing"},{"location":"stonith/#openstack-nova-fencing","text":"# Create OpenStack fence device pcs stonith create fence-openstack \\ fence_openstack \\ auth-url=http://192.168.1.200:5000/v3 \\ username=admin \\ password=AdminPass123 \\ project-id=service \\ domain-id=default \\ region-name=RegionOne # Map instances pcs stonith create fence-openstack-node1 \\ fence_openstack \\ auth-url=http://192.168.1.200:5000/v3 \\ username=admin \\ password=AdminPass123 \\ project-id=service \\ domain-id=default \\ region-name=RegionOne \\ pcmk_host_map=\"node1:server-1-uuid\"","title":"OpenStack Nova Fencing"},{"location":"stonith/#best-practices","text":"Always enable STONITH in production Use redundant fencing with multiple levels Test fencing devices regularly Monitor fencing operations Use appropriate timeout values Secure fence credentials Document fencing configuration Use fence device monitoring Test fencing scenarios Have manual fencing procedure ready","title":"Best Practices"},{"location":"stonith/#fencing-strategies","text":"","title":"Fencing Strategies"},{"location":"stonith/#ipmi-vmware","text":"# Primary: IPMI (fast) pcs stonith level add 1 node1 fence-ipmi-node1 # Secondary: VMware (reliable) pcs stonith level add 2 node1 fence-vmware","title":"IPMI + VMware"},{"location":"stonith/#ilo-drac","text":"# Primary: iLO pcs stonith level add 1 node1 fence-ilo-node1 # Secondary: DRAC pcs stonith level add 2 node1 fence-drac-node1","title":"iLO + DRAC"},{"location":"stonith/#pdu-network","text":"# Primary: PDU (power off) pcs stonith level add 1 node1 fence-apc-pdu # Secondary: Network switch (disconnect) pcs stonith level add 2 node1 fence-switch","title":"PDU + Network"},{"location":"stonith/#security","text":"","title":"Security"},{"location":"stonith/#secure-fence-credentials","text":"# Use Pacemaker CIB secrets pcs resource secret set fence-ipmi-node1 passwd # Or use environment variables export FENCE_PASSWD=\"AdminPass123\" pcs stonith create fence-ipmi-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=\"${FENCE_PASSWD}\"","title":"Secure Fence Credentials"},{"location":"stonith/#network-security","text":"# Use dedicated network for fencing ip route add 192.168.100.0/24 dev eth1 # Use firewall rules iptables -A INPUT -s 192.168.100.0/24 -p tcp --dport 623 -j ACCEPT iptables -A INPUT -p tcp --dport 623 -j DROP # Use IPsec for fencing traffic ip xfrm state add src 192.168.100.0/24 dst 192.168.1.0/24 proto esp spi 0x200 mode transport reqid 1000","title":"Network Security"},{"location":"stonith/#monitoring","text":"","title":"Monitoring"},{"location":"stonith/#fencing-alerts","text":"# Monitor fencing events crm_mon --one-shot --as-xml | grep stonith # Monitor fence device status pcs stonith show --full # Monitor fencing logs tail -f /var/log/pacemaker/pacemaker.log | grep stonith","title":"Fencing Alerts"},{"location":"stonith/#fencing-metrics","text":"# Check fence device performance pcs resource show fence-ipmi-node1 # Check fencing history crm_history --show fence-ipmi-node1 # Check fencing statistics pcs status resources | grep fence","title":"Fencing Metrics"},{"location":"stonith/#fencing-simulation","text":"# Simulate fencing (without actually fencing) pcs stonith fence node1 --dry-run # Simulate fencing failure pcs stonith fence node1 --timeout 1 # Simulate network partition iptables -A INPUT -s 192.168.1.0/24 -j DROP","title":"Fencing Simulation"},{"location":"stonith/#recovery-procedures","text":"","title":"Recovery Procedures"},{"location":"stonith/#manual-fencing","text":"# Manually fence node via IPMI ipmitool -I lanplus -H 192.168.1.201 -U admin -P AdminPass123 \\ power cycle # Manually fence node via iLO curl -k -X POST https://192.168.1.202/rest/v1/Systems/1/Actions/ComputerSystem.Reset \\ -d '{\"ResetType\":\"ForceRestart\"}' \\ --user admin:AdminPass123 # Manually fence node via DRAC racadm -r 192.168.1.203 -u root -p AdminPass123 serveraction graceful","title":"Manual Fencing"},{"location":"stonith/#recovery-after-fencing","text":"# Wait for node to reboot sleep 60 # Check cluster status pcs status # Verify node rejoined cluster pcs status nodes # Start resources on node pcs resource move apache node1","title":"Recovery After Fencing"},{"location":"stonith/#source-code","text":"Fence Agents : https://github.com/ClusterLabs/fence-agents Pacemaker : https://github.com/ClusterLabs/pacemaker STONITH Documentation : https://www.clusterlabs.org/pacemaker/doc/2.0/Pacemaker_Explained/","title":"Source Code"},{"location":"stonith/#key-fence-agent-locations","text":"Agent Location Description fence_ipmilan agents/ipmilan/fence_ipmilan.py IPMI LAN fence_ilo agents/ilo/fence_ilo.py HP iLO fence_drac5 agents/drac5/fence_drac5.py Dell DRAC fence_vmware_soap agents/vmware_soap/fence_vmware_soap.py VMware vSphere fence_aws agents/aws/fence_aws.py Amazon AWS fence_apc agents/apc/fence_apc.py APC PDU","title":"Key Fence Agent Locations"},{"location":"troubleshooting-code-behavior/","text":"Troubleshooting Code Behavior and Diagnostics This section provides comprehensive guidance on diagnosing and resolving issues with clustering, virtualization, and storage technologies from a code behavior perspective. Debugging Techniques Enable Debug Logging Corosync: # Enable debug mode in corosync.conf logging { to_logfile: yes logfile: /var/log/corosync/corosync.log debug: on debug_logfile: /var/log/corosync/debug.log } # Or use cormapctl corosync-cmapctl totem.debug = 1 corosync-cmapctl totem.logging_debug = 1 Pacemaker: # Enable verbose logging crm_simulate --live-check crm_simulate --show-failcounts crm_simulate --showscores # Check detailed resource actions pcs resource show-status # Monitor CIB changes crm_mon --show-detail crm_mon --show-node-attributes QEMU/KVM: # Enable QEMU debug output qemu-system-x86_64 -d in_asm,op,exec,cpu_reset,guest_errors,int,mmu # QMP debugging qemu-system-x86_64 -qmp stdio -monitor stdio # Enable KVM debug echo 1 > /sys/module/kvm/parameters/debug echo 1 > /sys/module/kvm_intel/parameters/debug echo 1 > /sys/module/kvm_amd/parameters/debug Ceph: # Enable debug logging ceph config set global debug_msgr 10 ceph config set mon debug_mon 10 # Enable object logging ceph config set global debug_filestore 20 ceph config set global debug_rados 20 # Monitor OSD operations ceph tell osd.0 dump_ops Common Code-Level Issues Corosync Issues 1. Token Circulation Problems Symptoms: - Nodes unable to send messages - Token stuck on one node - Messages delayed excessively Causes: - Network partition - Token timeout too short - Network buffer overflow Diagnosis: corosync-cfgtool -s corosync-quorumtool -s Check token status: corosync-cmapctl runtime.totem.token Check ring state: corosync-cmapctl runtime.totem.token_hold_time Solution: Increase token timeout totem { token: 10000 # Increase from default 10000 token_retransmit: 500 } Reduce network traffic totem { window_size: 50 # Reduce from default 100 max_messages: 17 } Check network configuration ping -c 3 tcpdump -i any 'port 5405' **2. Quorum Failures** Symptoms: - Cluster loses quorum - Nodes operate independently - Resources stop responding Diagnosis: corosync-quorumtool -s Check expected vs actual votes Check node connectivity Solution: # Verify network connectivity ping -c 5 <all-cluster-nodes> # Check Corosync communication corosync-cfgtool -s # Adjust quorum settings nodelist { node { quorum_votes: 3 } } # Ensure proper node count # Use odd number of nodes for majority Pacemaker Issues 1. Resource Start Failures Symptoms: - Resources stuck in \"Starting\" state - Resources fail to start repeatedly - CRM crashes or restarts Diagnosis: pcs status Check resource status: pcs resource show-status resource-name Check CRM logs: journalctl -u pacemaker -n 50 | less Check transition history: crm_history --show Solution: Increase operation timeout pcs resource op add resource-name start \\ timeout=60s interval=0s on-fail=restart Check resource agent pcs resource debug-resource resource-name crm_resource --validate --resource resource-name Check for configuration errors crm_verify -L -V **2. Constraint Violations** Symptoms: - Resources not placed according to constraints - Unexpected resource placement - Colocation not respected Diagnosis: pcs constraint show --full Check constraint scores: crm_simulate --showscores Verify constraint definitions: pcs constraint order list pcs constraint colocation list Solution: # Verify constraints are correct pcs constraint order list pcs constraint colocation list # Adjust constraint scores pcs constraint order set order-id \\ symmetrical=false # Clear broken constraints pcs constraint remove constraint-id # Recalculate cluster state pcs resource cleanup 3. Resource Agent Bugs Symptoms: - OCF agent returns invalid status codes - Agent timeout during start/stop - Agent crashes Diagnosis: # Test agent manually ocf_resource:ocf:heartbeat:IPaddr2 monitor # Check agent logs grep \"resource-name\" /var/log/pacemaker/pengine/* # Validate agent syntax crm_resource --validate --resource resource-name Solution: Test agent manually export OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat /usr/lib/ocf/resource.d/heartbeat/IPaddr2 monitor OCF_RESKEY=OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat Update agent operation timeout pcs resource op add resource-name start \\ timeout=120s Monitor agent behavior pcs resource op monitor resource-name \\ timeout=30s interval=10s Use standard agents if possible Ensure OCF-compliant agents #### QEMU/KVM Issues **1. VM Startup Failures** Symptoms: - VM fails to boot - Kernel panic in guest - No console output Diagnosis: qemu-system-x86_64 -d in_asm,cpu_reset Check VM configuration virsh dumpxml vm-name Monitor QMP events echo '{\"execute\":\"qmp_capabilities\"}' | \\ socat UNIX-CONNECT:/var/run/libvirt/qemu/vm-name.monitor Solution: # Simplify VM configuration virt-install --name vm1 --memory 1024 --vcpus 1 # Disable problematic features qemu-system-x86_64 -no-hpet -no-acpi # Use different machine type virt-install --name vm1 --machine pc # Check disk image integrity qemu-img check disk.qcow2 # Enable serial console qemu-system-x86_64 -serial pty -monitor stdio 2. Performance Issues Symptoms: - VM runs slowly - High CPU usage on host - Poor I/O performance Diagnosis: # Monitor host CPU top -H -p qemu-system-x86_64 # Check KVM configuration lsmod | grep kvm # Monitor VM internals virsh qemu-monitor-command vm-name \"info status\" # Check disk I/O iotop -o -a # Check network I/O iftop -i br0 Solution: Enable KVM qemu-system-x86_64 -enable-kvm Use virtio drivers virt-install --disk bus=virtio --network model=virtio Enable virtio multiqueue virt-install --cpu host-passthrough,cache=writeback Tune scheduler echo 1 > /sys/module/kvm/parameters/lapic **3. Memory Issues** Symptoms: - Out of memory errors - Swap usage high - Memory leak in QEMU process Diagnosis: Check host memory free -h Check VM memory virsh dommemstat vm-name Monitor KVM memory cat /sys/module/kvm/parameters/hugepages Check QEMU memory ps -o pid,vsz,pmem -C qemu-system-x86_64 Check for ballooning virsh dominfo vm-name | grep balloon Solution: # Enable memory ballooning virt-install --balloon virtio # Enable hugepages echo 1 > /sys/kernel/mm/hugepages/hugepages-2048kB echo 1024 > /proc/sys/vm/nr_hugepages # Tune memory allocation virt-install --memory 1024 --memballoc=interleave # Limit VM memory virsh setmem vm-name 512M # Disable unused features virt-install --no-usb --no-sound Ceph Issues 1. OSD Performance Problems Symptoms: - Slow I/O operations - High latency - OSD rebalancing constantly Diagnosis: ceph tell osd.* iostat ceph osd perf # Check OSD stats ceph osd dump_ops osd.0 # Check PG states ceph pg dump # Monitor recovery ceph -w Solution: Tune OSD configuration ceph config set osd osd_op_threads 2 ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3 Enable BlueStore ceph config set osd osd_objectstore bluestore Tune memory ceph config set osd osd_memory_target 4G Adjust max open files ceph config set osd max_open_files 4096 **2. PG Distribution Issues** Symptoms: - CRUSH misplacement - Uneven data distribution - High latency on specific OSDs Diagnosis: ceph osd tree ceph pg dump Check CRUSH map ceph osd getcrushmap -o crush.map Analyze PG distribution ceph pg dump | grep pg_num Check OSD utilization ceph osd perf Solution: # Adjust PG count ceph osd pool set <pool-name> pg_num <new-pg-count> # Rebalance cluster ceph osd reweight-by-utilization # Check CRUSH rules ceph osd getcrushmap -o crush.map crushtool -i crush.map --test --num-rep 3 --rules <rule-id> # Adjust CRUSH tunables ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3 3. Network Partition Issues Symptoms: - OSDs marked down - Network timeouts - Split-brain scenario Diagnosis: ceph -s ceph quorum_status # Check network connectivity ping -c 3 <mon-host> # Monitor OSD status ceph osd tree # Check Corosync status corosync-cfgtool -s Solution: Adjust network timeouts totem { token: 10000 # Increase timeout join: 120 } Enable redundant network totem { interface { ringnumber: 1 bindnetaddr: 192.168.2.10 mcastport: 5405 } } Check firewall iptables -L -n corosync Use dedicated network Separate cluster and public networks ### Performance Analysis #### Profiling Techniques **Corosync:** ```bash # Profile with perf perf record -e cycles,instructions -g -o perf.data corosync # Analyze token circulation perf report -i perf.data -s token # Profile memory usage perf record -e cycles -g -o mem.data corosync Pacemaker: # Profile PE calculations time crm_simulate --simulate > /dev/null # Monitor PE frequency grep \"calculated transition\" /var/log/pacemaker/pengine/* QEMU: # Profile QEMU perf record -e cycles,instructions,cache-misses -g -o qemu.perf \\ qemu-system-x86_64 ... # Profile KVM operations perf record -e cycles,instructions,kvm:kvm_exit,kvm:kvm_entry -g -o kvm.perf \\ qemu-system-x86_64 -enable-kvm ... Ceph: # Profile OSD perf record -e cycles,instructions,cache-misses -g -o osd.perf \\ -p <osd-pid> # Profile network perf record -e cycles,instructions,kvm:kvm_exit -g -o network.perf \\ tcpdump -i any 'port 6800' # Profile I/O iostat -x 5 -d -c <rados-device> Memory Analysis Detecting Memory Leaks: # Monitor process memory watch -n 1 'ps -o pid,vsz,pmem -C corosync' # Check for memory growth ps -o pid,rss,vsz -C pacemaker \\ | awk '{print $2}' | sort -n # Check for unfreed objects valgrind --leak-check=full ./corosync # Analyze heap usage gdb -p corosync -batch -ex \"bt\" Concurrency Issues Pacemaker: # Monitor concurrent operations crm_mon --show-detail # Check for deadlocks grep \"deadlock\" /var/log/pacemaker/* # Monitor transition queue crm_simulate --show-failcounts Ceph: # Check for lock contention ceph daemon --admin socket tell osd.0 dump_historic_ops # Monitor thread pool ceph tell osd.0 dump_ops_in_flight # Check recovery activity ceph -w Log Analysis Understanding Log Levels Corosync: - DEBUG : Detailed diagnostic information - INFO : General informational messages - WARN : Warning conditions - ERROR : Error conditions Pacemaker: - notice : Information about cluster events - warning : Potential issues - error : Error conditions - crit : Critical failures Ceph: - debug/10 : Detailed information (level 10) - debug/20 : More information - info : General messages - warn : Warning messages - err : Error messages Log Locations Corosync: /var/log/corosync/corosync.log /var/log/corosync/debug.log /var/log/syslog Pacemaker: /var/log/pacemaker/pengine/* /var/log/pacemaker/crmd/* /var/log/pacemaker/attrd/* Ceph: /var/log/ceph/ceph.log /var/log/ceph/ceph-mon.log /var/log/ceph/ceph-osd.*.log Log Analysis Commands Corosync: # Token analysis grep \"token\" /var/log/corosync/corosync.log # Quorum events grep \"quorum\" /var/log/corosync/corosync.log # Network issues grep \"network\" /var/log/corosync/corosync.log Pacemaker: # Find failed resources grep \"FAILED\" /var/log/pacemaker/pengine/* # Analyze transitions grep \"calculated transition\" /var/log/pacemaker/pengine/* # Check for errors grep \"error\" /var/log/pacemaker/* Ceph: # OSD crashes grep \"segfault\" /var/log/ceph/ceph-osd.*.log # Slow requests grep \"slow request\" /var/log/ceph/ceph-osd.*.log # Network issues grep \"timed out\" /var/log/ceph/ceph-osd.*.log Configuration Problems Validation and Testing Before Applying Changes: # Validate Pacemaker configuration crm_verify -L -V # Test with simulation crm_simulate --live-check # Dry-run Ceph commands ceph tell osd.* injectargs --dry-run # Test iSCSI configuration gwcli.py target list Rollback Procedures: # Pacemaker snapshot cibadmin --query --local > cib-backup.xml # Rollback if needed cibadmin --replace --local --xml-file cib-backup.xml # Restore Ceph configuration ceph config set <option> <previous-value> Integration Issues Cluster Coordination Corosync and Pacemaker: # Check Corosync status from Pacemaker crm_resource --list # Verify quorum crm_attribute -q -n quorum # Check node status pcs status nodes # Ensure Pacemaker can communicate crm_mon --show-detail Ceph with Cluster Managers: # Check if cluster manager sees Ceph ceph -s # Verify network connectivity ping <mon-host> # Check monitor status ceph -w Storage Integration iSCSI Issues: # Check gateway status gwcli.py target list # Check initiator connections iscsiadm -m session # Check multipath status multipath -ll # Reset initiator state iscsiadm -m node logout -T <target-iqn> -p <portal-ip> iscsiadm -m node login -T <target-iqn> -p <portal-ip> --login RBD Issues: # Check mapped images rbd showmapped # Check image status rbd info <pool>/<image> # Unmap and remap rbd unmap <pool>/<image> rbd map <pool>/<image> # Check for stale mappings dmesg | grep rbd Advanced Diagnostics System-Level Debugging # Enable kernel debugging echo 1 > /proc/sys/kernel/sched_sched_min_granularity echo 1 > /proc/sys/kernel/khung_task_timeout_secs # Enable KVM tracing echo 1 > /sys/module/kvm/parameters/trace_gue # Enable Corosync tracing corosync-cmapctl totem.trace_buffer_size 1000000 # Monitor system resources top -b -H -d 2 vmstat 1 iostat -x 2 Network Diagnostics # Check network latency ping -i 3 -s 64 <target-host> # Trace routes traceroute -n <target-host> # Check bandwidth iperf -c 3 -t 10 <target-host> # Capture packets tcpdump -i any -w /tmp/capture.pcap host <target-ip> Best Practices for Troubleshooting Always start with debug enabled when investigating issues Collect logs before clearing them - keep a complete copy Document the issue - note exact error messages, timestamps Test in isolated environment if possible Check configuration drift - ensure all nodes have same configuration Monitor system resources - CPU, memory, disk I/O, network Use incremental debugging - narrow down the problem step by step Have rollback plan - know how to quickly revert changes Additional Resources Corosync Wiki : https://github.com/corosync/corosync/wiki/Troubleshooting Pacemaker Guide : https://www.clusterlabs.org/pacemaker/doc/2.0/Pacemaker_Explained/ Ceph Troubleshooting : https://docs.ceph.com/en/latest/rados/troubleshooting/ QEMU Debugging : https://wiki.qemu.org/Documentation/Debugging/ Linux Performance : https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/tuning_guide/","title":"Troubleshooting code behavior"},{"location":"troubleshooting-code-behavior/#troubleshooting-code-behavior-and-diagnostics","text":"This section provides comprehensive guidance on diagnosing and resolving issues with clustering, virtualization, and storage technologies from a code behavior perspective.","title":"Troubleshooting Code Behavior and Diagnostics"},{"location":"troubleshooting-code-behavior/#debugging-techniques","text":"","title":"Debugging Techniques"},{"location":"troubleshooting-code-behavior/#enable-debug-logging","text":"Corosync: # Enable debug mode in corosync.conf logging { to_logfile: yes logfile: /var/log/corosync/corosync.log debug: on debug_logfile: /var/log/corosync/debug.log } # Or use cormapctl corosync-cmapctl totem.debug = 1 corosync-cmapctl totem.logging_debug = 1 Pacemaker: # Enable verbose logging crm_simulate --live-check crm_simulate --show-failcounts crm_simulate --showscores # Check detailed resource actions pcs resource show-status # Monitor CIB changes crm_mon --show-detail crm_mon --show-node-attributes QEMU/KVM: # Enable QEMU debug output qemu-system-x86_64 -d in_asm,op,exec,cpu_reset,guest_errors,int,mmu # QMP debugging qemu-system-x86_64 -qmp stdio -monitor stdio # Enable KVM debug echo 1 > /sys/module/kvm/parameters/debug echo 1 > /sys/module/kvm_intel/parameters/debug echo 1 > /sys/module/kvm_amd/parameters/debug Ceph: # Enable debug logging ceph config set global debug_msgr 10 ceph config set mon debug_mon 10 # Enable object logging ceph config set global debug_filestore 20 ceph config set global debug_rados 20 # Monitor OSD operations ceph tell osd.0 dump_ops","title":"Enable Debug Logging"},{"location":"troubleshooting-code-behavior/#common-code-level-issues","text":"","title":"Common Code-Level Issues"},{"location":"troubleshooting-code-behavior/#corosync-issues","text":"1. Token Circulation Problems Symptoms: - Nodes unable to send messages - Token stuck on one node - Messages delayed excessively Causes: - Network partition - Token timeout too short - Network buffer overflow Diagnosis: corosync-cfgtool -s corosync-quorumtool -s Check token status: corosync-cmapctl runtime.totem.token Check ring state: corosync-cmapctl runtime.totem.token_hold_time Solution:","title":"Corosync Issues"},{"location":"troubleshooting-code-behavior/#increase-token-timeout","text":"totem { token: 10000 # Increase from default 10000 token_retransmit: 500 }","title":"Increase token timeout"},{"location":"troubleshooting-code-behavior/#reduce-network-traffic","text":"totem { window_size: 50 # Reduce from default 100 max_messages: 17 }","title":"Reduce network traffic"},{"location":"troubleshooting-code-behavior/#check-network-configuration","text":"ping -c 3 tcpdump -i any 'port 5405' **2. Quorum Failures** Symptoms: - Cluster loses quorum - Nodes operate independently - Resources stop responding Diagnosis: corosync-quorumtool -s Check expected vs actual votes Check node connectivity Solution: # Verify network connectivity ping -c 5 <all-cluster-nodes> # Check Corosync communication corosync-cfgtool -s # Adjust quorum settings nodelist { node { quorum_votes: 3 } } # Ensure proper node count # Use odd number of nodes for majority","title":"Check network configuration"},{"location":"troubleshooting-code-behavior/#pacemaker-issues","text":"1. Resource Start Failures Symptoms: - Resources stuck in \"Starting\" state - Resources fail to start repeatedly - CRM crashes or restarts Diagnosis: pcs status Check resource status: pcs resource show-status resource-name Check CRM logs: journalctl -u pacemaker -n 50 | less Check transition history: crm_history --show Solution:","title":"Pacemaker Issues"},{"location":"troubleshooting-code-behavior/#increase-operation-timeout","text":"pcs resource op add resource-name start \\ timeout=60s interval=0s on-fail=restart","title":"Increase operation timeout"},{"location":"troubleshooting-code-behavior/#check-resource-agent","text":"pcs resource debug-resource resource-name crm_resource --validate --resource resource-name","title":"Check resource agent"},{"location":"troubleshooting-code-behavior/#check-for-configuration-errors","text":"crm_verify -L -V **2. Constraint Violations** Symptoms: - Resources not placed according to constraints - Unexpected resource placement - Colocation not respected Diagnosis: pcs constraint show --full Check constraint scores: crm_simulate --showscores Verify constraint definitions: pcs constraint order list pcs constraint colocation list Solution: # Verify constraints are correct pcs constraint order list pcs constraint colocation list # Adjust constraint scores pcs constraint order set order-id \\ symmetrical=false # Clear broken constraints pcs constraint remove constraint-id # Recalculate cluster state pcs resource cleanup 3. Resource Agent Bugs Symptoms: - OCF agent returns invalid status codes - Agent timeout during start/stop - Agent crashes Diagnosis: # Test agent manually ocf_resource:ocf:heartbeat:IPaddr2 monitor # Check agent logs grep \"resource-name\" /var/log/pacemaker/pengine/* # Validate agent syntax crm_resource --validate --resource resource-name Solution:","title":"Check for configuration errors"},{"location":"troubleshooting-code-behavior/#test-agent-manually","text":"export OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat /usr/lib/ocf/resource.d/heartbeat/IPaddr2 monitor OCF_RESKEY=OCF_ROOT=/usr/lib/ocf/resource.d/heartbeat","title":"Test agent manually"},{"location":"troubleshooting-code-behavior/#update-agent-operation-timeout","text":"pcs resource op add resource-name start \\ timeout=120s","title":"Update agent operation timeout"},{"location":"troubleshooting-code-behavior/#monitor-agent-behavior","text":"pcs resource op monitor resource-name \\ timeout=30s interval=10s","title":"Monitor agent behavior"},{"location":"troubleshooting-code-behavior/#use-standard-agents-if-possible","text":"","title":"Use standard agents if possible"},{"location":"troubleshooting-code-behavior/#ensure-ocf-compliant-agents","text":"#### QEMU/KVM Issues **1. VM Startup Failures** Symptoms: - VM fails to boot - Kernel panic in guest - No console output Diagnosis: qemu-system-x86_64 -d in_asm,cpu_reset","title":"Ensure OCF-compliant agents"},{"location":"troubleshooting-code-behavior/#check-vm-configuration","text":"virsh dumpxml vm-name","title":"Check VM configuration"},{"location":"troubleshooting-code-behavior/#monitor-qmp-events","text":"echo '{\"execute\":\"qmp_capabilities\"}' | \\ socat UNIX-CONNECT:/var/run/libvirt/qemu/vm-name.monitor Solution: # Simplify VM configuration virt-install --name vm1 --memory 1024 --vcpus 1 # Disable problematic features qemu-system-x86_64 -no-hpet -no-acpi # Use different machine type virt-install --name vm1 --machine pc # Check disk image integrity qemu-img check disk.qcow2 # Enable serial console qemu-system-x86_64 -serial pty -monitor stdio 2. Performance Issues Symptoms: - VM runs slowly - High CPU usage on host - Poor I/O performance Diagnosis: # Monitor host CPU top -H -p qemu-system-x86_64 # Check KVM configuration lsmod | grep kvm # Monitor VM internals virsh qemu-monitor-command vm-name \"info status\" # Check disk I/O iotop -o -a # Check network I/O iftop -i br0 Solution:","title":"Monitor QMP events"},{"location":"troubleshooting-code-behavior/#enable-kvm","text":"qemu-system-x86_64 -enable-kvm","title":"Enable KVM"},{"location":"troubleshooting-code-behavior/#use-virtio-drivers","text":"virt-install --disk bus=virtio --network model=virtio","title":"Use virtio drivers"},{"location":"troubleshooting-code-behavior/#enable-virtio-multiqueue","text":"virt-install --cpu host-passthrough,cache=writeback","title":"Enable virtio multiqueue"},{"location":"troubleshooting-code-behavior/#tune-scheduler","text":"echo 1 > /sys/module/kvm/parameters/lapic **3. Memory Issues** Symptoms: - Out of memory errors - Swap usage high - Memory leak in QEMU process Diagnosis:","title":"Tune scheduler"},{"location":"troubleshooting-code-behavior/#check-host-memory","text":"free -h","title":"Check host memory"},{"location":"troubleshooting-code-behavior/#check-vm-memory","text":"virsh dommemstat vm-name","title":"Check VM memory"},{"location":"troubleshooting-code-behavior/#monitor-kvm-memory","text":"cat /sys/module/kvm/parameters/hugepages","title":"Monitor KVM memory"},{"location":"troubleshooting-code-behavior/#check-qemu-memory","text":"ps -o pid,vsz,pmem -C qemu-system-x86_64","title":"Check QEMU memory"},{"location":"troubleshooting-code-behavior/#check-for-ballooning","text":"virsh dominfo vm-name | grep balloon Solution: # Enable memory ballooning virt-install --balloon virtio # Enable hugepages echo 1 > /sys/kernel/mm/hugepages/hugepages-2048kB echo 1024 > /proc/sys/vm/nr_hugepages # Tune memory allocation virt-install --memory 1024 --memballoc=interleave # Limit VM memory virsh setmem vm-name 512M # Disable unused features virt-install --no-usb --no-sound","title":"Check for ballooning"},{"location":"troubleshooting-code-behavior/#ceph-issues","text":"1. OSD Performance Problems Symptoms: - Slow I/O operations - High latency - OSD rebalancing constantly Diagnosis: ceph tell osd.* iostat ceph osd perf # Check OSD stats ceph osd dump_ops osd.0 # Check PG states ceph pg dump # Monitor recovery ceph -w Solution:","title":"Ceph Issues"},{"location":"troubleshooting-code-behavior/#tune-osd-configuration","text":"ceph config set osd osd_op_threads 2 ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3","title":"Tune OSD configuration"},{"location":"troubleshooting-code-behavior/#enable-bluestore","text":"ceph config set osd osd_objectstore bluestore","title":"Enable BlueStore"},{"location":"troubleshooting-code-behavior/#tune-memory","text":"ceph config set osd osd_memory_target 4G","title":"Tune memory"},{"location":"troubleshooting-code-behavior/#adjust-max-open-files","text":"ceph config set osd max_open_files 4096 **2. PG Distribution Issues** Symptoms: - CRUSH misplacement - Uneven data distribution - High latency on specific OSDs Diagnosis: ceph osd tree ceph pg dump","title":"Adjust max open files"},{"location":"troubleshooting-code-behavior/#check-crush-map","text":"ceph osd getcrushmap -o crush.map","title":"Check CRUSH map"},{"location":"troubleshooting-code-behavior/#analyze-pg-distribution","text":"ceph pg dump | grep pg_num","title":"Analyze PG distribution"},{"location":"troubleshooting-code-behavior/#check-osd-utilization","text":"ceph osd perf Solution: # Adjust PG count ceph osd pool set <pool-name> pg_num <new-pg-count> # Rebalance cluster ceph osd reweight-by-utilization # Check CRUSH rules ceph osd getcrushmap -o crush.map crushtool -i crush.map --test --num-rep 3 --rules <rule-id> # Adjust CRUSH tunables ceph config set osd osd_max_backfills 1 ceph config set osd osd_recovery_max_active 3 3. Network Partition Issues Symptoms: - OSDs marked down - Network timeouts - Split-brain scenario Diagnosis: ceph -s ceph quorum_status # Check network connectivity ping -c 3 <mon-host> # Monitor OSD status ceph osd tree # Check Corosync status corosync-cfgtool -s Solution:","title":"Check OSD utilization"},{"location":"troubleshooting-code-behavior/#adjust-network-timeouts","text":"totem { token: 10000 # Increase timeout join: 120 }","title":"Adjust network timeouts"},{"location":"troubleshooting-code-behavior/#enable-redundant-network","text":"totem { interface { ringnumber: 1 bindnetaddr: 192.168.2.10 mcastport: 5405 } }","title":"Enable redundant network"},{"location":"troubleshooting-code-behavior/#check-firewall","text":"iptables -L -n corosync","title":"Check firewall"},{"location":"troubleshooting-code-behavior/#use-dedicated-network","text":"","title":"Use dedicated network"},{"location":"troubleshooting-code-behavior/#separate-cluster-and-public-networks","text":"### Performance Analysis #### Profiling Techniques **Corosync:** ```bash # Profile with perf perf record -e cycles,instructions -g -o perf.data corosync # Analyze token circulation perf report -i perf.data -s token # Profile memory usage perf record -e cycles -g -o mem.data corosync Pacemaker: # Profile PE calculations time crm_simulate --simulate > /dev/null # Monitor PE frequency grep \"calculated transition\" /var/log/pacemaker/pengine/* QEMU: # Profile QEMU perf record -e cycles,instructions,cache-misses -g -o qemu.perf \\ qemu-system-x86_64 ... # Profile KVM operations perf record -e cycles,instructions,kvm:kvm_exit,kvm:kvm_entry -g -o kvm.perf \\ qemu-system-x86_64 -enable-kvm ... Ceph: # Profile OSD perf record -e cycles,instructions,cache-misses -g -o osd.perf \\ -p <osd-pid> # Profile network perf record -e cycles,instructions,kvm:kvm_exit -g -o network.perf \\ tcpdump -i any 'port 6800' # Profile I/O iostat -x 5 -d -c <rados-device>","title":"Separate cluster and public networks"},{"location":"troubleshooting-code-behavior/#memory-analysis","text":"Detecting Memory Leaks: # Monitor process memory watch -n 1 'ps -o pid,vsz,pmem -C corosync' # Check for memory growth ps -o pid,rss,vsz -C pacemaker \\ | awk '{print $2}' | sort -n # Check for unfreed objects valgrind --leak-check=full ./corosync # Analyze heap usage gdb -p corosync -batch -ex \"bt\"","title":"Memory Analysis"},{"location":"troubleshooting-code-behavior/#concurrency-issues","text":"Pacemaker: # Monitor concurrent operations crm_mon --show-detail # Check for deadlocks grep \"deadlock\" /var/log/pacemaker/* # Monitor transition queue crm_simulate --show-failcounts Ceph: # Check for lock contention ceph daemon --admin socket tell osd.0 dump_historic_ops # Monitor thread pool ceph tell osd.0 dump_ops_in_flight # Check recovery activity ceph -w","title":"Concurrency Issues"},{"location":"troubleshooting-code-behavior/#log-analysis","text":"","title":"Log Analysis"},{"location":"troubleshooting-code-behavior/#understanding-log-levels","text":"Corosync: - DEBUG : Detailed diagnostic information - INFO : General informational messages - WARN : Warning conditions - ERROR : Error conditions Pacemaker: - notice : Information about cluster events - warning : Potential issues - error : Error conditions - crit : Critical failures Ceph: - debug/10 : Detailed information (level 10) - debug/20 : More information - info : General messages - warn : Warning messages - err : Error messages","title":"Understanding Log Levels"},{"location":"troubleshooting-code-behavior/#log-locations","text":"Corosync: /var/log/corosync/corosync.log /var/log/corosync/debug.log /var/log/syslog Pacemaker: /var/log/pacemaker/pengine/* /var/log/pacemaker/crmd/* /var/log/pacemaker/attrd/* Ceph: /var/log/ceph/ceph.log /var/log/ceph/ceph-mon.log /var/log/ceph/ceph-osd.*.log","title":"Log Locations"},{"location":"troubleshooting-code-behavior/#log-analysis-commands","text":"Corosync: # Token analysis grep \"token\" /var/log/corosync/corosync.log # Quorum events grep \"quorum\" /var/log/corosync/corosync.log # Network issues grep \"network\" /var/log/corosync/corosync.log Pacemaker: # Find failed resources grep \"FAILED\" /var/log/pacemaker/pengine/* # Analyze transitions grep \"calculated transition\" /var/log/pacemaker/pengine/* # Check for errors grep \"error\" /var/log/pacemaker/* Ceph: # OSD crashes grep \"segfault\" /var/log/ceph/ceph-osd.*.log # Slow requests grep \"slow request\" /var/log/ceph/ceph-osd.*.log # Network issues grep \"timed out\" /var/log/ceph/ceph-osd.*.log","title":"Log Analysis Commands"},{"location":"troubleshooting-code-behavior/#configuration-problems","text":"","title":"Configuration Problems"},{"location":"troubleshooting-code-behavior/#validation-and-testing","text":"Before Applying Changes: # Validate Pacemaker configuration crm_verify -L -V # Test with simulation crm_simulate --live-check # Dry-run Ceph commands ceph tell osd.* injectargs --dry-run # Test iSCSI configuration gwcli.py target list Rollback Procedures: # Pacemaker snapshot cibadmin --query --local > cib-backup.xml # Rollback if needed cibadmin --replace --local --xml-file cib-backup.xml # Restore Ceph configuration ceph config set <option> <previous-value>","title":"Validation and Testing"},{"location":"troubleshooting-code-behavior/#integration-issues","text":"","title":"Integration Issues"},{"location":"troubleshooting-code-behavior/#cluster-coordination","text":"Corosync and Pacemaker: # Check Corosync status from Pacemaker crm_resource --list # Verify quorum crm_attribute -q -n quorum # Check node status pcs status nodes # Ensure Pacemaker can communicate crm_mon --show-detail Ceph with Cluster Managers: # Check if cluster manager sees Ceph ceph -s # Verify network connectivity ping <mon-host> # Check monitor status ceph -w","title":"Cluster Coordination"},{"location":"troubleshooting-code-behavior/#storage-integration","text":"iSCSI Issues: # Check gateway status gwcli.py target list # Check initiator connections iscsiadm -m session # Check multipath status multipath -ll # Reset initiator state iscsiadm -m node logout -T <target-iqn> -p <portal-ip> iscsiadm -m node login -T <target-iqn> -p <portal-ip> --login RBD Issues: # Check mapped images rbd showmapped # Check image status rbd info <pool>/<image> # Unmap and remap rbd unmap <pool>/<image> rbd map <pool>/<image> # Check for stale mappings dmesg | grep rbd","title":"Storage Integration"},{"location":"troubleshooting-code-behavior/#advanced-diagnostics","text":"","title":"Advanced Diagnostics"},{"location":"troubleshooting-code-behavior/#system-level-debugging","text":"# Enable kernel debugging echo 1 > /proc/sys/kernel/sched_sched_min_granularity echo 1 > /proc/sys/kernel/khung_task_timeout_secs # Enable KVM tracing echo 1 > /sys/module/kvm/parameters/trace_gue # Enable Corosync tracing corosync-cmapctl totem.trace_buffer_size 1000000 # Monitor system resources top -b -H -d 2 vmstat 1 iostat -x 2","title":"System-Level Debugging"},{"location":"troubleshooting-code-behavior/#network-diagnostics","text":"# Check network latency ping -i 3 -s 64 <target-host> # Trace routes traceroute -n <target-host> # Check bandwidth iperf -c 3 -t 10 <target-host> # Capture packets tcpdump -i any -w /tmp/capture.pcap host <target-ip>","title":"Network Diagnostics"},{"location":"troubleshooting-code-behavior/#best-practices-for-troubleshooting","text":"Always start with debug enabled when investigating issues Collect logs before clearing them - keep a complete copy Document the issue - note exact error messages, timestamps Test in isolated environment if possible Check configuration drift - ensure all nodes have same configuration Monitor system resources - CPU, memory, disk I/O, network Use incremental debugging - narrow down the problem step by step Have rollback plan - know how to quickly revert changes","title":"Best Practices for Troubleshooting"},{"location":"troubleshooting-code-behavior/#additional-resources","text":"Corosync Wiki : https://github.com/corosync/corosync/wiki/Troubleshooting Pacemaker Guide : https://www.clusterlabs.org/pacemaker/doc/2.0/Pacemaker_Explained/ Ceph Troubleshooting : https://docs.ceph.com/en/latest/rados/troubleshooting/ QEMU Debugging : https://wiki.qemu.org/Documentation/Debugging/ Linux Performance : https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/tuning_guide/","title":"Additional Resources"},{"location":"cluster/","text":"Complete reference guide for Corosync and Pacemaker cluster management technologies. Corosync Corosync is a Group Communication System providing virtual synchrony guarantees and quorum management for Linux HA clusters. Architecture Overview graph TB A[Applications] --> B[Corosync Cluster Engine] B --> C[TOTEM Protocol] B --> D[Quorum System] B --> E[Configuration DB] B --> F[Availability Manager] C --> G[Network Layer] Key Features Virtual Synchrony : Consistent message ordering across all nodes TOTEM Protocol : Reliable group communication Quorum Management : Split-brain prevention Configuration Database : In-memory key-value store Ring Topology : Efficient message delivery Quick Commands # Installation apt-get install corosync # Generate authentication key corosync-keygen # Status corosync-cfgtool -s corosync-quorumtool -s # Configuration corosync-cmapctl Deployment # 1. Install and configure apt-get install corosync corosync-keygen vim /etc/corosync/corosync.conf # 2. Start on all nodes systemctl enable corosync systemctl start corosync # 3. Verify cluster corosync-cfgtool -s Source Code Repository : corosync/corosync Documentation : corosync.github.io Common Issues Issue Solution Token timeout errors Increase token: 10000 in corosync.conf Quorum loss Check network connectivity and node count Authentication failures Verify authkey permissions (600) Pacemaker Pacemaker is an advanced cluster resource manager for Linux high availability. Architecture Overview graph TB A[CRM - Cluster Resource Manager] --> B[PE - Policy Engine] A --> C[TE - Transition Engine] A --> D[LRM - Local Resource Manager] A --> E[CIB - Cluster Info Base] B --> F[Calculate Transitions] C --> G[Execute Actions] D --> H[Monitor Resources] E --> I[Cluster State Database] Key Features Resource Classes : OCF, LSB, systemd, Upstart Constraints : Ordering, colocation, location STONITH : Fencing integration Cloning : Resource and node templates Batches : Group operations Quick Commands # Cluster status pcs status # Resource management pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource start vip # Constraints pcs constraint order start vip then apache pcs constraint colocation add apache with vip # STONITH pcs stonith create fence-device fence_ipmilan Deployment # 1. Install components apt-get install pacemaker pcs # 2. Configure cluster pcs cluster setup --name mycluster node1 node2 node3 # 3. Start cluster pcs cluster start --all # 4. Enable STONITH pcs property set stonith-enabled=true Source Code Repository : ClusterLabs/pacemaker Documentation : clusterlabs.org/pacemaker/doc/ Common Issues Issue Solution Resource stuck starting Check agent with pcs resource debug-resource Constraint violations Verify constraints with pcs constraint show --full STONITH failures Configure fencing devices properly Quorum issues Check network and node connectivity Deployment Workflow Complete Corosync and Pacemaker HA cluster setup. 1. Initial Setup # Install all packages apt-get install corosync pacemaker pcs # Generate Corosync key corosync-keygen # Distribute key scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ # Configure Corosync # Edit /etc/corosync/corosync.conf on first node # Copy configuration to all nodes scp /etc/corosync/corosync.conf node2:/etc/corosync/ scp /etc/corosync/corosync.conf node3:/etc/corosync/ # Start services systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd 2. Verify Cluster # Check Corosync corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker pcs status 3. Configure STONITH # Create fencing device pcs stonith create fence-device fence_ipmilan \\ ipaddr=192.168.1.200 login=admin passwd=password # Verify fencing pcs stonith list 4. Add Resources # Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 # Create Apache resource pcs resource create apache ocf:heartbeat:apache \\ configfile=/etc/apache2/apache2.conf # Add ordering constraint pcs constraint order start vip then apache # Add colocation constraint pcs constraint colocation add apache with vip # Start resources pcs resource start vip 5. Testing # Test resource failover pcs resource move apache node2 # Verify cluster health pcs status crm_mon Source Code References Component Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org/pacemaker/doc/ Nifty Behaviors Corosync Optimizations Kronosnet : Multi-link transport with automatic failover Token tuning : token: 5000 for faster failure detection Network buffers : window_size: 50 for better throughput Pacemaker Tips Resource stickiness : Keep resources on current node Batch operations : Apply multiple constraints together Symmetric monitoring : Enable pacemaker alerts Security Considerations Corosync Protect authentication key: chmod 600 /etc/corosync/authkey Use encrypted transport (knet) Isolate cluster network Pacemaker Always enable STONITH for production Limit cluster daemon permissions Secure cluster communication Corosync + Pacemaker + OVS Stack Integrated high availability stack combining Corosync messaging, Pacemaker resource management, and Open vSwitch networking. Architecture Overview graph TB A[VMs] --> B[OVS Bridge] B --> C[Physical Network] D[Pacemaker CRM] --> E[OVS Resources] D --> F[VIP Resources] D --> G[Application Resources] E --> B H[Corosync] --> D I[Quorum] --> D J[STONITH] --> D style B fill:#c8e6c9 style D fill:#ffecb3 style H fill:#e1f5ff Integration Components Corosync : Cluster messaging and quorum Pacemaker : Resource management and failover OVS : Network virtualization and connectivity OVS Resources in Pacemaker # Create OVS bridge resource pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge \\ bridge_name=br0 # Create OVS port resource pcs resource create ovs-port ocf:heartbeat:ovs-port \\ bridge_name=br0 port_name=eth0 # Create OVS interface resource pcs resource create ovs-intf ocf:heartbeat:ovs-interface \\ bridge_name=br0 interface=eth1 ip=192.168.1.100/24 # Add ordering constraints pcs constraint order start ovs-bridge then ovs-port pcs constraint order start ovs-port then ovs-intf # Add colocation constraints pcs constraint colocation add ovs-port with ovs-bridge pcs constraint colocation add ovs-intf with ovs-port Full Stack Deployment # 1. Install all components apt-get install corosync pacemaker pcs openvswitch-switch # 2. Configure Corosync corosync-keygen vim /etc/corosync/corosync.conf # 3. Configure Pacemaker cluster pcs cluster setup --name ha-cluster node1 node2 node3 pcs cluster start --all # 4. Configure OVS ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 eth1 # 5. Create OVS resources pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge bridge_name=br0 pcs resource create ovs-port ocf:heartbeat:ovs-port bridge_name=br0 port_name=eth0 pcs resource create ovs-intf ocf:heartbeat:ovs-interface bridge_name=br0 interface=eth1 ip=192.168.1.100/24 # 6. Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.200 cidr_netmask=24 # 7. Create application resource pcs resource create app systemd:myapp # 8. Add constraints pcs constraint order start ovs-bridge then ovs-port pcs constraint order start ovs-port then ovs-intf pcs constraint order start ovs-intf then vip pcs constraint order start vip then app pcs constraint colocation add ovs-port with ovs-bridge pcs constraint colocation add ovs-intf with ovs-port pcs constraint colocation add vip with ovs-intf pcs constraint colocation add app with vip # 9. Enable STONITH pcs stonith create fence-device fence_ipmilan pcs property set stonith-enabled=true # 10. Start resources pcs resource start ovs-bridge Sample Corosync Configuration totem { version: 2 cluster_name: ha-cluster transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } } nodelist { node { ring0_addr: 192.168.1.10 name: node1 } node { ring0_addr: 192.168.1.11 name: node2 } node { ring0_addr: 192.168.1.12 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes } Resource Monitoring # Monitor all resources pcs status resources # Monitor OVS-specific resources pcs status resources ovs-bridge ovs-port ovs-intf # Check OVS configuration ovs-vsctl show # Check Corosync status corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker status pcs status crm_mon Failover Testing # Test OVS failover pcs resource move ovs-bridge node2 # Test application failover pcs resource move app node3 # Verify network connectivity ping 192.168.1.200 # Verify OVS bridge ovs-ofctl show br0 Common Issues Issue Solution OVS bridge not starting Check OVS agent logs: pcs resource debug-resource ovs-bridge VIP not accessible Verify OVS bridge is running before VIP Network partition Verify Corosync quorum: corosync-quorumtool -s Resource stuck Check constraints: pcs constraint show --full Nifty Behaviors OVS Bridge Resource Clone pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge bridge_name=br0 \\ --clone Nifty : Run OVS bridge on all nodes simultaneously Symmetric Cloning for High Availability pcs resource clone app clone-max=2 clone-node-max=1 Nifty : Application runs on multiple nodes with load balancing OVS Interface with Bonding ovs-vsctl add-bond bond0 eth0 eth1 pcs resource create ovs-bond ocf:heartbeat:ovs-port \\ bridge_name=br0 port_name=bond0 Nifty : Network redundancy with bonded interfaces Troubleshooting For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment section.","title":"Overview"},{"location":"cluster/#corosync","text":"Corosync is a Group Communication System providing virtual synchrony guarantees and quorum management for Linux HA clusters.","title":"Corosync"},{"location":"cluster/#architecture-overview","text":"graph TB A[Applications] --> B[Corosync Cluster Engine] B --> C[TOTEM Protocol] B --> D[Quorum System] B --> E[Configuration DB] B --> F[Availability Manager] C --> G[Network Layer]","title":"Architecture Overview"},{"location":"cluster/#key-features","text":"Virtual Synchrony : Consistent message ordering across all nodes TOTEM Protocol : Reliable group communication Quorum Management : Split-brain prevention Configuration Database : In-memory key-value store Ring Topology : Efficient message delivery","title":"Key Features"},{"location":"cluster/#quick-commands","text":"# Installation apt-get install corosync # Generate authentication key corosync-keygen # Status corosync-cfgtool -s corosync-quorumtool -s # Configuration corosync-cmapctl","title":"Quick Commands"},{"location":"cluster/#deployment","text":"# 1. Install and configure apt-get install corosync corosync-keygen vim /etc/corosync/corosync.conf # 2. Start on all nodes systemctl enable corosync systemctl start corosync # 3. Verify cluster corosync-cfgtool -s","title":"Deployment"},{"location":"cluster/#source-code","text":"Repository : corosync/corosync Documentation : corosync.github.io","title":"Source Code"},{"location":"cluster/#common-issues","text":"Issue Solution Token timeout errors Increase token: 10000 in corosync.conf Quorum loss Check network connectivity and node count Authentication failures Verify authkey permissions (600)","title":"Common Issues"},{"location":"cluster/#pacemaker","text":"Pacemaker is an advanced cluster resource manager for Linux high availability.","title":"Pacemaker"},{"location":"cluster/#architecture-overview_1","text":"graph TB A[CRM - Cluster Resource Manager] --> B[PE - Policy Engine] A --> C[TE - Transition Engine] A --> D[LRM - Local Resource Manager] A --> E[CIB - Cluster Info Base] B --> F[Calculate Transitions] C --> G[Execute Actions] D --> H[Monitor Resources] E --> I[Cluster State Database]","title":"Architecture Overview"},{"location":"cluster/#key-features_1","text":"Resource Classes : OCF, LSB, systemd, Upstart Constraints : Ordering, colocation, location STONITH : Fencing integration Cloning : Resource and node templates Batches : Group operations","title":"Key Features"},{"location":"cluster/#quick-commands_1","text":"# Cluster status pcs status # Resource management pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource start vip # Constraints pcs constraint order start vip then apache pcs constraint colocation add apache with vip # STONITH pcs stonith create fence-device fence_ipmilan","title":"Quick Commands"},{"location":"cluster/#deployment_1","text":"# 1. Install components apt-get install pacemaker pcs # 2. Configure cluster pcs cluster setup --name mycluster node1 node2 node3 # 3. Start cluster pcs cluster start --all # 4. Enable STONITH pcs property set stonith-enabled=true","title":"Deployment"},{"location":"cluster/#source-code_1","text":"Repository : ClusterLabs/pacemaker Documentation : clusterlabs.org/pacemaker/doc/","title":"Source Code"},{"location":"cluster/#common-issues_1","text":"Issue Solution Resource stuck starting Check agent with pcs resource debug-resource Constraint violations Verify constraints with pcs constraint show --full STONITH failures Configure fencing devices properly Quorum issues Check network and node connectivity","title":"Common Issues"},{"location":"cluster/#deployment-workflow","text":"Complete Corosync and Pacemaker HA cluster setup.","title":"Deployment Workflow"},{"location":"cluster/#1-initial-setup","text":"# Install all packages apt-get install corosync pacemaker pcs # Generate Corosync key corosync-keygen # Distribute key scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ # Configure Corosync # Edit /etc/corosync/corosync.conf on first node # Copy configuration to all nodes scp /etc/corosync/corosync.conf node2:/etc/corosync/ scp /etc/corosync/corosync.conf node3:/etc/corosync/ # Start services systemctl enable corosync pacemaker pcsd systemctl start corosync pacemaker pcsd","title":"1. Initial Setup"},{"location":"cluster/#2-verify-cluster","text":"# Check Corosync corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker pcs status","title":"2. Verify Cluster"},{"location":"cluster/#3-configure-stonith","text":"# Create fencing device pcs stonith create fence-device fence_ipmilan \\ ipaddr=192.168.1.200 login=admin passwd=password # Verify fencing pcs stonith list","title":"3. Configure STONITH"},{"location":"cluster/#4-add-resources","text":"# Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 # Create Apache resource pcs resource create apache ocf:heartbeat:apache \\ configfile=/etc/apache2/apache2.conf # Add ordering constraint pcs constraint order start vip then apache # Add colocation constraint pcs constraint colocation add apache with vip # Start resources pcs resource start vip","title":"4. Add Resources"},{"location":"cluster/#5-testing","text":"# Test resource failover pcs resource move apache node2 # Verify cluster health pcs status crm_mon","title":"5. Testing"},{"location":"cluster/#source-code-references","text":"Component Repository Documentation Corosync corosync/corosync corosync.github.io Pacemaker ClusterLabs/pacemaker clusterlabs.org/pacemaker/doc/","title":"Source Code References"},{"location":"cluster/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"cluster/#corosync-optimizations","text":"Kronosnet : Multi-link transport with automatic failover Token tuning : token: 5000 for faster failure detection Network buffers : window_size: 50 for better throughput","title":"Corosync Optimizations"},{"location":"cluster/#pacemaker-tips","text":"Resource stickiness : Keep resources on current node Batch operations : Apply multiple constraints together Symmetric monitoring : Enable pacemaker alerts","title":"Pacemaker Tips"},{"location":"cluster/#security-considerations","text":"","title":"Security Considerations"},{"location":"cluster/#corosync_1","text":"Protect authentication key: chmod 600 /etc/corosync/authkey Use encrypted transport (knet) Isolate cluster network","title":"Corosync"},{"location":"cluster/#pacemaker_1","text":"Always enable STONITH for production Limit cluster daemon permissions Secure cluster communication","title":"Pacemaker"},{"location":"cluster/#corosync-pacemaker-ovs-stack","text":"Integrated high availability stack combining Corosync messaging, Pacemaker resource management, and Open vSwitch networking.","title":"Corosync + Pacemaker + OVS Stack"},{"location":"cluster/#architecture-overview_2","text":"graph TB A[VMs] --> B[OVS Bridge] B --> C[Physical Network] D[Pacemaker CRM] --> E[OVS Resources] D --> F[VIP Resources] D --> G[Application Resources] E --> B H[Corosync] --> D I[Quorum] --> D J[STONITH] --> D style B fill:#c8e6c9 style D fill:#ffecb3 style H fill:#e1f5ff","title":"Architecture Overview"},{"location":"cluster/#integration-components","text":"Corosync : Cluster messaging and quorum Pacemaker : Resource management and failover OVS : Network virtualization and connectivity","title":"Integration Components"},{"location":"cluster/#ovs-resources-in-pacemaker","text":"# Create OVS bridge resource pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge \\ bridge_name=br0 # Create OVS port resource pcs resource create ovs-port ocf:heartbeat:ovs-port \\ bridge_name=br0 port_name=eth0 # Create OVS interface resource pcs resource create ovs-intf ocf:heartbeat:ovs-interface \\ bridge_name=br0 interface=eth1 ip=192.168.1.100/24 # Add ordering constraints pcs constraint order start ovs-bridge then ovs-port pcs constraint order start ovs-port then ovs-intf # Add colocation constraints pcs constraint colocation add ovs-port with ovs-bridge pcs constraint colocation add ovs-intf with ovs-port","title":"OVS Resources in Pacemaker"},{"location":"cluster/#full-stack-deployment","text":"# 1. Install all components apt-get install corosync pacemaker pcs openvswitch-switch # 2. Configure Corosync corosync-keygen vim /etc/corosync/corosync.conf # 3. Configure Pacemaker cluster pcs cluster setup --name ha-cluster node1 node2 node3 pcs cluster start --all # 4. Configure OVS ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 eth1 # 5. Create OVS resources pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge bridge_name=br0 pcs resource create ovs-port ocf:heartbeat:ovs-port bridge_name=br0 port_name=eth0 pcs resource create ovs-intf ocf:heartbeat:ovs-interface bridge_name=br0 interface=eth1 ip=192.168.1.100/24 # 6. Create VIP resource pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.200 cidr_netmask=24 # 7. Create application resource pcs resource create app systemd:myapp # 8. Add constraints pcs constraint order start ovs-bridge then ovs-port pcs constraint order start ovs-port then ovs-intf pcs constraint order start ovs-intf then vip pcs constraint order start vip then app pcs constraint colocation add ovs-port with ovs-bridge pcs constraint colocation add ovs-intf with ovs-port pcs constraint colocation add vip with ovs-intf pcs constraint colocation add app with vip # 9. Enable STONITH pcs stonith create fence-device fence_ipmilan pcs property set stonith-enabled=true # 10. Start resources pcs resource start ovs-bridge","title":"Full Stack Deployment"},{"location":"cluster/#sample-corosync-configuration","text":"totem { version: 2 cluster_name: ha-cluster transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } } nodelist { node { ring0_addr: 192.168.1.10 name: node1 } node { ring0_addr: 192.168.1.11 name: node2 } node { ring0_addr: 192.168.1.12 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes }","title":"Sample Corosync Configuration"},{"location":"cluster/#resource-monitoring","text":"# Monitor all resources pcs status resources # Monitor OVS-specific resources pcs status resources ovs-bridge ovs-port ovs-intf # Check OVS configuration ovs-vsctl show # Check Corosync status corosync-cfgtool -s corosync-quorumtool -s # Check Pacemaker status pcs status crm_mon","title":"Resource Monitoring"},{"location":"cluster/#failover-testing","text":"# Test OVS failover pcs resource move ovs-bridge node2 # Test application failover pcs resource move app node3 # Verify network connectivity ping 192.168.1.200 # Verify OVS bridge ovs-ofctl show br0","title":"Failover Testing"},{"location":"cluster/#common-issues_2","text":"Issue Solution OVS bridge not starting Check OVS agent logs: pcs resource debug-resource ovs-bridge VIP not accessible Verify OVS bridge is running before VIP Network partition Verify Corosync quorum: corosync-quorumtool -s Resource stuck Check constraints: pcs constraint show --full","title":"Common Issues"},{"location":"cluster/#nifty-behaviors_1","text":"","title":"Nifty Behaviors"},{"location":"cluster/#ovs-bridge-resource-clone","text":"pcs resource create ovs-bridge ocf:heartbeat:ovs-bridge bridge_name=br0 \\ --clone Nifty : Run OVS bridge on all nodes simultaneously","title":"OVS Bridge Resource Clone"},{"location":"cluster/#symmetric-cloning-for-high-availability","text":"pcs resource clone app clone-max=2 clone-node-max=1 Nifty : Application runs on multiple nodes with load balancing","title":"Symmetric Cloning for High Availability"},{"location":"cluster/#ovs-interface-with-bonding","text":"ovs-vsctl add-bond bond0 eth0 eth1 pcs resource create ovs-bond ocf:heartbeat:ovs-port \\ bridge_name=br0 port_name=bond0 Nifty : Network redundancy with bonded interfaces","title":"OVS Interface with Bonding"},{"location":"cluster/#troubleshooting","text":"For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment section.","title":"Troubleshooting"},{"location":"cluster/corosync/","text":"Cluster messaging and synchronization framework providing virtual synchrony guarantees and quorum management for Linux HA clusters. Architecture graph TB subgraph \"Application Layer\" A[Pacemaker] B[DLM] C[Custom Apps] end subgraph \"Corosync Core\" D[Configuration DB] E[Configuration API] F[Log System] G[Statistic System] H[Quorum System] I[Votequorum] J[Corosync Votequorum] K[TOTEM Protocol] L[Transport UDP] M[Transport KNF] N[Transport KNET] O[IPC Service] P[Unix Sockets] Q[SHM] end A --> D B --> D C --> D A --> O B --> O C --> O O --> P O --> Q A --> H B --> H C --> H H --> I I --> J A --> K B --> K C --> K K --> L K --> M K --> N E --> D F --> D G --> D style D fill:#c8e6c9 style H fill:#ffecb3 style K fill:#e1f5ff Core Components TOTEM Protocol The Totem Single Ring Ordering and Membership protocol provides virtual synchrony for cluster messaging. Virtual Synchrony Properties : - Total order: All messages delivered in same order to all nodes - Agreement: Messages delivered to live nodes in same view - Safety: No messages delivered after view change completes Protocol Operation : graph LR A[Application Message] --> B[Message Queue] B --> C[Sequencer Assignment] C --> D[Ring Broadcast] D --> E[Message Delivery] E --> F[Application Receive] G[Node Failure] --> H[View Change] H --> I[Membership Reconfiguration] I --> J[Recovery] style B fill:#c8e6c9 style D fill:#e1f5ff style H fill:#ffecb3 TOTEM Parameters : - token : Time between token rotations (ms) - token_retransmit : Retransmission timeout (ms) - hold : Token hold time before release (ms) - token_retransmits_before_loss_const : Token loss threshold - join : Join timeout (ms) - consensus : Consensus timeout (ms) - merge : Merge timeout (ms) - downcheck : Node down detection (ms) Quorum System Corosync uses quorum to prevent split-brain scenarios where clusters form separate groups. Quorum Types : Split-Brain Prevention : Ensure cluster consistency Votequorum : Simple voting mechanism QNet : Network-based quorum device QDevice : External quorum device Votequorum Operation : # Check quorum state corosync-quorumtool -s # Expected output example: Quorum information ------------------ Date: Mon Jan 26 2026 Quorum provider: corosync_votequorum Nodes: 3 Node ID: 1 Ring ID: 1/8168 Quorate: Yes Votequorum information ---------------------- Expected votes: 3 Highest expected: 3 Total votes: 3 Quorum: 2 Flags: Quorate Quorum Calculation : - Minimum votes required: floor(total_votes / 2) + 1 - 3-node cluster: quorum = 2 - 5-node cluster: quorum = 3 - 2-node cluster: requires QDevice Configuration Database In-memory key-value store for cluster configuration. Configuration API : # List all keys corosync-cmapctl # Read specific key corosync-cmapctl -g runtime.totem.token # Set key value corosync-cmapctl -s nodelist.node.0.nodeid -t u32 -v 1 # Map keys to values corosync-cmapctl -b runtime Configuration Keys : Category Example Keys Description Runtime runtime.totem.token Current token timeout Totem totem.token TOTEM configuration Quorum quorum.expected_votes Expected vote count Nodelist nodelist.node.*.ring0_addr Node addresses Logging logging.to_logfile Logging configuration Configuration Complete corosync.conf totem { version: 2 cluster_name: ha-cluster transport: knet # Ring 0 - Primary network interface { ringnumber: 0 bindnetaddr: 192.168.1.0 broadcast: yes mcastport: 5405 } # Ring 1 - Secondary network (redundancy) interface { ringnumber: 1 bindnetaddr: 10.0.1.0 broadcast: yes mcastport: 5406 } # TOTEM protocol tuning token: 5000 token_retransmit: 250 hold: 180 token_retransmits_before_loss_const: 4 join: 60 consensus: 4800 merge: 200 downcheck: 1000 fail_to_recv_const: 2500 seqno_unchanged_const: 2000 window_size: 50 max_messages: 17 # Security crypto_cipher: aes256 crypto_hash: sha256 } nodelist { node { ring0_addr: 192.168.1.10 ring1_addr: 10.0.1.10 nodeid: 1 name: node1 } node { ring0_addr: 192.168.1.11 ring1_addr: 10.0.1.11 nodeid: 2 name: node2 } node { ring0_addr: 192.168.1.12 ring1_addr: 10.0.1.12 nodeid: 3 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 two_node: 0 wait_for_all: 0 auto_tie_breaker: 1 last_man_standing: 1 last_man_standing_window: 10000 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes timestamp: on debug: off logger_subsys { subsys: QUORUM debug: off } } # IPC service configuration quorum { provider: corosync_votequorum } nodelist { # As defined above } Kronosnet (knet) Transport Kronosnet provides multi-link transport with automatic failover. Knet Configuration : totem { version: 2 cluster_name: ha-cluster transport: knet # Link configuration knet_transport { link_mode: passive link_priority: [100, 50] link_0 { link_priority: 100 mcastport: 5405 ttl: 1 } link_1 { link_priority: 50 mcastport: 5406 ttl: 1 } } interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } interface { ringnumber: 1 bindnetaddr: 10.0.1.0 mcastport: 5406 } # Crypto configuration crypto_cipher: aes256 crypto_hash: sha256 crypto_model: nss # Compression compress_model: zlib compress_threshold: 1000 } Knet Benefits : - Automatic link failover - Traffic prioritization - Built-in encryption - Compression support - Multiple transports per link UDP Transport (Legacy) totem { version: 2 cluster_name: ha-cluster transport: udpu interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } } Key Features Virtual Synchrony Message Ordering : All nodes receive messages in identical order across cluster views. Virtual Synchrony Properties : 1. Total Order : Single global message order 2. Atomic Broadcast : All or nothing delivery 3. View Synchrony : View changes coordinate with messages 4. Safety : No message loss or duplication Use Cases : - Cluster state synchronization - Distributed lock management - Resource state coordination - Configuration distribution Quorum Management Quorum States : - Quorate : Cluster has majority - Inquorate : Cluster lacks majority - Recovering : Quorum being restored Quorum Actions : - Quorate: Normal operation - Inquorate: Resource fencing, graceful degradation Configuration Database In-Memory KV Store : - Fast access to cluster state - Dynamic reconfiguration - Key-value API for applications Key Patterns : - config.* : Configuration parameters - runtime.* : Runtime statistics - statemachine.* : State machine data Quick Commands Installation # Debian/Ubuntu apt-get install corosync # RHEL/CentOS yum install corosync # Generate authentication key corosync-keygen # Default location: /etc/corosync/authkey # Permissions: 600 Status and Monitoring # Check cluster membership corosync-cfgtool -s # Check quorum status corosync-quorumtool -s # View configuration database corosync-cmapctl # Monitor runtime statistics corosync-cmapctl -b runtime # View IPC statistics corosync-cmapctl -b stats # Check node status corosync-cmapctl -g runtime.votequorum.node_id Configuration Management # Validate configuration corosync-cfgtool -v # View loaded configuration corosync-cmapctl -b config # Reload configuration systemctl reload corosync # Backup configuration cp /etc/corosync/corosync.conf /etc/corosync/corosync.conf.bak Debugging # Enable debug logging corosync-cmapctl -s logging.debug -t str on # View message logs journalctl -u corosync -f # Check network connectivity corosync-cfgtool -s -i <interface> # View token statistics corosync-cmapctl -b runtime.totem # Check IPC service status corosync-cmapctl -b runtime.services Nifty Behaviors Kronosnet Multi-Link Transport totem { transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } interface { ringnumber: 1 bindnetaddr: 10.0.1.0 mcastport: 5406 } } Nifty : Automatic failover between network links, better performance, built-in encryption Token Timeout Optimization totem { token: 5000 # Faster failure detection token_retransmit: 250 hold: 180 } Nifty : Faster cluster response to failures Auto Tie-Breaker quorum { provider: corosync_votequorum auto_tie_breaker: 1 last_man_standing: 1 } Nifty : 2-node cluster operation without QDevice Quorum Device Integration quorum { provider: qdevice model: net net { host: qdevice.example.com algorithm: lms tie_breaker: lowest } } Nifty : External quorum for even-numbered clusters Protocol Internals TOTEM Protocol Details Message Types : 1. Regular Messages : Application data 2. Join Messages : Node join requests 3. Consensus Messages : Membership agreement 4. Merge Messages : Cluster merge operations Ring Operation : Node A -> Token -> Node B -> Token -> Node C -> Token -> Node A Token Rotation : 1. Sequencer holds token 2. Processes pending messages 3. Broadcasts new token with message IDs 4. Releases token to next node View Change : sequenceDiagram participant N1 as Node 1 participant N2 as Node 2 participant N3 as Node 3 N1->>N2: Normal operation N2->>N3: Normal operation N3->>N1: Normal operation Note over N1: Node failure detected N1->>N2: Join request N2->>N3: Join request N3->>N1: Consensus Note over N1,N3: New view formed N1->>N2: Resynchronize N2->>N3: Resynchronize IPC Service Communication Unix Domain Sockets : - /var/run/corosync/corosync.sock - Used by local applications - Fast IPC Shared Memory : - Used for high-performance communication - Zero-copy message passing IPC Protocol : // Simplified IPC flow 1. Application opens IPC connection 2. Application subscribes to notification type 3. Corosync sends messages to subscribers 4. Application acknowledges receipt 5. Corosync tracks delivery state Performance Tuning Network Optimization totem { # Reduce token timeout for faster failover token: 3000 token_retransmit: 200 # Increase window for high latency window_size: 100 # Tune for high throughput max_messages: 32 } Memory Optimization totem { # Reduce memory usage max_messages: 10 window_size: 30 } CPU Optimization # Disable compression for CPU-bound clusters totem { compress_model: none } # Use faster crypto totem { crypto_cipher: aes128 crypto_hash: sha1 } Security Authentication Key Key Generation : corosync-keygen # Uses /dev/random # Creates 128-byte key # Stored in /etc/corosync/authkey Key Distribution : # Copy to all nodes scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ Key Permissions : chmod 600 /etc/corosync/authkey chown root:root /etc/corosync/authkey Transport Security Knet Encryption : totem { transport: knet crypto_cipher: aes256 crypto_hash: sha256 crypto_model: nss } Network Isolation : - Use dedicated cluster network - Separate management traffic - VLAN segmentation Access Control IPC Permissions : # Default: root only chmod 600 /var/run/corosync/corosync.sock # Custom: Group access chown root:corosync /var/run/corosync/corosync.sock chmod 660 /var/run/corosync/corosync.sock Troubleshooting Common Issues Token Timeout Errors Symptoms : - corosync-cfgtool -s shows timeouts - Cluster instability Solution : totem { token: 10000 token_retransmit: 1000 } Quorum Loss Symptoms : - corosync-quorumtool -s shows \"Inquorate\" - Cluster degraded Solutions : # Check network connectivity ping node2 ping node3 # Verify node count corosync-quorumtool -s # Force quorum (emergency only) corosync-quorumtool -f # Add QDevice for 2-node clusters Authentication Failures Symptoms : - authkey errors in logs - Nodes cannot join cluster Solution : # Regenerate key on all nodes rm /etc/corosync/authkey corosync-keygen # Distribute key scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ # Restart services systemctl restart corosync Node Cannot Join Cluster Symptoms : - Node not listed in membership - Firewall blocking traffic Solution : # Check firewall iptables -L -n | grep 5405 # Allow cluster traffic iptables -A INPUT -p udp --dport 5405 -j ACCEPT iptables -A INPUT -p udp --dport 5406 -j ACCEPT # Check network connectivity nc -zuv node1 5405 nc -zuv node1 5406 # Verify configuration corosync-cfgtool -v Debug Commands # Enable verbose logging corosync-cmapctl -s logging.debug -t str on systemctl restart corosync # View detailed logs journalctl -u corosync -f # Check all interfaces corosync-cfgtool -s -i all # Dump configuration corosync-cmapctl > corosync_config.txt # Monitor runtime statistics corosync-cmapctl -b runtime -b stats -w # Check IPC connections corosync-cmapctl -b runtime.services Log Analysis Common Log Messages : # Normal operation [MAIN ] Corosync Cluster Engine exiting normally [TOTEM ] The token was lost 12 times # Quorum loss [QUORUM] Quorum lost, inquorate cluster # Node failure [MAIN ] Node 1 left the cluster # Configuration error [CFG ] Invalid configuration: unknown token Log Locations : - /var/log/corosync/corosync.log - Corosync logs - /var/log/syslog - System logs (if enabled) - journalctl -u corosync - Systemd logs Best Practices Production Deployment Redundant Networks : Use at least 2 ring interfaces Proper Token Tuning : Balance failover speed vs stability Quorum Device : Use QDevice for even-numbered clusters Secure Keys : Protect authkey with proper permissions Network Isolation : Dedicate cluster network Monitoring : Set up alerts for quorum and membership changes Configuration Checklist [ ] Generate and distribute authkey [ ] Configure at least 2 ring interfaces [ ] Set appropriate token timeouts [ ] Configure quorum for node count [ ] Enable logging to file [ ] Test failover scenarios [ ] Verify network connectivity [ ] Set up monitoring Network Design Physical Network : - Separate cluster network from production - Use 10Gbps or faster - Redundant switches Network Segments : - Cluster: 192.168.1.0/24 - Management: 10.0.1.0/24 - Storage: 10.0.2.0/24 Source Code Repository : https://github.com/corosync/corosync Documentation : https://corosync.github.io/corosync/ Mailing List : https://lists.corosync.org/mailman/listinfo/corosync Key Source Locations Component Location Description TOTEM exec/totem* TOTEM protocol implementation Quorum exec/quorum.c Quorum system Config DB exec/cmap.c Configuration database IPC exec/ipc.c IPC service Logging exec/logsys.c Logging system","title":"Corosync"},{"location":"cluster/corosync/#architecture","text":"graph TB subgraph \"Application Layer\" A[Pacemaker] B[DLM] C[Custom Apps] end subgraph \"Corosync Core\" D[Configuration DB] E[Configuration API] F[Log System] G[Statistic System] H[Quorum System] I[Votequorum] J[Corosync Votequorum] K[TOTEM Protocol] L[Transport UDP] M[Transport KNF] N[Transport KNET] O[IPC Service] P[Unix Sockets] Q[SHM] end A --> D B --> D C --> D A --> O B --> O C --> O O --> P O --> Q A --> H B --> H C --> H H --> I I --> J A --> K B --> K C --> K K --> L K --> M K --> N E --> D F --> D G --> D style D fill:#c8e6c9 style H fill:#ffecb3 style K fill:#e1f5ff","title":"Architecture"},{"location":"cluster/corosync/#core-components","text":"","title":"Core Components"},{"location":"cluster/corosync/#totem-protocol","text":"The Totem Single Ring Ordering and Membership protocol provides virtual synchrony for cluster messaging. Virtual Synchrony Properties : - Total order: All messages delivered in same order to all nodes - Agreement: Messages delivered to live nodes in same view - Safety: No messages delivered after view change completes Protocol Operation : graph LR A[Application Message] --> B[Message Queue] B --> C[Sequencer Assignment] C --> D[Ring Broadcast] D --> E[Message Delivery] E --> F[Application Receive] G[Node Failure] --> H[View Change] H --> I[Membership Reconfiguration] I --> J[Recovery] style B fill:#c8e6c9 style D fill:#e1f5ff style H fill:#ffecb3 TOTEM Parameters : - token : Time between token rotations (ms) - token_retransmit : Retransmission timeout (ms) - hold : Token hold time before release (ms) - token_retransmits_before_loss_const : Token loss threshold - join : Join timeout (ms) - consensus : Consensus timeout (ms) - merge : Merge timeout (ms) - downcheck : Node down detection (ms)","title":"TOTEM Protocol"},{"location":"cluster/corosync/#quorum-system","text":"Corosync uses quorum to prevent split-brain scenarios where clusters form separate groups. Quorum Types : Split-Brain Prevention : Ensure cluster consistency Votequorum : Simple voting mechanism QNet : Network-based quorum device QDevice : External quorum device Votequorum Operation : # Check quorum state corosync-quorumtool -s # Expected output example: Quorum information ------------------ Date: Mon Jan 26 2026 Quorum provider: corosync_votequorum Nodes: 3 Node ID: 1 Ring ID: 1/8168 Quorate: Yes Votequorum information ---------------------- Expected votes: 3 Highest expected: 3 Total votes: 3 Quorum: 2 Flags: Quorate Quorum Calculation : - Minimum votes required: floor(total_votes / 2) + 1 - 3-node cluster: quorum = 2 - 5-node cluster: quorum = 3 - 2-node cluster: requires QDevice","title":"Quorum System"},{"location":"cluster/corosync/#configuration-database","text":"In-memory key-value store for cluster configuration. Configuration API : # List all keys corosync-cmapctl # Read specific key corosync-cmapctl -g runtime.totem.token # Set key value corosync-cmapctl -s nodelist.node.0.nodeid -t u32 -v 1 # Map keys to values corosync-cmapctl -b runtime Configuration Keys : Category Example Keys Description Runtime runtime.totem.token Current token timeout Totem totem.token TOTEM configuration Quorum quorum.expected_votes Expected vote count Nodelist nodelist.node.*.ring0_addr Node addresses Logging logging.to_logfile Logging configuration","title":"Configuration Database"},{"location":"cluster/corosync/#configuration","text":"","title":"Configuration"},{"location":"cluster/corosync/#complete-corosyncconf","text":"totem { version: 2 cluster_name: ha-cluster transport: knet # Ring 0 - Primary network interface { ringnumber: 0 bindnetaddr: 192.168.1.0 broadcast: yes mcastport: 5405 } # Ring 1 - Secondary network (redundancy) interface { ringnumber: 1 bindnetaddr: 10.0.1.0 broadcast: yes mcastport: 5406 } # TOTEM protocol tuning token: 5000 token_retransmit: 250 hold: 180 token_retransmits_before_loss_const: 4 join: 60 consensus: 4800 merge: 200 downcheck: 1000 fail_to_recv_const: 2500 seqno_unchanged_const: 2000 window_size: 50 max_messages: 17 # Security crypto_cipher: aes256 crypto_hash: sha256 } nodelist { node { ring0_addr: 192.168.1.10 ring1_addr: 10.0.1.10 nodeid: 1 name: node1 } node { ring0_addr: 192.168.1.11 ring1_addr: 10.0.1.11 nodeid: 2 name: node2 } node { ring0_addr: 192.168.1.12 ring1_addr: 10.0.1.12 nodeid: 3 name: node3 } } quorum { provider: corosync_votequorum expected_votes: 3 two_node: 0 wait_for_all: 0 auto_tie_breaker: 1 last_man_standing: 1 last_man_standing_window: 10000 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes timestamp: on debug: off logger_subsys { subsys: QUORUM debug: off } } # IPC service configuration quorum { provider: corosync_votequorum } nodelist { # As defined above }","title":"Complete corosync.conf"},{"location":"cluster/corosync/#kronosnet-knet-transport","text":"Kronosnet provides multi-link transport with automatic failover. Knet Configuration : totem { version: 2 cluster_name: ha-cluster transport: knet # Link configuration knet_transport { link_mode: passive link_priority: [100, 50] link_0 { link_priority: 100 mcastport: 5405 ttl: 1 } link_1 { link_priority: 50 mcastport: 5406 ttl: 1 } } interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } interface { ringnumber: 1 bindnetaddr: 10.0.1.0 mcastport: 5406 } # Crypto configuration crypto_cipher: aes256 crypto_hash: sha256 crypto_model: nss # Compression compress_model: zlib compress_threshold: 1000 } Knet Benefits : - Automatic link failover - Traffic prioritization - Built-in encryption - Compression support - Multiple transports per link","title":"Kronosnet (knet) Transport"},{"location":"cluster/corosync/#udp-transport-legacy","text":"totem { version: 2 cluster_name: ha-cluster transport: udpu interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } }","title":"UDP Transport (Legacy)"},{"location":"cluster/corosync/#key-features","text":"","title":"Key Features"},{"location":"cluster/corosync/#virtual-synchrony","text":"Message Ordering : All nodes receive messages in identical order across cluster views. Virtual Synchrony Properties : 1. Total Order : Single global message order 2. Atomic Broadcast : All or nothing delivery 3. View Synchrony : View changes coordinate with messages 4. Safety : No message loss or duplication Use Cases : - Cluster state synchronization - Distributed lock management - Resource state coordination - Configuration distribution","title":"Virtual Synchrony"},{"location":"cluster/corosync/#quorum-management","text":"Quorum States : - Quorate : Cluster has majority - Inquorate : Cluster lacks majority - Recovering : Quorum being restored Quorum Actions : - Quorate: Normal operation - Inquorate: Resource fencing, graceful degradation","title":"Quorum Management"},{"location":"cluster/corosync/#configuration-database_1","text":"In-Memory KV Store : - Fast access to cluster state - Dynamic reconfiguration - Key-value API for applications Key Patterns : - config.* : Configuration parameters - runtime.* : Runtime statistics - statemachine.* : State machine data","title":"Configuration Database"},{"location":"cluster/corosync/#quick-commands","text":"","title":"Quick Commands"},{"location":"cluster/corosync/#installation","text":"# Debian/Ubuntu apt-get install corosync # RHEL/CentOS yum install corosync # Generate authentication key corosync-keygen # Default location: /etc/corosync/authkey # Permissions: 600","title":"Installation"},{"location":"cluster/corosync/#status-and-monitoring","text":"# Check cluster membership corosync-cfgtool -s # Check quorum status corosync-quorumtool -s # View configuration database corosync-cmapctl # Monitor runtime statistics corosync-cmapctl -b runtime # View IPC statistics corosync-cmapctl -b stats # Check node status corosync-cmapctl -g runtime.votequorum.node_id","title":"Status and Monitoring"},{"location":"cluster/corosync/#configuration-management","text":"# Validate configuration corosync-cfgtool -v # View loaded configuration corosync-cmapctl -b config # Reload configuration systemctl reload corosync # Backup configuration cp /etc/corosync/corosync.conf /etc/corosync/corosync.conf.bak","title":"Configuration Management"},{"location":"cluster/corosync/#debugging","text":"# Enable debug logging corosync-cmapctl -s logging.debug -t str on # View message logs journalctl -u corosync -f # Check network connectivity corosync-cfgtool -s -i <interface> # View token statistics corosync-cmapctl -b runtime.totem # Check IPC service status corosync-cmapctl -b runtime.services","title":"Debugging"},{"location":"cluster/corosync/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"cluster/corosync/#kronosnet-multi-link-transport","text":"totem { transport: knet interface { ringnumber: 0 bindnetaddr: 192.168.1.0 mcastport: 5405 } interface { ringnumber: 1 bindnetaddr: 10.0.1.0 mcastport: 5406 } } Nifty : Automatic failover between network links, better performance, built-in encryption","title":"Kronosnet Multi-Link Transport"},{"location":"cluster/corosync/#token-timeout-optimization","text":"totem { token: 5000 # Faster failure detection token_retransmit: 250 hold: 180 } Nifty : Faster cluster response to failures","title":"Token Timeout Optimization"},{"location":"cluster/corosync/#auto-tie-breaker","text":"quorum { provider: corosync_votequorum auto_tie_breaker: 1 last_man_standing: 1 } Nifty : 2-node cluster operation without QDevice","title":"Auto Tie-Breaker"},{"location":"cluster/corosync/#quorum-device-integration","text":"quorum { provider: qdevice model: net net { host: qdevice.example.com algorithm: lms tie_breaker: lowest } } Nifty : External quorum for even-numbered clusters","title":"Quorum Device Integration"},{"location":"cluster/corosync/#protocol-internals","text":"","title":"Protocol Internals"},{"location":"cluster/corosync/#totem-protocol-details","text":"Message Types : 1. Regular Messages : Application data 2. Join Messages : Node join requests 3. Consensus Messages : Membership agreement 4. Merge Messages : Cluster merge operations Ring Operation : Node A -> Token -> Node B -> Token -> Node C -> Token -> Node A Token Rotation : 1. Sequencer holds token 2. Processes pending messages 3. Broadcasts new token with message IDs 4. Releases token to next node View Change : sequenceDiagram participant N1 as Node 1 participant N2 as Node 2 participant N3 as Node 3 N1->>N2: Normal operation N2->>N3: Normal operation N3->>N1: Normal operation Note over N1: Node failure detected N1->>N2: Join request N2->>N3: Join request N3->>N1: Consensus Note over N1,N3: New view formed N1->>N2: Resynchronize N2->>N3: Resynchronize","title":"TOTEM Protocol Details"},{"location":"cluster/corosync/#ipc-service-communication","text":"Unix Domain Sockets : - /var/run/corosync/corosync.sock - Used by local applications - Fast IPC Shared Memory : - Used for high-performance communication - Zero-copy message passing IPC Protocol : // Simplified IPC flow 1. Application opens IPC connection 2. Application subscribes to notification type 3. Corosync sends messages to subscribers 4. Application acknowledges receipt 5. Corosync tracks delivery state","title":"IPC Service Communication"},{"location":"cluster/corosync/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"cluster/corosync/#network-optimization","text":"totem { # Reduce token timeout for faster failover token: 3000 token_retransmit: 200 # Increase window for high latency window_size: 100 # Tune for high throughput max_messages: 32 }","title":"Network Optimization"},{"location":"cluster/corosync/#memory-optimization","text":"totem { # Reduce memory usage max_messages: 10 window_size: 30 }","title":"Memory Optimization"},{"location":"cluster/corosync/#cpu-optimization","text":"# Disable compression for CPU-bound clusters totem { compress_model: none } # Use faster crypto totem { crypto_cipher: aes128 crypto_hash: sha1 }","title":"CPU Optimization"},{"location":"cluster/corosync/#security","text":"","title":"Security"},{"location":"cluster/corosync/#authentication-key","text":"Key Generation : corosync-keygen # Uses /dev/random # Creates 128-byte key # Stored in /etc/corosync/authkey Key Distribution : # Copy to all nodes scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ Key Permissions : chmod 600 /etc/corosync/authkey chown root:root /etc/corosync/authkey","title":"Authentication Key"},{"location":"cluster/corosync/#transport-security","text":"Knet Encryption : totem { transport: knet crypto_cipher: aes256 crypto_hash: sha256 crypto_model: nss } Network Isolation : - Use dedicated cluster network - Separate management traffic - VLAN segmentation","title":"Transport Security"},{"location":"cluster/corosync/#access-control","text":"IPC Permissions : # Default: root only chmod 600 /var/run/corosync/corosync.sock # Custom: Group access chown root:corosync /var/run/corosync/corosync.sock chmod 660 /var/run/corosync/corosync.sock","title":"Access Control"},{"location":"cluster/corosync/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"cluster/corosync/#common-issues","text":"","title":"Common Issues"},{"location":"cluster/corosync/#token-timeout-errors","text":"Symptoms : - corosync-cfgtool -s shows timeouts - Cluster instability Solution : totem { token: 10000 token_retransmit: 1000 }","title":"Token Timeout Errors"},{"location":"cluster/corosync/#quorum-loss","text":"Symptoms : - corosync-quorumtool -s shows \"Inquorate\" - Cluster degraded Solutions : # Check network connectivity ping node2 ping node3 # Verify node count corosync-quorumtool -s # Force quorum (emergency only) corosync-quorumtool -f # Add QDevice for 2-node clusters","title":"Quorum Loss"},{"location":"cluster/corosync/#authentication-failures","text":"Symptoms : - authkey errors in logs - Nodes cannot join cluster Solution : # Regenerate key on all nodes rm /etc/corosync/authkey corosync-keygen # Distribute key scp /etc/corosync/authkey node2:/etc/corosync/ scp /etc/corosync/authkey node3:/etc/corosync/ # Restart services systemctl restart corosync","title":"Authentication Failures"},{"location":"cluster/corosync/#node-cannot-join-cluster","text":"Symptoms : - Node not listed in membership - Firewall blocking traffic Solution : # Check firewall iptables -L -n | grep 5405 # Allow cluster traffic iptables -A INPUT -p udp --dport 5405 -j ACCEPT iptables -A INPUT -p udp --dport 5406 -j ACCEPT # Check network connectivity nc -zuv node1 5405 nc -zuv node1 5406 # Verify configuration corosync-cfgtool -v","title":"Node Cannot Join Cluster"},{"location":"cluster/corosync/#debug-commands","text":"# Enable verbose logging corosync-cmapctl -s logging.debug -t str on systemctl restart corosync # View detailed logs journalctl -u corosync -f # Check all interfaces corosync-cfgtool -s -i all # Dump configuration corosync-cmapctl > corosync_config.txt # Monitor runtime statistics corosync-cmapctl -b runtime -b stats -w # Check IPC connections corosync-cmapctl -b runtime.services","title":"Debug Commands"},{"location":"cluster/corosync/#log-analysis","text":"Common Log Messages : # Normal operation [MAIN ] Corosync Cluster Engine exiting normally [TOTEM ] The token was lost 12 times # Quorum loss [QUORUM] Quorum lost, inquorate cluster # Node failure [MAIN ] Node 1 left the cluster # Configuration error [CFG ] Invalid configuration: unknown token Log Locations : - /var/log/corosync/corosync.log - Corosync logs - /var/log/syslog - System logs (if enabled) - journalctl -u corosync - Systemd logs","title":"Log Analysis"},{"location":"cluster/corosync/#best-practices","text":"","title":"Best Practices"},{"location":"cluster/corosync/#production-deployment","text":"Redundant Networks : Use at least 2 ring interfaces Proper Token Tuning : Balance failover speed vs stability Quorum Device : Use QDevice for even-numbered clusters Secure Keys : Protect authkey with proper permissions Network Isolation : Dedicate cluster network Monitoring : Set up alerts for quorum and membership changes","title":"Production Deployment"},{"location":"cluster/corosync/#configuration-checklist","text":"[ ] Generate and distribute authkey [ ] Configure at least 2 ring interfaces [ ] Set appropriate token timeouts [ ] Configure quorum for node count [ ] Enable logging to file [ ] Test failover scenarios [ ] Verify network connectivity [ ] Set up monitoring","title":"Configuration Checklist"},{"location":"cluster/corosync/#network-design","text":"Physical Network : - Separate cluster network from production - Use 10Gbps or faster - Redundant switches Network Segments : - Cluster: 192.168.1.0/24 - Management: 10.0.1.0/24 - Storage: 10.0.2.0/24","title":"Network Design"},{"location":"cluster/corosync/#source-code","text":"Repository : https://github.com/corosync/corosync Documentation : https://corosync.github.io/corosync/ Mailing List : https://lists.corosync.org/mailman/listinfo/corosync","title":"Source Code"},{"location":"cluster/corosync/#key-source-locations","text":"Component Location Description TOTEM exec/totem* TOTEM protocol implementation Quorum exec/quorum.c Quorum system Config DB exec/cmap.c Configuration database IPC exec/ipc.c IPC service Logging exec/logsys.c Logging system","title":"Key Source Locations"},{"location":"cluster/pacemaker/","text":"Advanced cluster resource manager coordinating configuration, start-up, monitoring, and recovery of interrelated services in Linux HA clusters. Architecture graph TB subgraph \"Cluster Layer\" A[CRM - Cluster Resource Manager] B[PE - Policy Engine] C[TE - Transition Engine] D[LRM - Local Resource Manager] E[CIB - Cluster Information Base] F[STONITH] G[Quorum] end subgraph \"Node Layer\" H[Node 1] I[Node 2] J[Node 3] K[Resource Agents] L[LRM Processes] M[Applications] end A --> B A --> C A --> D A --> E A --> F A --> G B --> N[Calculate Transitions] C --> O[Execute Actions] D --> P[Monitor Resources] E --> Q[Cluster State Database] D --> H D --> I D --> J H --> K H --> L H --> M I --> K I --> L I --> M J --> K J --> L J --> M style A fill:#c8e6c9 style B fill:#e1f5ff style C fill:#ffecb3 style E fill:#fff3e0 Core Components CRM - Cluster Resource Manager Central coordinator for all cluster operations. Responsibilities : - Coordinate PE, TE, LRM components - Handle quorum and cluster membership - Process resource operations - Manage STONITH operations - Distribute cluster configuration Communication Flow : sequenceDiagram participant C as CRM participant P as PE participant T as TE participant L as LRM C->>P: Calculate state transitions P->>C: Return transition graph C->>T: Execute transitions T->>L: Resource operations L->>C: Operation results C->>C: Update CIB PE - Policy Engine Calculates optimal resource placement and transitions. Decision Process : 1. Analyze current cluster state 2. Evaluate constraints and priorities 3. Calculate optimal transitions 4. Generate transition graph 5. Submit to TE for execution Algorithm : - Score-based optimization - Constraint satisfaction - Failover calculation - Load balancing - Resource stickiness consideration PE States : - Idle: No calculations in progress - Pending: Waiting for quorum or data - Active: Calculating transitions TE - Transition Engine Executes state transitions calculated by PE. Transition Types : - Start: Start resource on node - Stop: Stop resource on node - Monitor: Check resource health - Promote/Demote: State change for master/slave resources - Migrate: Move resource between nodes - Recover: Handle resource failure Execution Flow : 1. Receive transition graph 2. Validate dependencies 3. Execute operations in order 4. Wait for completion 5. Report results 6. Update cluster state LRM - Local Resource Manager Manages resources on individual nodes. LRM Process : - Resource agent execution - Operation tracking - Status reporting - Timeout handling - Error handling LRM States : - Idle: No active operations - Busy: Executing operation - Stopped: Process stopped CIB - Cluster Information Base Distributed database storing cluster configuration and state. CIB Structure : <cib> <configuration> <crm_config> <!-- Cluster properties --> <nodes> <!-- Node definitions --> <resources> <!-- Resource definitions --> <constraints> <!-- Resource constraints --> </configuration> <status> <node_state> <!-- Node status --> <lrm_resource> <!-- Resource status --> </status> </cib> CIB Distribution : - Replicated to all nodes - Version-controlled - Automatic synchronization - Conflict resolution CIB Updates : # Show CIB pcs config # Export CIB pcs cluster cib > cluster.xml # Edit CIB pcs cluster cib edit # Push CIB pcs cluster cib-push cluster.xml Resource Types Resource Classes OCF - Open Cluster Framework Standard resource agent interface. OCF Actions : # Start <resource-agent> start # Stop <resource-agent> stop # Monitor <resource-agent> monitor # Validate configuration <resource-agent> validate-all # Metadata <resource-agent> meta-data OCF Exit Codes : | Code | Meaning | |------|---------| | 0 | Success | | 7 | Not running | | 1 | Generic error | | 2 | Invalid parameters | | 3 | Unimplemented feature | | 4 | Insufficient privileges | | 5 | Not installed | | 6 | Not configured | OCF Metadata Example : <?xml version=\"1.0\"?> <resource-agent name=\"IPaddr2\"> <version>1.0</version> <longdesc lang=\"en\">IP address management</longdesc> <shortdesc lang=\"en\">IP address management</shortdesc> <parameters> <parameter name=\"ip\" unique=\"1\"> <longdesc lang=\"en\">IP address</longdesc> <shortdesc lang=\"en\">IP address</shortdesc> <content type=\"string\" default=\"\"/> </parameter> <parameter name=\"cidr_netmask\"> <longdesc lang=\"en\">CIDR netmask</longdesc> <shortdesc lang=\"en\">CIDR netmask</shortdesc> <content type=\"integer\" default=\"24\"/> </parameter> </parameters> <actions> <action name=\"start\" timeout=\"20s\"/> <action name=\"stop\" timeout=\"20s\"/> <action name=\"monitor\" timeout=\"20s\" interval=\"10s\"/> <action name=\"validate-all\" timeout=\"20s\"/> </actions> </resource-agent> LSB - Linux Standard Base Standard SysV init scripts. LSB Compatibility : # Start /etc/init.d/apache2 start # Stop /etc/init.d/apache2 stop # Status /etc/init.d/apache2 status # Reload /etc/init.d/apache2 reload LSB Resource Creation : pcs resource create apache lsb:apache2 systemd Native systemd service support. systemd Resource Creation : pcs resource create myapp systemd:myapp systemd Actions : - start: systemctl start - stop: systemctl stop - monitor: systemctl is-active Resource Standards Standard Prefix Description OCF ocf: Open Cluster Framework LSB lsb: Linux Standard Base systemd systemd: systemd service stonith stonith: Fencing devices Resource Providers Provider Description Example heartbeat Heartbeat project agents ocf:heartbeat:IPaddr2 pacemaker Pacemaker agents ocf:pacemaker:ping redhat Red Hat agents ocf:redhat:apache Resource Management Creating Resources VIP Resource # Basic VIP pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 # VIP with NIC specification pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 \\ nic=eth0 # VIP with ARP ping pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 \\ arp_ping=true Apache Resource # Apache service pcs resource create apache ocf:heartbeat:apache \\ configfile=/etc/apache2/apache2.conf \\ statusurl=http://localhost/server-status \\ client=1 Filesystem Resource # Mount filesystem pcs resource create fs ocf:heartbeat:Filesystem \\ device=/dev/drbd/by-res/data \\ directory=/mnt/data \\ fstype=ext4 NFS Resource # NFS export pcs resource create nfs_export ocf:heartbeat:nfsserver \\ directory=/mnt/data \\ options=rw,no_root_squash Resource Cloning Anonymous Clone # Clone to all nodes pcs resource clone vip # Clone with specific settings pcs resource clone vip clone-max=3 clone-node-max=1 Named Clone # Create named clone pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource clone vip \\ clone-max=3 \\ clone-node-max=1 \\ interleave=true Clone Parameters : - clone-max : Maximum number of copies - clone-node-max : Copies per node - interleave : Clone ordering with multi-state resources - globally-unique : Unique instance per node Master/Slave Resources # Create master/slave resource pcs resource create db ocf:heartbeat:mysql \\ config=/etc/mysql/my.cnf pcs resource master master-db db \\ master-max=1 \\ master-node-max=1 \\ clone-max=3 \\ clone-node-max=1 Master Parameters : - master-max : Maximum master instances - master-node-max : Masters per node - clone-max : Total instances - clone-node-max : Instances per node Resource Groups # Create group pcs resource group add web-group vip apache # Add to existing group pcs resource group add web-group nfs_export # Remove from group pcs resource group remove web-group apache Group Properties : - Resources start/stop together - Ordered start sequence - Colocated on same node - Group-level stickiness Constraints Ordering Constraints Define start/stop order for resources. Basic Ordering # Start vip before apache pcs constraint order start vip then apache # Stop apache before vip pcs constraint order stop apache then vip Ordering with Kind # Mandatory (default) pcs constraint order start vip then apache kind=Mandatory # Optional pcs constraint order start vip then apache kind=Optional # Serialize pcs constraint order start vip then apache kind=Serialize # Symmetrical ordering pcs constraint order start vip then apache symmetrical=true Resource Sets # Order multiple resources pcs constraint order start vip then start apache \\ then start nfs_export # Order with resource sets pcs constraint order \\ start vip then apache \\ kind=Mandatory \\ symmetrical=true Colocation Constraints Define resource placement relationships. Basic Colocation # Apache must run with vip pcs constraint colocation add apache with vip # Apache must run with vip, score 100 pcs constraint colocation add apache with vip score=100 Negative Colocation # Apache must NOT run with vip pcs constraint colocation add apache with vip score=-INFINITY Colocation Score Score Meaning INFINITY Must colocate High Prefer colocate 0 No preference -High Avoid colocate -INFINITY Must NOT colocate Location Constraints Define preferred nodes for resources. Basic Location # Prefer node1 for apache pcs constraint location apache prefers node1=100 # Avoid node2 for apache pcs constraint location apache avoids node2 # Rule-based location pcs constraint location apache rule score=100 node#node1 Rule-Based Location # Rule based on node attributes pcs constraint location apache rule score=100 \\ #uname eq node1 # Rule with multiple conditions pcs constraint location apache rule \\ score=100 #uname eq node1 \\ and #uname eq node2 # Rule with expressions pcs constraint location apache rule \\ score=100 defined #master Rule Operators : | Operator | Meaning | |----------|---------| | eq | Equal | | ne | Not equal | | lt | Less than | | lte | Less than or equal | | gt | Greater than | | gte | Greater than or equal | Constraint Examples Three-Tier Application # Resources pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource create app ocf:heartbeat:apache pcs resource create db ocf:heartbeat:mysql config=/etc/mysql/my.cnf pcs resource master master-db db # Order: DB -> App -> VIP pcs constraint order start master-db then app pcs constraint order start app then vip # Colocation: All on same node pcs constraint colocation add app with master-db pcs constraint colocation add vip with app # Location: Prefer node1 for DB master pcs constraint location master-db rule score=100 \\ #uname eq node1 Active/Active Web Cluster # Clone VIP to all nodes pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource clone vip # Clone Apache to all nodes pcs resource create app ocf:heartbeat:apache pcs resource clone app # Load balancer resource pcs resource create lb ocf:heartbeat:IPaddr2 ip=192.168.1.200 pcs resource clone lb Resource Operations Operation Attributes # Custom monitor interval pcs resource update vip op \\ monitor interval=5s timeout=30s # Start timeout pcs resource update vip op \\ start timeout=20s # Multiple operations pcs resource update vip op \\ start timeout=20s \\ stop timeout=20s \\ monitor interval=10s timeout=20s \\ monitor interval=30s on-fail=restart role=Started Operation Types Operation Purpose start Start resource stop Stop resource monitor Check resource health validate-all Validate configuration reload Reload configuration restart Restart resource promote Promote to master demote Demote to slave notify Notification On-Fail Actions # Restart on failure pcs resource update vip op \\ monitor on-fail=restart # Block on failure pcs resource update vip op \\ monitor on-fail=block # Fence on failure pcs resource update vip op \\ monitor on-fail=fence # Ignore on failure pcs resource update vip op \\ monitor on-fail=ignore STONITH - Fencing STONITH Overview STONITH (Shoot The Other Node In The Head) prevents split-brain by fencing failed nodes. STONITH Devices IPMI Fencing # Create IPMI fence device pcs stonith create fence-ipmi \\ fence_ipmilan \\ ipaddr=192.168.1.200 \\ login=admin \\ passwd=password # Create per-node IPMI fencing pcs stonith create fence-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=password \\ pcmk_host_list=node1 pcs stonith create fence-node2 \\ fence_ipmilan \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=password \\ pcmk_host_list=node2 iLO/DRAC Fencing # iLO fencing pcs stonith create fence-ilo \\ fence_ilo \\ ipaddr=192.168.1.203 \\ login=admin \\ passwd=password \\ pcmk_host_list=node1 # DRAC fencing pcs stonith create fence-drac \\ fence_drac5 \\ ipaddr=192.168.1.204 \\ login=admin \\ passwd=password \\ pcmk_host_list=node2 Virtual Fencing # VMware fencing pcs stonith create fence-vmware \\ fence_vmware_soap \\ ipaddr=192.168.1.210 \\ login=admin \\ passwd=password \\ pcmk_host_list=\"node1 node2 node3\" # Libvirt fencing pcs stonith create fence-libvirt \\ fence_virsh \\ ipaddr=192.168.1.211 \\ login=root \\ passwd=password \\ pcmk_host_list=\"node1 node2 node3\" STONITH Configuration # Enable STONITH pcs property set stonith-enabled=true # Set STONITH timeout pcs property set stonith-timeout=60s # Set STONITH action pcs property set stonith-action=reboot # Set stonith-level for redundancy pcs stonith level add 1 node1 fence-ipmi,fence-vmware pcs stonith level add 2 node1 fence-drac # Test fencing pcs stonith fence node1 STONITH Levels # Primary and secondary fencing pcs stonith level add 1 node1 fence-ipmi pcs stonith level add 2 node1 fence-drac # Multiple fencing methods pcs stonith level add 1 node1 fence-ipmi pcs stonith level add 2 node1 fence-ilo pcs stonith level add 3 node1 fence-vmware # View levels pcs stonith level Resource Agents Writing OCF Agents #!/bin/bash # Description: My custom resource agent # OCF functions . ${OCF_ROOT}/resource.d/heartbeat/.ocf-shellfuncs # Resource metadata meta_data() { cat <<EOF <?xml version=\"1.0\"?> <resource-agent name=\"myresource\"> <version>1.0</version> <longdesc>My custom resource</longdesc> <shortdesc>My custom resource</shortdesc> <parameters> <parameter name=\"config\"> <longdesc>Configuration file</longdesc> <shortdesc>Config file</shortdesc> <content type=\"string\" default=\"\"/> </parameter> </parameters> <actions> <action name=\"start\" timeout=\"20s\"/> <action name=\"stop\" timeout=\"20s\"/> <action name=\"monitor\" timeout=\"20s\"/> <action name=\"validate-all\" timeout=\"20s\"/> </actions> </resource-agent> EOF } # Start resource myresource_start() { # Start logic here return $OCF_SUCCESS } # Stop resource myresource_stop() { # Stop logic here return $OCF_SUCCESS } # Monitor resource myresource_monitor() { # Monitor logic here return $OCF_SUCCESS } # Validate configuration myresource_validate() { # Validate logic here return $OCF_SUCCESS } # Main case \"$1\" in start) myresource_start ;; stop) myresource_stop ;; monitor) myresource_monitor ;; validate-all) myresource_validate ;; meta-data) meta_data ;; *) echo \"Usage: $0 {start|stop|monitor|validate-all|meta-data}\" exit $OCF_ERR_UNIMPLEMENTED ;; esac exit $? Quick Commands Cluster Status # Full status pcs status # Cluster status pcs status cluster # Resource status pcs status resources # Node status pcs status nodes # STONITH status pcs status stonith Resource Management # Create resource pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 # Start resource pcs resource start vip # Stop resource pcs resource stop vip # Delete resource pcs resource delete vip # Show resource pcs resource show vip # Show all resources pcs resource show # Move resource pcs resource move vip node2 # Unmove resource pcs resource clear vip Constraint Management # Show constraints pcs constraint show # Show ordering constraints pcs constraint show --full # Add ordering pcs constraint order start vip then apache # Add colocation pcs constraint colocation add apache with vip # Add location pcs constraint location apache prefers node1 # Remove constraint pcs constraint remove order-vip-apache STONITH Management # Show STONITH devices pcs stonith show # Create STONITH device pcs stonith create fence-ipmi fence_ipmilan ipaddr=192.168.1.200 # Test fencing pcs stonith fence node1 # Enable STONITH pcs property set stonith-enabled=true Nifty Behaviors Resource Stickiness <primitive id=\"apache\" class=\"ocf\" provider=\"heartbeat\" type=\"apache\"> <meta_attributes id=\"apache-meta\"> <nvpair name=\"resource-stickiness\" value=\"100\"/> <nvpair name=\"migration-threshold\" value=\"3\"/> </meta_attributes> </primitive> Nifty : Resources prefer current location but move after multiple failures Batch Constraint Updates # Apply multiple constraints at once pcs constraint order start vip then apache pcs constraint colocation add apache with vip pcs constraint location apache prefers node1 Nifty : Constraints applied atomically, ensure consistency Resource Templates # Create template pcs resource create vip-template ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 \\ clone clone-max=3 # Use template pcs resource create vip2 ocf:heartbeat:IPaddr2 \\ ip=192.168.1.101 cidr_netmask=24 Nifty : Reusable resource configurations Symmetric Clustering # Clone resource to all nodes pcs resource create haproxy ocf:heartbeat:haproxy pcs resource clone haproxy # Interleave with multi-state resource pcs resource create db ocf:heartbeat:mysql pcs resource master master-db db clone-max=3 pcs resource clone haproxy interleave=true Nifty : Active/active configuration with load balancing Resource Operations with Failback # Monitor with restart on failure pcs resource update vip op \\ monitor interval=10s timeout=20s on-fail=restart # Monitor with migration pcs resource update vip op \\ monitor interval=10s timeout=20s on-fail=migrate Nifty : Automatic recovery strategies Performance Tuning Cluster Properties # Transition timeout pcs property set transition-timeout=60s # Start failure is fatal pcs property set start-failure-is-fatal=false # Stop all resources pcs property set stop-orphan-resources=true # Shutdown timeout pcs property set shutdown-timeout=60s # Resource stickiness pcs resource update vip \\ meta resource-stickiness=100 Resource Optimization # Reduce monitor interval pcs resource update vip op \\ monitor interval=5s # Increase timeout for slow resources pcs resource update db op \\ start timeout=60s \\ stop timeout=60s # Enable resource caching pcs resource update vip \\ meta target-role=Started Load Balancing # Distribute resources evenly pcs constraint location apache prefers node1=50 pcs constraint location apache prefers node2=50 # Use placement strategy pcs property set placement-strategy=default Security STONITH Security # Secure IPMI credentials pcs stonith create fence-ipmi fence_ipmilan \\ ipaddr=192.168.1.200 \\ login=admin \\ passwd=secure_password # Use encrypted STONITH pcs property set stonith-enabled=true pcs property set stonith-timeout=60s Access Control # Limit cluster daemon permissions chown root:haclient /etc/corosync/authkey chmod 640 /etc/corosync/authkey # Secure CIB chmod 644 /var/lib/pacemaker/cib Troubleshooting Resource Stuck Starting # Check resource agent pcs resource debug-resource vip # Check logs journalctl -u pacemaker -f # Test resource agent manually ocf_tester -n vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 Constraint Violations # Show constraints pcs constraint show --full # Show transition history pcs resource history # Check current state crm_simulate -Ls STONITH Failures # Test fencing manually fence_ipmilan -a 192.168.1.200 -l admin -p password \\ -o status -m node1 # Check fencing logs journalctl -u stonithd -f # Verify fencing device pcs stonith show --full Best Practices Always enable STONITH in production Use resource stickiness to prevent unnecessary failover Set appropriate timeouts for resources Test failover scenarios regularly Monitor resource health with proper intervals Use constraints wisely - avoid over-constraining Document cluster configuration Implement alerting for cluster events Source Code Repository : https://github.com/ClusterLabs/pacemaker Documentation : https://www.clusterlabs.org/pacemaker/doc/ Key Source Locations Component Location Description CRM lib/crm Cluster Resource Manager PE lib/pengine Policy Engine TE lib/transitioner Transition Engine LRM lib/lrmd Local Resource Manager CIB lib/cib Cluster Information Base","title":"Pacemaker"},{"location":"cluster/pacemaker/#architecture","text":"graph TB subgraph \"Cluster Layer\" A[CRM - Cluster Resource Manager] B[PE - Policy Engine] C[TE - Transition Engine] D[LRM - Local Resource Manager] E[CIB - Cluster Information Base] F[STONITH] G[Quorum] end subgraph \"Node Layer\" H[Node 1] I[Node 2] J[Node 3] K[Resource Agents] L[LRM Processes] M[Applications] end A --> B A --> C A --> D A --> E A --> F A --> G B --> N[Calculate Transitions] C --> O[Execute Actions] D --> P[Monitor Resources] E --> Q[Cluster State Database] D --> H D --> I D --> J H --> K H --> L H --> M I --> K I --> L I --> M J --> K J --> L J --> M style A fill:#c8e6c9 style B fill:#e1f5ff style C fill:#ffecb3 style E fill:#fff3e0","title":"Architecture"},{"location":"cluster/pacemaker/#core-components","text":"","title":"Core Components"},{"location":"cluster/pacemaker/#crm-cluster-resource-manager","text":"Central coordinator for all cluster operations. Responsibilities : - Coordinate PE, TE, LRM components - Handle quorum and cluster membership - Process resource operations - Manage STONITH operations - Distribute cluster configuration Communication Flow : sequenceDiagram participant C as CRM participant P as PE participant T as TE participant L as LRM C->>P: Calculate state transitions P->>C: Return transition graph C->>T: Execute transitions T->>L: Resource operations L->>C: Operation results C->>C: Update CIB","title":"CRM - Cluster Resource Manager"},{"location":"cluster/pacemaker/#pe-policy-engine","text":"Calculates optimal resource placement and transitions. Decision Process : 1. Analyze current cluster state 2. Evaluate constraints and priorities 3. Calculate optimal transitions 4. Generate transition graph 5. Submit to TE for execution Algorithm : - Score-based optimization - Constraint satisfaction - Failover calculation - Load balancing - Resource stickiness consideration PE States : - Idle: No calculations in progress - Pending: Waiting for quorum or data - Active: Calculating transitions","title":"PE - Policy Engine"},{"location":"cluster/pacemaker/#te-transition-engine","text":"Executes state transitions calculated by PE. Transition Types : - Start: Start resource on node - Stop: Stop resource on node - Monitor: Check resource health - Promote/Demote: State change for master/slave resources - Migrate: Move resource between nodes - Recover: Handle resource failure Execution Flow : 1. Receive transition graph 2. Validate dependencies 3. Execute operations in order 4. Wait for completion 5. Report results 6. Update cluster state","title":"TE - Transition Engine"},{"location":"cluster/pacemaker/#lrm-local-resource-manager","text":"Manages resources on individual nodes. LRM Process : - Resource agent execution - Operation tracking - Status reporting - Timeout handling - Error handling LRM States : - Idle: No active operations - Busy: Executing operation - Stopped: Process stopped","title":"LRM - Local Resource Manager"},{"location":"cluster/pacemaker/#cib-cluster-information-base","text":"Distributed database storing cluster configuration and state. CIB Structure : <cib> <configuration> <crm_config> <!-- Cluster properties --> <nodes> <!-- Node definitions --> <resources> <!-- Resource definitions --> <constraints> <!-- Resource constraints --> </configuration> <status> <node_state> <!-- Node status --> <lrm_resource> <!-- Resource status --> </status> </cib> CIB Distribution : - Replicated to all nodes - Version-controlled - Automatic synchronization - Conflict resolution CIB Updates : # Show CIB pcs config # Export CIB pcs cluster cib > cluster.xml # Edit CIB pcs cluster cib edit # Push CIB pcs cluster cib-push cluster.xml","title":"CIB - Cluster Information Base"},{"location":"cluster/pacemaker/#resource-types","text":"","title":"Resource Types"},{"location":"cluster/pacemaker/#resource-classes","text":"","title":"Resource Classes"},{"location":"cluster/pacemaker/#ocf-open-cluster-framework","text":"Standard resource agent interface. OCF Actions : # Start <resource-agent> start # Stop <resource-agent> stop # Monitor <resource-agent> monitor # Validate configuration <resource-agent> validate-all # Metadata <resource-agent> meta-data OCF Exit Codes : | Code | Meaning | |------|---------| | 0 | Success | | 7 | Not running | | 1 | Generic error | | 2 | Invalid parameters | | 3 | Unimplemented feature | | 4 | Insufficient privileges | | 5 | Not installed | | 6 | Not configured | OCF Metadata Example : <?xml version=\"1.0\"?> <resource-agent name=\"IPaddr2\"> <version>1.0</version> <longdesc lang=\"en\">IP address management</longdesc> <shortdesc lang=\"en\">IP address management</shortdesc> <parameters> <parameter name=\"ip\" unique=\"1\"> <longdesc lang=\"en\">IP address</longdesc> <shortdesc lang=\"en\">IP address</shortdesc> <content type=\"string\" default=\"\"/> </parameter> <parameter name=\"cidr_netmask\"> <longdesc lang=\"en\">CIDR netmask</longdesc> <shortdesc lang=\"en\">CIDR netmask</shortdesc> <content type=\"integer\" default=\"24\"/> </parameter> </parameters> <actions> <action name=\"start\" timeout=\"20s\"/> <action name=\"stop\" timeout=\"20s\"/> <action name=\"monitor\" timeout=\"20s\" interval=\"10s\"/> <action name=\"validate-all\" timeout=\"20s\"/> </actions> </resource-agent>","title":"OCF - Open Cluster Framework"},{"location":"cluster/pacemaker/#lsb-linux-standard-base","text":"Standard SysV init scripts. LSB Compatibility : # Start /etc/init.d/apache2 start # Stop /etc/init.d/apache2 stop # Status /etc/init.d/apache2 status # Reload /etc/init.d/apache2 reload LSB Resource Creation : pcs resource create apache lsb:apache2","title":"LSB - Linux Standard Base"},{"location":"cluster/pacemaker/#systemd","text":"Native systemd service support. systemd Resource Creation : pcs resource create myapp systemd:myapp systemd Actions : - start: systemctl start - stop: systemctl stop - monitor: systemctl is-active","title":"systemd"},{"location":"cluster/pacemaker/#resource-standards","text":"Standard Prefix Description OCF ocf: Open Cluster Framework LSB lsb: Linux Standard Base systemd systemd: systemd service stonith stonith: Fencing devices","title":"Resource Standards"},{"location":"cluster/pacemaker/#resource-providers","text":"Provider Description Example heartbeat Heartbeat project agents ocf:heartbeat:IPaddr2 pacemaker Pacemaker agents ocf:pacemaker:ping redhat Red Hat agents ocf:redhat:apache","title":"Resource Providers"},{"location":"cluster/pacemaker/#resource-management","text":"","title":"Resource Management"},{"location":"cluster/pacemaker/#creating-resources","text":"","title":"Creating Resources"},{"location":"cluster/pacemaker/#vip-resource","text":"# Basic VIP pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 # VIP with NIC specification pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 \\ nic=eth0 # VIP with ARP ping pcs resource create vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 \\ arp_ping=true","title":"VIP Resource"},{"location":"cluster/pacemaker/#apache-resource","text":"# Apache service pcs resource create apache ocf:heartbeat:apache \\ configfile=/etc/apache2/apache2.conf \\ statusurl=http://localhost/server-status \\ client=1","title":"Apache Resource"},{"location":"cluster/pacemaker/#filesystem-resource","text":"# Mount filesystem pcs resource create fs ocf:heartbeat:Filesystem \\ device=/dev/drbd/by-res/data \\ directory=/mnt/data \\ fstype=ext4","title":"Filesystem Resource"},{"location":"cluster/pacemaker/#nfs-resource","text":"# NFS export pcs resource create nfs_export ocf:heartbeat:nfsserver \\ directory=/mnt/data \\ options=rw,no_root_squash","title":"NFS Resource"},{"location":"cluster/pacemaker/#resource-cloning","text":"","title":"Resource Cloning"},{"location":"cluster/pacemaker/#anonymous-clone","text":"# Clone to all nodes pcs resource clone vip # Clone with specific settings pcs resource clone vip clone-max=3 clone-node-max=1","title":"Anonymous Clone"},{"location":"cluster/pacemaker/#named-clone","text":"# Create named clone pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource clone vip \\ clone-max=3 \\ clone-node-max=1 \\ interleave=true Clone Parameters : - clone-max : Maximum number of copies - clone-node-max : Copies per node - interleave : Clone ordering with multi-state resources - globally-unique : Unique instance per node","title":"Named Clone"},{"location":"cluster/pacemaker/#masterslave-resources","text":"# Create master/slave resource pcs resource create db ocf:heartbeat:mysql \\ config=/etc/mysql/my.cnf pcs resource master master-db db \\ master-max=1 \\ master-node-max=1 \\ clone-max=3 \\ clone-node-max=1 Master Parameters : - master-max : Maximum master instances - master-node-max : Masters per node - clone-max : Total instances - clone-node-max : Instances per node","title":"Master/Slave Resources"},{"location":"cluster/pacemaker/#resource-groups","text":"# Create group pcs resource group add web-group vip apache # Add to existing group pcs resource group add web-group nfs_export # Remove from group pcs resource group remove web-group apache Group Properties : - Resources start/stop together - Ordered start sequence - Colocated on same node - Group-level stickiness","title":"Resource Groups"},{"location":"cluster/pacemaker/#constraints","text":"","title":"Constraints"},{"location":"cluster/pacemaker/#ordering-constraints","text":"Define start/stop order for resources.","title":"Ordering Constraints"},{"location":"cluster/pacemaker/#basic-ordering","text":"# Start vip before apache pcs constraint order start vip then apache # Stop apache before vip pcs constraint order stop apache then vip","title":"Basic Ordering"},{"location":"cluster/pacemaker/#ordering-with-kind","text":"# Mandatory (default) pcs constraint order start vip then apache kind=Mandatory # Optional pcs constraint order start vip then apache kind=Optional # Serialize pcs constraint order start vip then apache kind=Serialize # Symmetrical ordering pcs constraint order start vip then apache symmetrical=true","title":"Ordering with Kind"},{"location":"cluster/pacemaker/#resource-sets","text":"# Order multiple resources pcs constraint order start vip then start apache \\ then start nfs_export # Order with resource sets pcs constraint order \\ start vip then apache \\ kind=Mandatory \\ symmetrical=true","title":"Resource Sets"},{"location":"cluster/pacemaker/#colocation-constraints","text":"Define resource placement relationships.","title":"Colocation Constraints"},{"location":"cluster/pacemaker/#basic-colocation","text":"# Apache must run with vip pcs constraint colocation add apache with vip # Apache must run with vip, score 100 pcs constraint colocation add apache with vip score=100","title":"Basic Colocation"},{"location":"cluster/pacemaker/#negative-colocation","text":"# Apache must NOT run with vip pcs constraint colocation add apache with vip score=-INFINITY","title":"Negative Colocation"},{"location":"cluster/pacemaker/#colocation-score","text":"Score Meaning INFINITY Must colocate High Prefer colocate 0 No preference -High Avoid colocate -INFINITY Must NOT colocate","title":"Colocation Score"},{"location":"cluster/pacemaker/#location-constraints","text":"Define preferred nodes for resources.","title":"Location Constraints"},{"location":"cluster/pacemaker/#basic-location","text":"# Prefer node1 for apache pcs constraint location apache prefers node1=100 # Avoid node2 for apache pcs constraint location apache avoids node2 # Rule-based location pcs constraint location apache rule score=100 node#node1","title":"Basic Location"},{"location":"cluster/pacemaker/#rule-based-location","text":"# Rule based on node attributes pcs constraint location apache rule score=100 \\ #uname eq node1 # Rule with multiple conditions pcs constraint location apache rule \\ score=100 #uname eq node1 \\ and #uname eq node2 # Rule with expressions pcs constraint location apache rule \\ score=100 defined #master Rule Operators : | Operator | Meaning | |----------|---------| | eq | Equal | | ne | Not equal | | lt | Less than | | lte | Less than or equal | | gt | Greater than | | gte | Greater than or equal |","title":"Rule-Based Location"},{"location":"cluster/pacemaker/#constraint-examples","text":"","title":"Constraint Examples"},{"location":"cluster/pacemaker/#three-tier-application","text":"# Resources pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource create app ocf:heartbeat:apache pcs resource create db ocf:heartbeat:mysql config=/etc/mysql/my.cnf pcs resource master master-db db # Order: DB -> App -> VIP pcs constraint order start master-db then app pcs constraint order start app then vip # Colocation: All on same node pcs constraint colocation add app with master-db pcs constraint colocation add vip with app # Location: Prefer node1 for DB master pcs constraint location master-db rule score=100 \\ #uname eq node1","title":"Three-Tier Application"},{"location":"cluster/pacemaker/#activeactive-web-cluster","text":"# Clone VIP to all nodes pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 pcs resource clone vip # Clone Apache to all nodes pcs resource create app ocf:heartbeat:apache pcs resource clone app # Load balancer resource pcs resource create lb ocf:heartbeat:IPaddr2 ip=192.168.1.200 pcs resource clone lb","title":"Active/Active Web Cluster"},{"location":"cluster/pacemaker/#resource-operations","text":"","title":"Resource Operations"},{"location":"cluster/pacemaker/#operation-attributes","text":"# Custom monitor interval pcs resource update vip op \\ monitor interval=5s timeout=30s # Start timeout pcs resource update vip op \\ start timeout=20s # Multiple operations pcs resource update vip op \\ start timeout=20s \\ stop timeout=20s \\ monitor interval=10s timeout=20s \\ monitor interval=30s on-fail=restart role=Started","title":"Operation Attributes"},{"location":"cluster/pacemaker/#operation-types","text":"Operation Purpose start Start resource stop Stop resource monitor Check resource health validate-all Validate configuration reload Reload configuration restart Restart resource promote Promote to master demote Demote to slave notify Notification","title":"Operation Types"},{"location":"cluster/pacemaker/#on-fail-actions","text":"# Restart on failure pcs resource update vip op \\ monitor on-fail=restart # Block on failure pcs resource update vip op \\ monitor on-fail=block # Fence on failure pcs resource update vip op \\ monitor on-fail=fence # Ignore on failure pcs resource update vip op \\ monitor on-fail=ignore","title":"On-Fail Actions"},{"location":"cluster/pacemaker/#stonith-fencing","text":"","title":"STONITH - Fencing"},{"location":"cluster/pacemaker/#stonith-overview","text":"STONITH (Shoot The Other Node In The Head) prevents split-brain by fencing failed nodes.","title":"STONITH Overview"},{"location":"cluster/pacemaker/#stonith-devices","text":"","title":"STONITH Devices"},{"location":"cluster/pacemaker/#ipmi-fencing","text":"# Create IPMI fence device pcs stonith create fence-ipmi \\ fence_ipmilan \\ ipaddr=192.168.1.200 \\ login=admin \\ passwd=password # Create per-node IPMI fencing pcs stonith create fence-node1 \\ fence_ipmilan \\ ipaddr=192.168.1.201 \\ login=admin \\ passwd=password \\ pcmk_host_list=node1 pcs stonith create fence-node2 \\ fence_ipmilan \\ ipaddr=192.168.1.202 \\ login=admin \\ passwd=password \\ pcmk_host_list=node2","title":"IPMI Fencing"},{"location":"cluster/pacemaker/#ilodrac-fencing","text":"# iLO fencing pcs stonith create fence-ilo \\ fence_ilo \\ ipaddr=192.168.1.203 \\ login=admin \\ passwd=password \\ pcmk_host_list=node1 # DRAC fencing pcs stonith create fence-drac \\ fence_drac5 \\ ipaddr=192.168.1.204 \\ login=admin \\ passwd=password \\ pcmk_host_list=node2","title":"iLO/DRAC Fencing"},{"location":"cluster/pacemaker/#virtual-fencing","text":"# VMware fencing pcs stonith create fence-vmware \\ fence_vmware_soap \\ ipaddr=192.168.1.210 \\ login=admin \\ passwd=password \\ pcmk_host_list=\"node1 node2 node3\" # Libvirt fencing pcs stonith create fence-libvirt \\ fence_virsh \\ ipaddr=192.168.1.211 \\ login=root \\ passwd=password \\ pcmk_host_list=\"node1 node2 node3\"","title":"Virtual Fencing"},{"location":"cluster/pacemaker/#stonith-configuration","text":"# Enable STONITH pcs property set stonith-enabled=true # Set STONITH timeout pcs property set stonith-timeout=60s # Set STONITH action pcs property set stonith-action=reboot # Set stonith-level for redundancy pcs stonith level add 1 node1 fence-ipmi,fence-vmware pcs stonith level add 2 node1 fence-drac # Test fencing pcs stonith fence node1","title":"STONITH Configuration"},{"location":"cluster/pacemaker/#stonith-levels","text":"# Primary and secondary fencing pcs stonith level add 1 node1 fence-ipmi pcs stonith level add 2 node1 fence-drac # Multiple fencing methods pcs stonith level add 1 node1 fence-ipmi pcs stonith level add 2 node1 fence-ilo pcs stonith level add 3 node1 fence-vmware # View levels pcs stonith level","title":"STONITH Levels"},{"location":"cluster/pacemaker/#resource-agents","text":"","title":"Resource Agents"},{"location":"cluster/pacemaker/#writing-ocf-agents","text":"#!/bin/bash # Description: My custom resource agent # OCF functions . ${OCF_ROOT}/resource.d/heartbeat/.ocf-shellfuncs # Resource metadata meta_data() { cat <<EOF <?xml version=\"1.0\"?> <resource-agent name=\"myresource\"> <version>1.0</version> <longdesc>My custom resource</longdesc> <shortdesc>My custom resource</shortdesc> <parameters> <parameter name=\"config\"> <longdesc>Configuration file</longdesc> <shortdesc>Config file</shortdesc> <content type=\"string\" default=\"\"/> </parameter> </parameters> <actions> <action name=\"start\" timeout=\"20s\"/> <action name=\"stop\" timeout=\"20s\"/> <action name=\"monitor\" timeout=\"20s\"/> <action name=\"validate-all\" timeout=\"20s\"/> </actions> </resource-agent> EOF } # Start resource myresource_start() { # Start logic here return $OCF_SUCCESS } # Stop resource myresource_stop() { # Stop logic here return $OCF_SUCCESS } # Monitor resource myresource_monitor() { # Monitor logic here return $OCF_SUCCESS } # Validate configuration myresource_validate() { # Validate logic here return $OCF_SUCCESS } # Main case \"$1\" in start) myresource_start ;; stop) myresource_stop ;; monitor) myresource_monitor ;; validate-all) myresource_validate ;; meta-data) meta_data ;; *) echo \"Usage: $0 {start|stop|monitor|validate-all|meta-data}\" exit $OCF_ERR_UNIMPLEMENTED ;; esac exit $?","title":"Writing OCF Agents"},{"location":"cluster/pacemaker/#quick-commands","text":"","title":"Quick Commands"},{"location":"cluster/pacemaker/#cluster-status","text":"# Full status pcs status # Cluster status pcs status cluster # Resource status pcs status resources # Node status pcs status nodes # STONITH status pcs status stonith","title":"Cluster Status"},{"location":"cluster/pacemaker/#resource-management_1","text":"# Create resource pcs resource create vip ocf:heartbeat:IPaddr2 ip=192.168.1.100 # Start resource pcs resource start vip # Stop resource pcs resource stop vip # Delete resource pcs resource delete vip # Show resource pcs resource show vip # Show all resources pcs resource show # Move resource pcs resource move vip node2 # Unmove resource pcs resource clear vip","title":"Resource Management"},{"location":"cluster/pacemaker/#constraint-management","text":"# Show constraints pcs constraint show # Show ordering constraints pcs constraint show --full # Add ordering pcs constraint order start vip then apache # Add colocation pcs constraint colocation add apache with vip # Add location pcs constraint location apache prefers node1 # Remove constraint pcs constraint remove order-vip-apache","title":"Constraint Management"},{"location":"cluster/pacemaker/#stonith-management","text":"# Show STONITH devices pcs stonith show # Create STONITH device pcs stonith create fence-ipmi fence_ipmilan ipaddr=192.168.1.200 # Test fencing pcs stonith fence node1 # Enable STONITH pcs property set stonith-enabled=true","title":"STONITH Management"},{"location":"cluster/pacemaker/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"cluster/pacemaker/#resource-stickiness","text":"<primitive id=\"apache\" class=\"ocf\" provider=\"heartbeat\" type=\"apache\"> <meta_attributes id=\"apache-meta\"> <nvpair name=\"resource-stickiness\" value=\"100\"/> <nvpair name=\"migration-threshold\" value=\"3\"/> </meta_attributes> </primitive> Nifty : Resources prefer current location but move after multiple failures","title":"Resource Stickiness"},{"location":"cluster/pacemaker/#batch-constraint-updates","text":"# Apply multiple constraints at once pcs constraint order start vip then apache pcs constraint colocation add apache with vip pcs constraint location apache prefers node1 Nifty : Constraints applied atomically, ensure consistency","title":"Batch Constraint Updates"},{"location":"cluster/pacemaker/#resource-templates","text":"# Create template pcs resource create vip-template ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100 cidr_netmask=24 \\ clone clone-max=3 # Use template pcs resource create vip2 ocf:heartbeat:IPaddr2 \\ ip=192.168.1.101 cidr_netmask=24 Nifty : Reusable resource configurations","title":"Resource Templates"},{"location":"cluster/pacemaker/#symmetric-clustering","text":"# Clone resource to all nodes pcs resource create haproxy ocf:heartbeat:haproxy pcs resource clone haproxy # Interleave with multi-state resource pcs resource create db ocf:heartbeat:mysql pcs resource master master-db db clone-max=3 pcs resource clone haproxy interleave=true Nifty : Active/active configuration with load balancing","title":"Symmetric Clustering"},{"location":"cluster/pacemaker/#resource-operations-with-failback","text":"# Monitor with restart on failure pcs resource update vip op \\ monitor interval=10s timeout=20s on-fail=restart # Monitor with migration pcs resource update vip op \\ monitor interval=10s timeout=20s on-fail=migrate Nifty : Automatic recovery strategies","title":"Resource Operations with Failback"},{"location":"cluster/pacemaker/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"cluster/pacemaker/#cluster-properties","text":"# Transition timeout pcs property set transition-timeout=60s # Start failure is fatal pcs property set start-failure-is-fatal=false # Stop all resources pcs property set stop-orphan-resources=true # Shutdown timeout pcs property set shutdown-timeout=60s # Resource stickiness pcs resource update vip \\ meta resource-stickiness=100","title":"Cluster Properties"},{"location":"cluster/pacemaker/#resource-optimization","text":"# Reduce monitor interval pcs resource update vip op \\ monitor interval=5s # Increase timeout for slow resources pcs resource update db op \\ start timeout=60s \\ stop timeout=60s # Enable resource caching pcs resource update vip \\ meta target-role=Started","title":"Resource Optimization"},{"location":"cluster/pacemaker/#load-balancing","text":"# Distribute resources evenly pcs constraint location apache prefers node1=50 pcs constraint location apache prefers node2=50 # Use placement strategy pcs property set placement-strategy=default","title":"Load Balancing"},{"location":"cluster/pacemaker/#security","text":"","title":"Security"},{"location":"cluster/pacemaker/#stonith-security","text":"# Secure IPMI credentials pcs stonith create fence-ipmi fence_ipmilan \\ ipaddr=192.168.1.200 \\ login=admin \\ passwd=secure_password # Use encrypted STONITH pcs property set stonith-enabled=true pcs property set stonith-timeout=60s","title":"STONITH Security"},{"location":"cluster/pacemaker/#access-control","text":"# Limit cluster daemon permissions chown root:haclient /etc/corosync/authkey chmod 640 /etc/corosync/authkey # Secure CIB chmod 644 /var/lib/pacemaker/cib","title":"Access Control"},{"location":"cluster/pacemaker/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"cluster/pacemaker/#resource-stuck-starting","text":"# Check resource agent pcs resource debug-resource vip # Check logs journalctl -u pacemaker -f # Test resource agent manually ocf_tester -n vip ocf:heartbeat:IPaddr2 \\ ip=192.168.1.100","title":"Resource Stuck Starting"},{"location":"cluster/pacemaker/#constraint-violations","text":"# Show constraints pcs constraint show --full # Show transition history pcs resource history # Check current state crm_simulate -Ls","title":"Constraint Violations"},{"location":"cluster/pacemaker/#stonith-failures","text":"# Test fencing manually fence_ipmilan -a 192.168.1.200 -l admin -p password \\ -o status -m node1 # Check fencing logs journalctl -u stonithd -f # Verify fencing device pcs stonith show --full","title":"STONITH Failures"},{"location":"cluster/pacemaker/#best-practices","text":"Always enable STONITH in production Use resource stickiness to prevent unnecessary failover Set appropriate timeouts for resources Test failover scenarios regularly Monitor resource health with proper intervals Use constraints wisely - avoid over-constraining Document cluster configuration Implement alerting for cluster events","title":"Best Practices"},{"location":"cluster/pacemaker/#source-code","text":"Repository : https://github.com/ClusterLabs/pacemaker Documentation : https://www.clusterlabs.org/pacemaker/doc/","title":"Source Code"},{"location":"cluster/pacemaker/#key-source-locations","text":"Component Location Description CRM lib/crm Cluster Resource Manager PE lib/pengine Policy Engine TE lib/transitioner Transition Engine LRM lib/lrmd Local Resource Manager CIB lib/cib Cluster Information Base","title":"Key Source Locations"},{"location":"network/libvirt-network/","text":"Network management for virtualization environments. Network Types graph TB subgraph \"libvirt Networks\" A[NAT Network] B[Bridged Network] C[Isolated Network] D[Direct Passthrough] end subgraph \"Backend\" E[Linux Bridge] F[Open vSwitch] G[MACvtap] end A --> E B --> F C --> F D --> F style A fill:#c8e6c9 style B fill:#bbdefb style C fill:#e1f5fe style D fill:#ff9800 style E fill:#ffecb3 style F fill:#a5d6a7 Quick Commands # Network management virsh net-list virsh net-define network.xml virsh net-start <network-name> # Bridge commands brctl show brctl addbr br0 brctl delbr br0","title":"libvirt Networking"},{"location":"network/libvirt-network/#network-types","text":"graph TB subgraph \"libvirt Networks\" A[NAT Network] B[Bridged Network] C[Isolated Network] D[Direct Passthrough] end subgraph \"Backend\" E[Linux Bridge] F[Open vSwitch] G[MACvtap] end A --> E B --> F C --> F D --> F style A fill:#c8e6c9 style B fill:#bbdefb style C fill:#e1f5fe style D fill:#ff9800 style E fill:#ffecb3 style F fill:#a5d6a7","title":"Network Types"},{"location":"network/libvirt-network/#quick-commands","text":"# Network management virsh net-list virsh net-define network.xml virsh net-start <network-name> # Bridge commands brctl show brctl addbr br0 brctl delbr br0","title":"Quick Commands"},{"location":"storage/","text":"Complete reference guide for CEPH, iSCSI, NVMe-oF, and GFS2 storage technologies. Topics CEPH - Distributed storage system iSCSI - SCSI gateway for Ceph NVMe-oF - NVMe over Fabrics gateway GFS2 - Global File System 2 Quick Reference CEPH Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image iSCSI Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets NVMe-oF Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems GFS2 Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes CEPH Distributed storage system providing object, block, and file storage in unified platform. Architecture graph TB A[Clients] --> B[RADOS] B --> C[MON Cluster] B --> D[OSD Daemons] B --> E[MDS Daemons] B --> F[RGW] C --> G[Monitor Map] D --> H[OSD Map] E --> I[MDS Map] Key Features Object storage with S3/Swift API Block device (RBD) with kernel support POSIX-compliant file system (CephFS) Erasure coding for data protection Automatic data rebalancing Quick Commands # Cluster status ceph -s ceph health # Storage management ceph osd pool create rbd 64 rbd create rbd/image1 --size 100G rbd map rbd/image1 # CephFS ceph fs new myfs metadata data mount -t ceph <mon-ip>:6789:/ /mnt/cephfs Source Code Repository : ceph/ceph Documentation : docs.ceph.com iSCSI with CEPH iSCSI gateway presenting RBD images as SCSI disks over TCP/IP network. Architecture graph TB A[iSCSI Initiator] --> B[LIO Target] B --> C[TCMU Backend] C --> D[RBD Library] D --> E[RADOS Cluster] A --> F[TCP/IP Network] Key Features LIO target framework for SCSI protocol TCMU userspace passthrough RBD backend for Ceph integration CHAP authentication support Multipath I/O support Quick Commands # Deploy gateway ceph orch apply iscsi gateway.yml # Target management gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1 # Initiator configuration iscsiadm -m discovery -t st -p <target-ip> iscsiadm -m node -T <target-iqn> -p <target-ip> --login Source Code Repository : ceph/ceph Documentation : docs.ceph.com/rbd/iscsi-overview/ NVMe-oF with CEPH NVMe over Fabrics gateway providing high-performance block access. Architecture graph TB A[NVMe Host] --> B[NVMe-oF Gateway] B --> C[SPDK/RBD] C --> D[RADOS Cluster] A --> E[TCP/IP or RDMA] Key Features NVMe/TCP protocol for block access SPDK integration for high performance HA with gateway groups Load balancing across gateways RDMA/RoCE support Quick Commands # Deploy gateway ceph orch apply nvmeof gateway.yml # Subsystem management gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 \\ --pool rbd --image disk1 # Initiator connection nvme connect -t tcp -n <subsystem-nqn> \\ -a <gateway-ip> -s 4420 Source Code Repository : ceph/ceph Documentation : docs.ceph.com/rbd/nvmeof-overview/ GFS2 Global File System 2 for shared-disk file system in Linux clusters. Architecture graph TB A[GFS2 Node 1] --> B[GFS2 Module] A --> C[DLM Lock Manager] A --> D[Shared Block Device] E[GFS2 Node 2] --> F[GFS2 Module] E --> G[DLM Lock Manager] E --> D D --> H[Lock Synchronization] Key Features POSIX-compliant file system Distributed lock manager (DLM) Cluster-wide volume management (CLVM) Journaling for metadata integrity Quota support Quick Commands # Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 # DLM management dlm_tool ls dlm_tool dump Source Code Location : fs/gfs2/ in Linux kernel Repository : torvalds/linux Documentation : /usr/share/doc/gfs2-utils/ Deployment Workflow 1. CEPH HCI Setup # Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm # Bootstrap cluster ./cephadm bootstrap --mon-ip <mon-ip> # Add storage ceph orch apply osd --all-available-devices # Verify ceph -s ceph status 2. iSCSI Gateway Setup # Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply iscsi gateway.yml # Create target gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1 3. NVMe-oF Gateway Setup # Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply nvmeof gateway.yml # Create subsystem gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 --pool rbd --image disk1 4. GFS2 Setup # Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 Security Considerations Enable CephX encryption Use CHAP authentication for iSCSI Use RDMA/RoCE for secure transport Implement proper network segmentation Source Code References Technology Repository Documentation CEPH ceph/ceph docs.ceph.com iSCSI ceph/ceph linux-iscsi.org NVMe-oF ceph/ceph nvmexpress.org GFS2 torvalds/linux gfs2-utils Troubleshooting For in-depth troubleshooting focused on code behavior and diagnostics, see Cluster Technologies section. Common Issues Issue Solution OSD high latency Check ceph tell osd.* iostat Cannot discover iSCSI Verify iscsiadm -m discovery NVMe connection fails Check gateway connectivity GFS2 stale locks Run dlm_tool dump to clear","title":"Overview"},{"location":"storage/#topics","text":"CEPH - Distributed storage system iSCSI - SCSI gateway for Ceph NVMe-oF - NVMe over Fabrics gateway GFS2 - Global File System 2","title":"Topics"},{"location":"storage/#quick-reference","text":"","title":"Quick Reference"},{"location":"storage/#ceph","text":"Command Description ceph -s Cluster status ceph health Health check rbd create Create RBD image rbd map Map RBD image","title":"CEPH"},{"location":"storage/#iscsi","text":"Command Description iscsiadm -m discovery Discover targets iscsiadm -m session List sessions gwcli.py target list List targets","title":"iSCSI"},{"location":"storage/#nvme-of","text":"Command Description nvme list List NVMe devices nvme discover Discover subsystems gwcli.py subsystem list List subsystems","title":"NVMe-oF"},{"location":"storage/#gfs2","text":"Command Description gfs2_tool sb Show superblock gfs2_tool df Show usage dlm_tool ls List DLM nodes","title":"GFS2"},{"location":"storage/#ceph_1","text":"Distributed storage system providing object, block, and file storage in unified platform.","title":"CEPH"},{"location":"storage/#architecture","text":"graph TB A[Clients] --> B[RADOS] B --> C[MON Cluster] B --> D[OSD Daemons] B --> E[MDS Daemons] B --> F[RGW] C --> G[Monitor Map] D --> H[OSD Map] E --> I[MDS Map]","title":"Architecture"},{"location":"storage/#key-features","text":"Object storage with S3/Swift API Block device (RBD) with kernel support POSIX-compliant file system (CephFS) Erasure coding for data protection Automatic data rebalancing","title":"Key Features"},{"location":"storage/#quick-commands","text":"# Cluster status ceph -s ceph health # Storage management ceph osd pool create rbd 64 rbd create rbd/image1 --size 100G rbd map rbd/image1 # CephFS ceph fs new myfs metadata data mount -t ceph <mon-ip>:6789:/ /mnt/cephfs","title":"Quick Commands"},{"location":"storage/#source-code","text":"Repository : ceph/ceph Documentation : docs.ceph.com","title":"Source Code"},{"location":"storage/#iscsi-with-ceph","text":"iSCSI gateway presenting RBD images as SCSI disks over TCP/IP network.","title":"iSCSI with CEPH"},{"location":"storage/#architecture_1","text":"graph TB A[iSCSI Initiator] --> B[LIO Target] B --> C[TCMU Backend] C --> D[RBD Library] D --> E[RADOS Cluster] A --> F[TCP/IP Network]","title":"Architecture"},{"location":"storage/#key-features_1","text":"LIO target framework for SCSI protocol TCMU userspace passthrough RBD backend for Ceph integration CHAP authentication support Multipath I/O support","title":"Key Features"},{"location":"storage/#quick-commands_1","text":"# Deploy gateway ceph orch apply iscsi gateway.yml # Target management gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1 # Initiator configuration iscsiadm -m discovery -t st -p <target-ip> iscsiadm -m node -T <target-iqn> -p <target-ip> --login","title":"Quick Commands"},{"location":"storage/#source-code_1","text":"Repository : ceph/ceph Documentation : docs.ceph.com/rbd/iscsi-overview/","title":"Source Code"},{"location":"storage/#nvme-of-with-ceph","text":"NVMe over Fabrics gateway providing high-performance block access.","title":"NVMe-oF with CEPH"},{"location":"storage/#architecture_2","text":"graph TB A[NVMe Host] --> B[NVMe-oF Gateway] B --> C[SPDK/RBD] C --> D[RADOS Cluster] A --> E[TCP/IP or RDMA]","title":"Architecture"},{"location":"storage/#key-features_2","text":"NVMe/TCP protocol for block access SPDK integration for high performance HA with gateway groups Load balancing across gateways RDMA/RoCE support","title":"Key Features"},{"location":"storage/#quick-commands_2","text":"# Deploy gateway ceph orch apply nvmeof gateway.yml # Subsystem management gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 \\ --pool rbd --image disk1 # Initiator connection nvme connect -t tcp -n <subsystem-nqn> \\ -a <gateway-ip> -s 4420","title":"Quick Commands"},{"location":"storage/#source-code_2","text":"Repository : ceph/ceph Documentation : docs.ceph.com/rbd/nvmeof-overview/","title":"Source Code"},{"location":"storage/#gfs2_1","text":"Global File System 2 for shared-disk file system in Linux clusters.","title":"GFS2"},{"location":"storage/#architecture_3","text":"graph TB A[GFS2 Node 1] --> B[GFS2 Module] A --> C[DLM Lock Manager] A --> D[Shared Block Device] E[GFS2 Node 2] --> F[GFS2 Module] E --> G[DLM Lock Manager] E --> D D --> H[Lock Synchronization]","title":"Architecture"},{"location":"storage/#key-features_3","text":"POSIX-compliant file system Distributed lock manager (DLM) Cluster-wide volume management (CLVM) Journaling for metadata integrity Quota support","title":"Key Features"},{"location":"storage/#quick-commands_3","text":"# Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2 # DLM management dlm_tool ls dlm_tool dump","title":"Quick Commands"},{"location":"storage/#source-code_3","text":"Location : fs/gfs2/ in Linux kernel Repository : torvalds/linux Documentation : /usr/share/doc/gfs2-utils/","title":"Source Code"},{"location":"storage/#deployment-workflow","text":"","title":"Deployment Workflow"},{"location":"storage/#1-ceph-hci-setup","text":"# Install cephadm curl --silent --remote-name --location \\ https://download.ceph.com/rpm-18.2.1/el9/noarch/cephadm \\ -o cephadm chmod +x cephadm # Bootstrap cluster ./cephadm bootstrap --mon-ip <mon-ip> # Add storage ceph orch apply osd --all-available-devices # Verify ceph -s ceph status","title":"1. CEPH HCI Setup"},{"location":"storage/#2-iscsi-gateway-setup","text":"# Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply iscsi gateway.yml # Create target gwcli.py target create <target-iqn> gwcli.py lun create <target-iqn> 0 --pool rbd --image disk1","title":"2. iSCSI Gateway Setup"},{"location":"storage/#3-nvme-of-gateway-setup","text":"# Create RBD image rbd create rbd/disk1 --size 100G # Configure gateway ceph orch apply nvmeof gateway.yml # Create subsystem gwcli.py subsystem create <subsystem-nqn> gwcli.py namespace create <subsystem-nqn> 1 --pool rbd --image disk1","title":"3. NVMe-oF Gateway Setup"},{"location":"storage/#4-gfs2-setup","text":"# Create filesystem mkfs.gfs2 -p lock_dlm -t mycluster -j 2 /dev/drbd/by-res/resource-data # Mount mount -t gfs2 -o noatime,nodiratime \\ /dev/drbd/by-res/resource-data /mnt/gfs2","title":"4. GFS2 Setup"},{"location":"storage/#security-considerations","text":"Enable CephX encryption Use CHAP authentication for iSCSI Use RDMA/RoCE for secure transport Implement proper network segmentation","title":"Security Considerations"},{"location":"storage/#source-code-references","text":"Technology Repository Documentation CEPH ceph/ceph docs.ceph.com iSCSI ceph/ceph linux-iscsi.org NVMe-oF ceph/ceph nvmexpress.org GFS2 torvalds/linux gfs2-utils","title":"Source Code References"},{"location":"storage/#troubleshooting","text":"For in-depth troubleshooting focused on code behavior and diagnostics, see Cluster Technologies section.","title":"Troubleshooting"},{"location":"storage/#common-issues","text":"Issue Solution OSD high latency Check ceph tell osd.* iostat Cannot discover iSCSI Verify iscsiadm -m discovery NVMe connection fails Check gateway connectivity GFS2 stale locks Run dlm_tool dump to clear","title":"Common Issues"},{"location":"storage/ceph/","text":"Distributed storage system providing object, block, and file storage in unified platform with self-healing and self-managing capabilities. Architecture graph TB subgraph \"Clients\" A[LIBRBD] B[LIBRADOS] C[CEPHFS Client] D[RGW/S3/Swift] end subgraph \"Monitor Layer\" E[MON 1] F[MON 2] G[MON 3] end subgraph \"Manager Layer\" H[MGR 1] I[MGR 2] end subgraph \"OSD Layer\" J[OSD 1] K[OSD 2] L[OSD 3] M[OSD N] N[BlueStore] O[FileStore] P[RocksDB] Q[WAL] end subgraph \"Storage\" R[HDD/SSD] S[NVMe] T[RAM] end subgraph \"Network\" U[Public Network] V[Cluster Network] end A --> B C --> B D --> B B --> E B --> F B --> G B --> J B --> K B --> L B --> M H --> J I --> K J --> N K --> O L --> P M --> Q N --> R O --> R P --> S Q --> S H --> T J --> U K --> U J --> V K --> V style E fill:#c8e6c9 style H fill:#ffecb3 style J fill:#e1f5ff style N fill:#fff3e0 Core Components MON (Monitor) Cluster management daemon maintaining cluster state and map information. MON Responsibilities : - Maintain cluster map (OSD map, MON map, MDS map, MGR map) - Monitor OSD health - Coordinate OSD failover/recovery - Maintain CRUSH map - Auth and authentication - Quorum management MON Map Structure : ceph mon dump # Output: # epoch 1 # fsid 12345678-1234-1234-1234-123456789abc # last_changed 2026-01-26 00:00:00.000000 # created 2026-01-26 00:00:00.000000 # min_mon_release 17 (quincy) # 0: [v2:192.168.1.10:3300/0,v1:192.168.1.10:6789/0] mon.node1 # 1: [v2:192.168.1.11:3300/0,v1:192.168.1.11:6789/0] mon.node2 # 2: [v2:192.168.1.12:3300/0,v1:192.168.1.12:6789/0] mon.node3 MON Quorum : - Odd number of MONs recommended - Minimum 3 MONs for production - Majority required for quorum - Split-brain prevention MGR (Manager) Daemon for monitoring and management, extending Ceph with plugins. MGR Plugins : | Plugin | Description | |--------|-------------| | dashboard | Web UI for monitoring | | prometheus | Prometheus metrics exporter | | zabbix | Zabbix integration | | influx | InfluxDB metrics | | rook | Kubernetes integration | | orchestrator | Cluster orchestration | | pg_autoscaler | PG auto-scaling | | balancer | Data balancer | MGR Status : ceph mgr stat # Output: # epoch 5 # active_since 2026-01-26 00:00:00.000000 # active_name mgr.node1 # available: node1, node2, node3 OSD (Object Storage Daemon) Core storage daemon storing data objects on local storage. OSD Responsibilities : - Store object data - Replicate data - Recover from failures - Rebalance data - Perform scrubbing - Report to MON OSD Storage Engines : BlueStore (Default) graph TB A[Object Write] --> B[RocksDB DB] A --> C[WAL] A --> D[BlueStore] B --> E[Metadata] C --> F[Journal] D --> G[Block Device] E --> H[Fast Storage] F --> H G --> I[Main Storage] style B fill:#c8e6c9 style C fill:#ffecb3 style D fill:#e1f5ff BlueStore Components : - Block Device : Main data storage - DB Device : RocksDB metadata (SSD/NVMe) - WAL Device : Write-ahead log (NVMe) BlueStore Benefits : - Better performance than FileStore - Direct object storage - Efficient compression - Better recovery FileStore (Legacy) FileStore Components : - Filesystem : XFS, Btrfs, ext4 - Journal : Write-ahead log - Data Directory : Object data FileStore Configuration : # Use XFS filesystem mkfs.xfs -f /dev/sdb mount /dev/sdb /var/lib/ceph/osd/ceph-0 # Use Btrfs with compression mkfs.btrfs /dev/sdc mount -o compress=lzo /dev/sdc /var/lib/ceph/osd/ceph-1 CRUSH (Controlled Replication Under Scalable Hashing) Algorithm for data distribution and replication. CRUSH Map Structure : crushtool -c /etc/ceph/crushmap -o crushmap.txt cat crushmap.txt CRUSH Hierarchy : root default host node1 osd.0 osd.1 host node2 osd.2 osd.3 host node3 osd.4 osd.5 CRUSH Rules : | Rule | Description | |------|-------------| | replicated_rule | Standard replication | | erasure-code | Erasure coding | | default_rule | Default replication | PG (Placement Group) Logical grouping of objects for data management. PG Calculation : PG = (OSDs * 100) / (Pool size * Replica count) Example: 10 OSDs, 3 replicas PG = (10 * 100) / (3) = 333 PG States : | State | Description | |-------|-------------| | creating | PG being created | | active | PG ready for I/O | | clean | PG fully replicated | | degraded | Missing replicas | | peering | Synchronizing replicas | | recovering | Recovering objects | | incomplete | Missing required replicas | Key Features Object Storage (RGW) RADOS Gateway provides S3 and Swift compatible object storage. RGW Setup : # Install RGW apt-get install radosgw # Create RGW user radosgw-admin user create \\ --uid=testuser \\ --display-name=\"Test User\" # Access keys radosgw-admin user info --uid=testuser S3 Access : # AWS CLI aws --endpoint-url http://radosgw.example.com s3 ls # Upload object aws --endpoint-url http://radosgw.example.com \\ s3 cp file.txt s3://bucket/file.txt # Download object aws --endpoint-url http://radosgw.example.com \\ s3 cp s3://bucket/file.txt file.txt Block Storage (RBD) RBD provides block devices via kernel module or QEMU. RBD Setup : # Create pool ceph osd pool create rbd 64 64 # Create RBD image rbd create rbd/image1 --size 100G --image-format 2 # Map RBD image rbd map rbd/image1 # Format filesystem mkfs.xfs /dev/rbd0 # Mount filesystem mount /dev/rbd0 /mnt/rbd RBD Features : | Feature | Description | |---------|-------------| | layering | Image snapshots | | striping | Object striping | | exclusive-lock | Exclusive lock | | object-map | Object map | | deep-flatten | Flatten snapshots | | fast-diff | Fast diff calculation | RBD with QEMU : # Create RBD image for VM rbd create vm1-disk --size 50G --image-format 2 # QEMU command qemu-system-x86_64 -drive \\ file=rbd:rbd/vm1-disk:id=admin,keyring=/etc/ceph/ceph.client.admin.keyring File Storage (CephFS) CephFS provides POSIX-compliant file system. CephFS Setup : # Create metadata pool ceph osd pool create cephfs_metadata 64 64 # Create data pool ceph osd pool create cephfs_data 128 128 # Create filesystem ceph fs new myfs cephfs_metadata cephfs_data # Mount CephFS mkdir /mnt/cephfs mount -t ceph 192.168.1.10:6789:/ /mnt/cephfs CephFS Features : - POSIX semantics - Multiple metadata servers (MDS) - Snapshots - Quotas - ACL support Quick Commands Cluster Status # Overall status ceph -s # Health detail ceph health detail # OSD status ceph osd status # OSD tree ceph osd tree # PG status ceph pg stat Pool Management # List pools ceph osd lspools # Create pool ceph osd pool create mypool 64 64 # Delete pool ceph osd pool delete mypool mypool --yes-i-really-really-mean-it # Pool stats ceph osd pool stats RBD Management # List images rbd ls rbd # Create image rbd create rbd/image1 --size 100G # Map image rbd map rbd/image1 # Unmap image rbd unmap /dev/rbd0 # Image info rbd info rbd/image1 Nifty Behaviors BlueStore Configuration # Enable BlueStore ceph config set osd osd_objectstore bluestore # Tune BlueStore ceph config set osd bluestore_max_blob_size 1M # Set DB device ceph config set osd bluestore_block_db_path /dev/nvme0n1p1 # Set WAL device ceph config set osd bluestore_block_wal_path /dev/nvme0n1p2 Nifty : Optimized storage engine for better performance Cache Tiering # Create cache pool ceph osd pool create cache-pool 32 32 ceph osd pool application enable cache-pool rbd # Create base pool ceph osd pool create base-pool 128 128 ceph osd pool application enable base-pool rbd # Add cache tier ceph osd tier add base-pool cache-pool # Set cache mode ceph osd tier cache-mode cache-pool writeback # Set cache target ratio ceph osd pool set cache-pool cache_target_dirty_ratio 0.4 # Enable cache ceph osd tier set-overlay base-pool cache-pool Nifty : Improve read performance with caching, ideally 4gb per drive Erasure Coding # Create erasure coded pool ceph osd pool create ec-pool 128 128 \\ erasure \\ k=4 m=2 \\ plugin=jerasure \\ technique=reed_sol_van # Create erasure coded profile ceph osd erasure-code-profile set ec-profile \\ k=4 m=2 \\ plugin=jerasure \\ technique=reed_sol_van # Create pool with profile ceph osd pool create ec-pool 128 128 erasure \\ profile=ec-profile Nifty : Better storage efficiency than replication RBD Mirroring # Enable RBD mirroring rbd mirror pool enable rbd pool # Create bootstrap token rbd mirror pool bootstrap create rbd > token # Import bootstrap token rbd mirror pool bootstrap import rbd token # Enable image mirroring rbd mirror image enable rbd/image1 Nifty : Disaster recovery with remote replication Production Configuration CRUSH Tuning # View CRUSH map crushtool -d /etc/ceph/crushmap.txt # Edit CRUSH map vim /etc/ceph/crushmap.txt # Compile CRUSH map crushtool -c /etc/ceph/crushmap.txt -o /etc/ceph/crushmap # Set CRUSH map ceph osd setcrushmap -i /etc/ceph/crushmap Custom CRUSH Rules : # Create root for failure domain crushtool --add-root ssd --default # Add host to root crushtool --add-bucket node1 host # Add OSD to host crushtool --add-item osd.0 --weight 1.0 # Create rule crushtool --add-rule ssd_rule ssd \\ chooseleaf firstn 0 type host OSD Configuration # Set OSD max backfill ceph config set osd osd_max_backfills 1 # Set OSD recovery sleep ceph config set osd osd_recovery_sleep 0.1 # Set OSD max scrub ceph config set osd osd_max_scrubs 1 # Set OSD client target ceph config set osd osd_client_target_max_inflight_ops 128 # Set OSD heartbeat interval ceph config set osd osd_heartbeat_interval 5 Performance Tuning # Enable compression ceph config set osd osd_compression_algorithm zstd # Set compression level ceph config set osd osd_compression_level 3 # Enable RBD cache rbd config global set rbd_cache true # Set RBD cache size rbd config global set rbd_cache_size 335544320 # Enable RBD writeback rbd config global set rbd_cache_writethrough_until_flush false Monitoring Ceph Dashboard # Enable dashboard ceph mgr module enable dashboard # Create admin user ceph dashboard ac-user-create admin admin administrator # Set SSL ceph dashboard set-ssl-certificate -i dashboard.crt ceph dashboard set-ssl-private-key -i dashboard.key # Access dashboard # https://192.168.1.10:8443 Prometheus Integration # Enable Prometheus exporter ceph mgr module enable prometheus # Access metrics curl http://192.168.1.10:9283/metrics Alerts # Set alert rules ceph config set mgr mgr_alerts \\ \"osd_down:3,pg_degraded:10,osd_full:0.9\" # View alerts ceph health detail Troubleshooting OSD Down # Check OSD status ceph osd tree # Bring OSD up ceph osd in osd.0 ceph osd up osd.0 # Check OSD logs ceph -w # Test OSD ceph tell osd.0 bench 4096 PG Degraded # Check PG status ceph pg stat # Show degraded PGs ceph pg ls degraded # Force recovery ceph pg force_recovery pgid # Set recovery speed ceph osd set-backfillfull ratio 0.75 OSD Full # Check OSD usage ceph osd df # Set cluster full ratio ceph osd set-full-ratio 0.85 ceph osd set-backfillfull-ratio 0.8 ceph osd set-nearfull-ratio 0.75 # Mark OSD as out ceph osd out osd.0 Best Practices Use BlueStore for production Separate networks for public and cluster traffic Use SSD for OSD DB/WAL Monitor cluster health continuously Set appropriate PG count for pools Enable compression for cold data Use erasure coding for large datasets Test recovery scenarios regularly Document cluster configuration Monitor capacity and plan expansions Source Code Repository : https://github.com/ceph/ceph Documentation : https://docs.ceph.com/ Key Source Locations Component Location Description MON src/mon/ Monitor daemon MGR src/mgr/ Manager daemon OSD src/osd/ OSD daemon BlueStore src/osd/BlueStore* BlueStore engine CRUSH src/crush/ CRUSH algorithm RBD src/rbd/ RBD client CephFS src/mds/ Metadata server","title":"CEPH"},{"location":"storage/ceph/#architecture","text":"graph TB subgraph \"Clients\" A[LIBRBD] B[LIBRADOS] C[CEPHFS Client] D[RGW/S3/Swift] end subgraph \"Monitor Layer\" E[MON 1] F[MON 2] G[MON 3] end subgraph \"Manager Layer\" H[MGR 1] I[MGR 2] end subgraph \"OSD Layer\" J[OSD 1] K[OSD 2] L[OSD 3] M[OSD N] N[BlueStore] O[FileStore] P[RocksDB] Q[WAL] end subgraph \"Storage\" R[HDD/SSD] S[NVMe] T[RAM] end subgraph \"Network\" U[Public Network] V[Cluster Network] end A --> B C --> B D --> B B --> E B --> F B --> G B --> J B --> K B --> L B --> M H --> J I --> K J --> N K --> O L --> P M --> Q N --> R O --> R P --> S Q --> S H --> T J --> U K --> U J --> V K --> V style E fill:#c8e6c9 style H fill:#ffecb3 style J fill:#e1f5ff style N fill:#fff3e0","title":"Architecture"},{"location":"storage/ceph/#core-components","text":"","title":"Core Components"},{"location":"storage/ceph/#mon-monitor","text":"Cluster management daemon maintaining cluster state and map information. MON Responsibilities : - Maintain cluster map (OSD map, MON map, MDS map, MGR map) - Monitor OSD health - Coordinate OSD failover/recovery - Maintain CRUSH map - Auth and authentication - Quorum management MON Map Structure : ceph mon dump # Output: # epoch 1 # fsid 12345678-1234-1234-1234-123456789abc # last_changed 2026-01-26 00:00:00.000000 # created 2026-01-26 00:00:00.000000 # min_mon_release 17 (quincy) # 0: [v2:192.168.1.10:3300/0,v1:192.168.1.10:6789/0] mon.node1 # 1: [v2:192.168.1.11:3300/0,v1:192.168.1.11:6789/0] mon.node2 # 2: [v2:192.168.1.12:3300/0,v1:192.168.1.12:6789/0] mon.node3 MON Quorum : - Odd number of MONs recommended - Minimum 3 MONs for production - Majority required for quorum - Split-brain prevention","title":"MON (Monitor)"},{"location":"storage/ceph/#mgr-manager","text":"Daemon for monitoring and management, extending Ceph with plugins. MGR Plugins : | Plugin | Description | |--------|-------------| | dashboard | Web UI for monitoring | | prometheus | Prometheus metrics exporter | | zabbix | Zabbix integration | | influx | InfluxDB metrics | | rook | Kubernetes integration | | orchestrator | Cluster orchestration | | pg_autoscaler | PG auto-scaling | | balancer | Data balancer | MGR Status : ceph mgr stat # Output: # epoch 5 # active_since 2026-01-26 00:00:00.000000 # active_name mgr.node1 # available: node1, node2, node3","title":"MGR (Manager)"},{"location":"storage/ceph/#osd-object-storage-daemon","text":"Core storage daemon storing data objects on local storage. OSD Responsibilities : - Store object data - Replicate data - Recover from failures - Rebalance data - Perform scrubbing - Report to MON OSD Storage Engines :","title":"OSD (Object Storage Daemon)"},{"location":"storage/ceph/#bluestore-default","text":"graph TB A[Object Write] --> B[RocksDB DB] A --> C[WAL] A --> D[BlueStore] B --> E[Metadata] C --> F[Journal] D --> G[Block Device] E --> H[Fast Storage] F --> H G --> I[Main Storage] style B fill:#c8e6c9 style C fill:#ffecb3 style D fill:#e1f5ff BlueStore Components : - Block Device : Main data storage - DB Device : RocksDB metadata (SSD/NVMe) - WAL Device : Write-ahead log (NVMe) BlueStore Benefits : - Better performance than FileStore - Direct object storage - Efficient compression - Better recovery","title":"BlueStore (Default)"},{"location":"storage/ceph/#filestore-legacy","text":"FileStore Components : - Filesystem : XFS, Btrfs, ext4 - Journal : Write-ahead log - Data Directory : Object data FileStore Configuration : # Use XFS filesystem mkfs.xfs -f /dev/sdb mount /dev/sdb /var/lib/ceph/osd/ceph-0 # Use Btrfs with compression mkfs.btrfs /dev/sdc mount -o compress=lzo /dev/sdc /var/lib/ceph/osd/ceph-1","title":"FileStore (Legacy)"},{"location":"storage/ceph/#crush-controlled-replication-under-scalable-hashing","text":"Algorithm for data distribution and replication. CRUSH Map Structure : crushtool -c /etc/ceph/crushmap -o crushmap.txt cat crushmap.txt CRUSH Hierarchy : root default host node1 osd.0 osd.1 host node2 osd.2 osd.3 host node3 osd.4 osd.5 CRUSH Rules : | Rule | Description | |------|-------------| | replicated_rule | Standard replication | | erasure-code | Erasure coding | | default_rule | Default replication |","title":"CRUSH (Controlled Replication Under Scalable Hashing)"},{"location":"storage/ceph/#pg-placement-group","text":"Logical grouping of objects for data management. PG Calculation : PG = (OSDs * 100) / (Pool size * Replica count) Example: 10 OSDs, 3 replicas PG = (10 * 100) / (3) = 333 PG States : | State | Description | |-------|-------------| | creating | PG being created | | active | PG ready for I/O | | clean | PG fully replicated | | degraded | Missing replicas | | peering | Synchronizing replicas | | recovering | Recovering objects | | incomplete | Missing required replicas |","title":"PG (Placement Group)"},{"location":"storage/ceph/#key-features","text":"","title":"Key Features"},{"location":"storage/ceph/#object-storage-rgw","text":"RADOS Gateway provides S3 and Swift compatible object storage. RGW Setup : # Install RGW apt-get install radosgw # Create RGW user radosgw-admin user create \\ --uid=testuser \\ --display-name=\"Test User\" # Access keys radosgw-admin user info --uid=testuser S3 Access : # AWS CLI aws --endpoint-url http://radosgw.example.com s3 ls # Upload object aws --endpoint-url http://radosgw.example.com \\ s3 cp file.txt s3://bucket/file.txt # Download object aws --endpoint-url http://radosgw.example.com \\ s3 cp s3://bucket/file.txt file.txt","title":"Object Storage (RGW)"},{"location":"storage/ceph/#block-storage-rbd","text":"RBD provides block devices via kernel module or QEMU. RBD Setup : # Create pool ceph osd pool create rbd 64 64 # Create RBD image rbd create rbd/image1 --size 100G --image-format 2 # Map RBD image rbd map rbd/image1 # Format filesystem mkfs.xfs /dev/rbd0 # Mount filesystem mount /dev/rbd0 /mnt/rbd RBD Features : | Feature | Description | |---------|-------------| | layering | Image snapshots | | striping | Object striping | | exclusive-lock | Exclusive lock | | object-map | Object map | | deep-flatten | Flatten snapshots | | fast-diff | Fast diff calculation | RBD with QEMU : # Create RBD image for VM rbd create vm1-disk --size 50G --image-format 2 # QEMU command qemu-system-x86_64 -drive \\ file=rbd:rbd/vm1-disk:id=admin,keyring=/etc/ceph/ceph.client.admin.keyring","title":"Block Storage (RBD)"},{"location":"storage/ceph/#file-storage-cephfs","text":"CephFS provides POSIX-compliant file system. CephFS Setup : # Create metadata pool ceph osd pool create cephfs_metadata 64 64 # Create data pool ceph osd pool create cephfs_data 128 128 # Create filesystem ceph fs new myfs cephfs_metadata cephfs_data # Mount CephFS mkdir /mnt/cephfs mount -t ceph 192.168.1.10:6789:/ /mnt/cephfs CephFS Features : - POSIX semantics - Multiple metadata servers (MDS) - Snapshots - Quotas - ACL support","title":"File Storage (CephFS)"},{"location":"storage/ceph/#quick-commands","text":"","title":"Quick Commands"},{"location":"storage/ceph/#cluster-status","text":"# Overall status ceph -s # Health detail ceph health detail # OSD status ceph osd status # OSD tree ceph osd tree # PG status ceph pg stat","title":"Cluster Status"},{"location":"storage/ceph/#pool-management","text":"# List pools ceph osd lspools # Create pool ceph osd pool create mypool 64 64 # Delete pool ceph osd pool delete mypool mypool --yes-i-really-really-mean-it # Pool stats ceph osd pool stats","title":"Pool Management"},{"location":"storage/ceph/#rbd-management","text":"# List images rbd ls rbd # Create image rbd create rbd/image1 --size 100G # Map image rbd map rbd/image1 # Unmap image rbd unmap /dev/rbd0 # Image info rbd info rbd/image1","title":"RBD Management"},{"location":"storage/ceph/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"storage/ceph/#bluestore-configuration","text":"# Enable BlueStore ceph config set osd osd_objectstore bluestore # Tune BlueStore ceph config set osd bluestore_max_blob_size 1M # Set DB device ceph config set osd bluestore_block_db_path /dev/nvme0n1p1 # Set WAL device ceph config set osd bluestore_block_wal_path /dev/nvme0n1p2 Nifty : Optimized storage engine for better performance","title":"BlueStore Configuration"},{"location":"storage/ceph/#cache-tiering","text":"# Create cache pool ceph osd pool create cache-pool 32 32 ceph osd pool application enable cache-pool rbd # Create base pool ceph osd pool create base-pool 128 128 ceph osd pool application enable base-pool rbd # Add cache tier ceph osd tier add base-pool cache-pool # Set cache mode ceph osd tier cache-mode cache-pool writeback # Set cache target ratio ceph osd pool set cache-pool cache_target_dirty_ratio 0.4 # Enable cache ceph osd tier set-overlay base-pool cache-pool Nifty : Improve read performance with caching, ideally 4gb per drive","title":"Cache Tiering"},{"location":"storage/ceph/#erasure-coding","text":"# Create erasure coded pool ceph osd pool create ec-pool 128 128 \\ erasure \\ k=4 m=2 \\ plugin=jerasure \\ technique=reed_sol_van # Create erasure coded profile ceph osd erasure-code-profile set ec-profile \\ k=4 m=2 \\ plugin=jerasure \\ technique=reed_sol_van # Create pool with profile ceph osd pool create ec-pool 128 128 erasure \\ profile=ec-profile Nifty : Better storage efficiency than replication","title":"Erasure Coding"},{"location":"storage/ceph/#rbd-mirroring","text":"# Enable RBD mirroring rbd mirror pool enable rbd pool # Create bootstrap token rbd mirror pool bootstrap create rbd > token # Import bootstrap token rbd mirror pool bootstrap import rbd token # Enable image mirroring rbd mirror image enable rbd/image1 Nifty : Disaster recovery with remote replication","title":"RBD Mirroring"},{"location":"storage/ceph/#production-configuration","text":"","title":"Production Configuration"},{"location":"storage/ceph/#crush-tuning","text":"# View CRUSH map crushtool -d /etc/ceph/crushmap.txt # Edit CRUSH map vim /etc/ceph/crushmap.txt # Compile CRUSH map crushtool -c /etc/ceph/crushmap.txt -o /etc/ceph/crushmap # Set CRUSH map ceph osd setcrushmap -i /etc/ceph/crushmap Custom CRUSH Rules : # Create root for failure domain crushtool --add-root ssd --default # Add host to root crushtool --add-bucket node1 host # Add OSD to host crushtool --add-item osd.0 --weight 1.0 # Create rule crushtool --add-rule ssd_rule ssd \\ chooseleaf firstn 0 type host","title":"CRUSH Tuning"},{"location":"storage/ceph/#osd-configuration","text":"# Set OSD max backfill ceph config set osd osd_max_backfills 1 # Set OSD recovery sleep ceph config set osd osd_recovery_sleep 0.1 # Set OSD max scrub ceph config set osd osd_max_scrubs 1 # Set OSD client target ceph config set osd osd_client_target_max_inflight_ops 128 # Set OSD heartbeat interval ceph config set osd osd_heartbeat_interval 5","title":"OSD Configuration"},{"location":"storage/ceph/#performance-tuning","text":"# Enable compression ceph config set osd osd_compression_algorithm zstd # Set compression level ceph config set osd osd_compression_level 3 # Enable RBD cache rbd config global set rbd_cache true # Set RBD cache size rbd config global set rbd_cache_size 335544320 # Enable RBD writeback rbd config global set rbd_cache_writethrough_until_flush false","title":"Performance Tuning"},{"location":"storage/ceph/#monitoring","text":"","title":"Monitoring"},{"location":"storage/ceph/#ceph-dashboard","text":"# Enable dashboard ceph mgr module enable dashboard # Create admin user ceph dashboard ac-user-create admin admin administrator # Set SSL ceph dashboard set-ssl-certificate -i dashboard.crt ceph dashboard set-ssl-private-key -i dashboard.key # Access dashboard # https://192.168.1.10:8443","title":"Ceph Dashboard"},{"location":"storage/ceph/#prometheus-integration","text":"# Enable Prometheus exporter ceph mgr module enable prometheus # Access metrics curl http://192.168.1.10:9283/metrics","title":"Prometheus Integration"},{"location":"storage/ceph/#alerts","text":"# Set alert rules ceph config set mgr mgr_alerts \\ \"osd_down:3,pg_degraded:10,osd_full:0.9\" # View alerts ceph health detail","title":"Alerts"},{"location":"storage/ceph/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"storage/ceph/#osd-down","text":"# Check OSD status ceph osd tree # Bring OSD up ceph osd in osd.0 ceph osd up osd.0 # Check OSD logs ceph -w # Test OSD ceph tell osd.0 bench 4096","title":"OSD Down"},{"location":"storage/ceph/#pg-degraded","text":"# Check PG status ceph pg stat # Show degraded PGs ceph pg ls degraded # Force recovery ceph pg force_recovery pgid # Set recovery speed ceph osd set-backfillfull ratio 0.75","title":"PG Degraded"},{"location":"storage/ceph/#osd-full","text":"# Check OSD usage ceph osd df # Set cluster full ratio ceph osd set-full-ratio 0.85 ceph osd set-backfillfull-ratio 0.8 ceph osd set-nearfull-ratio 0.75 # Mark OSD as out ceph osd out osd.0","title":"OSD Full"},{"location":"storage/ceph/#best-practices","text":"Use BlueStore for production Separate networks for public and cluster traffic Use SSD for OSD DB/WAL Monitor cluster health continuously Set appropriate PG count for pools Enable compression for cold data Use erasure coding for large datasets Test recovery scenarios regularly Document cluster configuration Monitor capacity and plan expansions","title":"Best Practices"},{"location":"storage/ceph/#source-code","text":"Repository : https://github.com/ceph/ceph Documentation : https://docs.ceph.com/","title":"Source Code"},{"location":"storage/ceph/#key-source-locations","text":"Component Location Description MON src/mon/ Monitor daemon MGR src/mgr/ Manager daemon OSD src/osd/ OSD daemon BlueStore src/osd/BlueStore* BlueStore engine CRUSH src/crush/ CRUSH algorithm RBD src/rbd/ RBD client CephFS src/mds/ Metadata server","title":"Key Source Locations"},{"location":"storage/gfs2/","text":"Global File System 2 for shared-disk file system in Linux clusters with distributed lock management and POSIX compliance. Architecture graph TB subgraph \"Cluster Nodes\" A[GFS2 Node 1] B[GFS2 Node 2] C[GFS2 Node 3] end subgraph \"Node 1 Components\" A1[VFS Layer] A2[GFS2 Module] A3[DLM Lock Manager] A4[Glock Lock Cache] end subgraph \"Node 2 Components\" B1[VFS Layer] B2[GFS2 Module] B3[DLM Lock Manager] B4[Glock Lock Cache] end subgraph \"Node 3 Components\" C1[VFS Layer] C2[GFS2 Module] C3[DLM Lock Manager] C4[Glock Lock Cache] end subgraph \"Shared Storage\" D[Block Device] E[DRBD/Storage] end subgraph \"Lock Synchronization\" F[DLM Network] G[Lock Exchange] end subgraph \"CLVM\" H[Volume Group] I[Logical Volumes] end A --> A1 A1 --> A2 A2 --> A3 A2 --> A4 B --> B1 B1 --> B2 B2 --> B3 B2 --> B4 C --> C1 C1 --> C2 C2 --> C3 C2 --> C4 A2 --> D B2 --> D C2 --> D D --> E A3 --> F B3 --> F C3 --> F F --> G D --> H H --> I style A2 fill:#c8e6c9 style A3 fill:#ffecb3 style A4 fill:#e1f5ff style D fill:#fff3e0 Core Components GFS2 Module Kernel module implementing shared filesystem. GFS2 Responsibilities : - File system operations (create, read, write, delete) - Metadata management (inodes, directories) - Journaling - Block allocation - Lock coordination with DLM GFS2 Structures : - Superblock : File system metadata - Resource Groups (RGs) : Block allocation units - Inodes : File metadata - Directories : File/directory entries - Journals : Write-ahead logs DLM (Distributed Lock Manager) Cluster-wide lock management service. DLM Responsibilities : - Coordinate file locks across nodes - Manage lock state - Handle lock conflicts - Support lock modes - Lock recovery Lock Modes : | Mode | Name | Description | |------|------|-------------| | NL | Null Lock | No lock | | CR | Concurrent Read | Shared read, no intent | | PR | Protected Read | Shared read with write intent | | PW | Protected Write | Exclusive read/write intent | | EX | Exclusive | Exclusive access | Lock Conversion : CR -> PR (upgrade) PR -> PW (upgrade) PW -> EX (upgrade) EX -> PW -> PR -> CR (downgrade) DLM Operation Flow : sequenceDiagram participant N1 as Node 1 participant DLM as DLM Service participant N2 as Node 2 N1->>DLM: Lock Request (EX) DLM->>DLM: Check Lock State DLM->>N2: Lock Grant Request N2->>DLM: Lock Release DLM->>N1: Lock Granted N1->>N1: Access Resource N1->>DLM: Lock Release Glock (GFS2 Lock) GFS2-specific lock structure for file system operations. Glock Types : | Type | Name | Description | |------|------|-------------| | 1 | Inode | Inode lock | | 2 | Rgrp | Resource group lock | | 3 | Meta | Metadata lock | | 4 | iopen | Inode open lock | | 5 | flock | File lock | | 6 | plock | Posix lock | | 8 | Quota | Quota lock | | 9 | Journal | Journal lock | Glock State Machine : stateDiagram-v2 [*] --> Unlocked Unlocked --> Shared: Shared Lock Request Unlocked --> Deferred: Exclusive Lock Request (blocked) Shared --> Unlocked: Release Shared --> Deferred: Upgrade Request (blocked) Deferred --> Shared: Grant Shared Deferred --> Exclusive: Grant Exclusive Exclusive --> Shared: Downgrade Exclusive --> Unlocked: Release Glock Cache : - Per-node lock cache - Improves performance - Reduces network traffic - Automatic cache invalidation Journaling GFS2 uses per-node journals for crash recovery. Journal Operation : 1. Transaction start 2. Write to journal 3. Commit to filesystem 4. Clean journal Journal Size Calculation : Journal size = (Max file size / Block size) * Journal entries Example: 1TB filesystem, 4KB blocks, 1000 entries Journal size = (1TB / 4KB) * 1000 = 256GB Journal Configuration : # Create journal during mkfs mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ -J size=1024 /dev/drbd/by-res/data # Add journal to existing filesystem gfs2_jadd -j 1 /mnt/gfs2 Key Features POSIX Compliance GFS2 provides full POSIX file system semantics. Supported Operations : - Standard file I/O (open, read, write, close) - Directory operations (mkdir, rmdir, readdir) - File permissions and ownership - Symbolic and hard links - File locking (fcntl, flock) - Extended attributes - ACLs Distributed Lock Management Cluster-wide coordination for concurrent access. Lock Granularity : - Glocks : High-level locks for resources - DLM locks : Low-level network locks - POSIX locks : Application-level file locks Lock Acquisition : # File lock with flock flock file.txt exclusive # POSIX lock with fcntl fcntl(fd, F_SETLK, &lock) Cluster-Wide Volume Management (CLVM) LVM extensions for cluster operations. CLVM Features : - Distributed volume management - Shared volume groups - Cluster-wide snapshots - Online resizing CLVM Components : - clvmd : Cluster LVM daemon - lvmlockd : LVM lock daemon - libvlock : Lock management library Quick Commands File System Creation # Create GFS2 filesystem mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ /dev/drbd/by-res/data # Create with specific journal size mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ -J size=1024 /dev/drbd/by-res/data # Create with block size mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ -b 4096 /dev/drbd/by-res/data Mounting # Mount GFS2 mount -t gfs2 /dev/drbd/by-res/data /mnt/gfs2 # Mount with options mount -t gfs2 -o noatime,nodiratime,data=ordered \\ /dev/drbd/by-res/data /mnt/gfs2 # Add to /etc/fstab /dev/drbd/by-res/data /mnt/gfs2 gfs2 \\ noatime,nodiratime 0 0 DLM Management # List locks dlm_tool ls # Dump locks dlm_tool dump # View lock statistics dlm_tool ls -v # Clear stale locks dlm_tool dump | grep -i stale File System Management # Show file system info gfs2_tool sb /dev/drbd/by-res/data all # Check file system gfs2_fsck -y /dev/drbd/by-res/data # View quota gfs2_quota list /mnt/gfs2 # Enable quotas gfs2_quota enable /mnt/gfs2 Nifty Behaviors Quota Management # Enable quotas gfs2_quota enable /mnt/gfs2 # Set user quota gfs2_quota limit -u 1000 100G /mnt/gfs2 # Set group quota gfs2_quota limit -g 1000 50G /mnt/gfs2 # Check quota gfs2_quota list /mnt/gfs2 gfs2_quota get -u 1000 /mnt/gfs2 Nifty : Enforce user quotas on shared filesystem Snapshots with CLVM # Create logical volume in cluster lvcreate -L 100G -n my_lv mycluster_vg # Create snapshot lvcreate -L 10G -s -n my_lv_snap mycluster_vg/my_lv # Display volumes lvdisplay mycluster_vg # Remove snapshot lvremove mycluster_vg/my_lv_snap Nifty : Point-in-time snapshots with LVM Journal Tuning # Check journal info gfs2_tool jindex /mnt/gfs2 # Add journal for new node gfs2_jadd -j 1 /mnt/gfs2 # Tune commit interval mount -t gfs2 -o commit=30 /dev/drbd/by-res/data /mnt/gfs2 Nifty : Dynamic journal expansion for node scaling Lock Contention Monitoring # Monitor glock stats cat /proc/fs/gfs2/cluster_name/glocks # Monitor lock dumps gfs2_tool lockdump /mnt/gfs2 # Monitor DLM activity dwatch -v glock /proc/fs/gfs2 Nifty : Real-time lock performance analysis CLVM Integration CLVM Setup # Install CLVM apt-get install lvm2-cluster # Configure cluster pvcreate /dev/sdb vgcreate -Ay -cy mycluster_vg /dev/sdb lvcreate -L 100G -n data_lv mycluster_vg # Start clvmd systemctl enable clvmd systemctl start clvmd # Create GFS2 on CLVM volume mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ /dev/mycluster_vg/data_lv CLVM Operations # Create volume lvcreate -L 50G -n my_lv mycluster_vg # Extend volume lvextend -L +20G mycluster_vg/my_lv resize_gfs2 /dev/mycluster_vg/my_lv # Reduce volume lvreduce -L -10G mycluster_vg/my_lv resize_gfs2 /dev/mycluster_vg/my_lv # Create snapshot lvcreate -L 5G -s -n snap_lv mycluster_vg/my_lv # Merge snapshot lvconvert --merge mycluster_vg/snap_lv Performance Tuning Mount Options # Optimized for performance mount -t gfs2 -o noatime,nodiratime,data=writeback \\ /dev/drbd/by-res/data /mnt/gfs2 # Optimized for safety mount -t gfs2 -o data=ordered,barrier=1 \\ /dev/drbd/by-res/data /mnt/gfs2 # Reduce lock contention mount -t gfs2 -o localflocks \\ /dev/drbd/by-res/data /mnt/gfs2 Option Description noatime Don't update access time nodiratime Don't update directory access time data=ordered Metadata before data (safe) data=writeback Data before metadata (fast) barrier Enable write barriers (safe) localflocks Use local lockd instead of DLM quota Enable quotas System Tuning # Increase journal commit interval sysctl -w fs.gfs2.logd_secs=30 # Increase lock cache size sysctl -w fs.gfs2.quota_scale=100 # Tune DLM sysctl -w fs.dlm.tcp_port_size=4096 File System Tuning # Tune GFS2 parameters gfs2_tool settune /mnt/gfs2 logd_secs 30 gfs2_tool settune /mnt/gfs2 quota_scale 100 gfs2_tool settune /mnt/gfs2 new_files_directio 1 # Tune inode cache gfs2_tool settune /mnt/gfs2 greedy_max 2000 Troubleshooting Stale Locks # Check for stale locks dlm_tool dump | grep -i stale # Clear stale locks dlm_tool unlock <resource> # Restart DLM if needed systemctl restart dlm systemctl restart clvmd Mount Issues # Check DLM status systemctl status dlm # Check cluster quorum corosync-quorumtool -s # Check fence status pcs status stonith # Remount filesystem umount /mnt/gfs2 mount -t gfs2 /dev/drbd/by-res/data /mnt/gfs2 Performance Issues # Monitor lock contention cat /proc/fs/gfs2/cluster_name/glocks # Monitor DLM stats dwatch -v dlm /proc/fs/dlm # Check journal size gfs2_tool jindex /mnt/gfs2 # Tune mount options mount -o remount,noatime,nodiratime /mnt/gfs2 File System Corruption # Unmount filesystem umount /mnt/gfs2 # Run fsck gfs2_fsck -y /dev/drbd/by-res/data # Check superblock gfs2_tool sb /dev/drbd/by-res/data all # Rebuild journals if needed gfs2_jadd -j 3 /dev/drbd/by-res/data Best Practices Use separate journals for each node Monitor lock contention regularly Use CLVM for volume management Set appropriate mount options for workload Enable quotas for user management Test failover scenarios Monitor DLM performance Use snapshots for backups Keep journal sizes adequate Document cluster configuration Use Cases Shared Web Content # Mount shared web root mount -t gfs2 /dev/drbd/by-res/webroot /var/www # Configure web servers on all nodes # All nodes can read/write same files # Automatic lock management Shared Application Data # Mount application data mount -t gfs2 /dev/drbd/by-res/appdata /opt/app/data # Application runs on multiple nodes # Concurrent access to shared data # POSIX semantics guaranteed Database Clustering # Mount database data mount -t gfs2 -o data=writeback /dev/drbd/by-res/dbdata /var/lib/mysql # MySQL cluster with shared storage # Note: Use with caution, consider Galera for MySQL Comparison with Other File Systems Feature GFS2 OCFS2 NFS CephFS POSIX Yes Yes Yes Yes Block Device Yes Yes No No Direct Access Yes Yes No No Cluster Size Medium Medium Large Large Scalability Medium Medium High High Complexity High Medium Low High Source Code Location in Linux kernel : fs/gfs2/ Repository : https://github.com/torvalds/linux/tree/master/fs/gfs2 Documentation : /usr/share/doc/gfs2-utils/ Key Source Locations Component Location Description Main module fs/gfs2/main.c GFS2 initialization Superblock fs/gfs2/super.c Superblock operations Inode fs/gfs2/inode.c Inode operations Dirthy fs/gfs2/dir.c Directory operations Glock fs/gfs2/glock.c Lock management Rgrp fs/gfs2/rgrp.c Resource group Log fs/gfs2/log.c Journaling","title":"GFS2"},{"location":"storage/gfs2/#architecture","text":"graph TB subgraph \"Cluster Nodes\" A[GFS2 Node 1] B[GFS2 Node 2] C[GFS2 Node 3] end subgraph \"Node 1 Components\" A1[VFS Layer] A2[GFS2 Module] A3[DLM Lock Manager] A4[Glock Lock Cache] end subgraph \"Node 2 Components\" B1[VFS Layer] B2[GFS2 Module] B3[DLM Lock Manager] B4[Glock Lock Cache] end subgraph \"Node 3 Components\" C1[VFS Layer] C2[GFS2 Module] C3[DLM Lock Manager] C4[Glock Lock Cache] end subgraph \"Shared Storage\" D[Block Device] E[DRBD/Storage] end subgraph \"Lock Synchronization\" F[DLM Network] G[Lock Exchange] end subgraph \"CLVM\" H[Volume Group] I[Logical Volumes] end A --> A1 A1 --> A2 A2 --> A3 A2 --> A4 B --> B1 B1 --> B2 B2 --> B3 B2 --> B4 C --> C1 C1 --> C2 C2 --> C3 C2 --> C4 A2 --> D B2 --> D C2 --> D D --> E A3 --> F B3 --> F C3 --> F F --> G D --> H H --> I style A2 fill:#c8e6c9 style A3 fill:#ffecb3 style A4 fill:#e1f5ff style D fill:#fff3e0","title":"Architecture"},{"location":"storage/gfs2/#core-components","text":"","title":"Core Components"},{"location":"storage/gfs2/#gfs2-module","text":"Kernel module implementing shared filesystem. GFS2 Responsibilities : - File system operations (create, read, write, delete) - Metadata management (inodes, directories) - Journaling - Block allocation - Lock coordination with DLM GFS2 Structures : - Superblock : File system metadata - Resource Groups (RGs) : Block allocation units - Inodes : File metadata - Directories : File/directory entries - Journals : Write-ahead logs","title":"GFS2 Module"},{"location":"storage/gfs2/#dlm-distributed-lock-manager","text":"Cluster-wide lock management service. DLM Responsibilities : - Coordinate file locks across nodes - Manage lock state - Handle lock conflicts - Support lock modes - Lock recovery Lock Modes : | Mode | Name | Description | |------|------|-------------| | NL | Null Lock | No lock | | CR | Concurrent Read | Shared read, no intent | | PR | Protected Read | Shared read with write intent | | PW | Protected Write | Exclusive read/write intent | | EX | Exclusive | Exclusive access | Lock Conversion : CR -> PR (upgrade) PR -> PW (upgrade) PW -> EX (upgrade) EX -> PW -> PR -> CR (downgrade) DLM Operation Flow : sequenceDiagram participant N1 as Node 1 participant DLM as DLM Service participant N2 as Node 2 N1->>DLM: Lock Request (EX) DLM->>DLM: Check Lock State DLM->>N2: Lock Grant Request N2->>DLM: Lock Release DLM->>N1: Lock Granted N1->>N1: Access Resource N1->>DLM: Lock Release","title":"DLM (Distributed Lock Manager)"},{"location":"storage/gfs2/#glock-gfs2-lock","text":"GFS2-specific lock structure for file system operations. Glock Types : | Type | Name | Description | |------|------|-------------| | 1 | Inode | Inode lock | | 2 | Rgrp | Resource group lock | | 3 | Meta | Metadata lock | | 4 | iopen | Inode open lock | | 5 | flock | File lock | | 6 | plock | Posix lock | | 8 | Quota | Quota lock | | 9 | Journal | Journal lock | Glock State Machine : stateDiagram-v2 [*] --> Unlocked Unlocked --> Shared: Shared Lock Request Unlocked --> Deferred: Exclusive Lock Request (blocked) Shared --> Unlocked: Release Shared --> Deferred: Upgrade Request (blocked) Deferred --> Shared: Grant Shared Deferred --> Exclusive: Grant Exclusive Exclusive --> Shared: Downgrade Exclusive --> Unlocked: Release Glock Cache : - Per-node lock cache - Improves performance - Reduces network traffic - Automatic cache invalidation","title":"Glock (GFS2 Lock)"},{"location":"storage/gfs2/#journaling","text":"GFS2 uses per-node journals for crash recovery. Journal Operation : 1. Transaction start 2. Write to journal 3. Commit to filesystem 4. Clean journal Journal Size Calculation : Journal size = (Max file size / Block size) * Journal entries Example: 1TB filesystem, 4KB blocks, 1000 entries Journal size = (1TB / 4KB) * 1000 = 256GB Journal Configuration : # Create journal during mkfs mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ -J size=1024 /dev/drbd/by-res/data # Add journal to existing filesystem gfs2_jadd -j 1 /mnt/gfs2","title":"Journaling"},{"location":"storage/gfs2/#key-features","text":"","title":"Key Features"},{"location":"storage/gfs2/#posix-compliance","text":"GFS2 provides full POSIX file system semantics. Supported Operations : - Standard file I/O (open, read, write, close) - Directory operations (mkdir, rmdir, readdir) - File permissions and ownership - Symbolic and hard links - File locking (fcntl, flock) - Extended attributes - ACLs","title":"POSIX Compliance"},{"location":"storage/gfs2/#distributed-lock-management","text":"Cluster-wide coordination for concurrent access. Lock Granularity : - Glocks : High-level locks for resources - DLM locks : Low-level network locks - POSIX locks : Application-level file locks Lock Acquisition : # File lock with flock flock file.txt exclusive # POSIX lock with fcntl fcntl(fd, F_SETLK, &lock)","title":"Distributed Lock Management"},{"location":"storage/gfs2/#cluster-wide-volume-management-clvm","text":"LVM extensions for cluster operations. CLVM Features : - Distributed volume management - Shared volume groups - Cluster-wide snapshots - Online resizing CLVM Components : - clvmd : Cluster LVM daemon - lvmlockd : LVM lock daemon - libvlock : Lock management library","title":"Cluster-Wide Volume Management (CLVM)"},{"location":"storage/gfs2/#quick-commands","text":"","title":"Quick Commands"},{"location":"storage/gfs2/#file-system-creation","text":"# Create GFS2 filesystem mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ /dev/drbd/by-res/data # Create with specific journal size mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ -J size=1024 /dev/drbd/by-res/data # Create with block size mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ -b 4096 /dev/drbd/by-res/data","title":"File System Creation"},{"location":"storage/gfs2/#mounting","text":"# Mount GFS2 mount -t gfs2 /dev/drbd/by-res/data /mnt/gfs2 # Mount with options mount -t gfs2 -o noatime,nodiratime,data=ordered \\ /dev/drbd/by-res/data /mnt/gfs2 # Add to /etc/fstab /dev/drbd/by-res/data /mnt/gfs2 gfs2 \\ noatime,nodiratime 0 0","title":"Mounting"},{"location":"storage/gfs2/#dlm-management","text":"# List locks dlm_tool ls # Dump locks dlm_tool dump # View lock statistics dlm_tool ls -v # Clear stale locks dlm_tool dump | grep -i stale","title":"DLM Management"},{"location":"storage/gfs2/#file-system-management","text":"# Show file system info gfs2_tool sb /dev/drbd/by-res/data all # Check file system gfs2_fsck -y /dev/drbd/by-res/data # View quota gfs2_quota list /mnt/gfs2 # Enable quotas gfs2_quota enable /mnt/gfs2","title":"File System Management"},{"location":"storage/gfs2/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"storage/gfs2/#quota-management","text":"# Enable quotas gfs2_quota enable /mnt/gfs2 # Set user quota gfs2_quota limit -u 1000 100G /mnt/gfs2 # Set group quota gfs2_quota limit -g 1000 50G /mnt/gfs2 # Check quota gfs2_quota list /mnt/gfs2 gfs2_quota get -u 1000 /mnt/gfs2 Nifty : Enforce user quotas on shared filesystem","title":"Quota Management"},{"location":"storage/gfs2/#snapshots-with-clvm","text":"# Create logical volume in cluster lvcreate -L 100G -n my_lv mycluster_vg # Create snapshot lvcreate -L 10G -s -n my_lv_snap mycluster_vg/my_lv # Display volumes lvdisplay mycluster_vg # Remove snapshot lvremove mycluster_vg/my_lv_snap Nifty : Point-in-time snapshots with LVM","title":"Snapshots with CLVM"},{"location":"storage/gfs2/#journal-tuning","text":"# Check journal info gfs2_tool jindex /mnt/gfs2 # Add journal for new node gfs2_jadd -j 1 /mnt/gfs2 # Tune commit interval mount -t gfs2 -o commit=30 /dev/drbd/by-res/data /mnt/gfs2 Nifty : Dynamic journal expansion for node scaling","title":"Journal Tuning"},{"location":"storage/gfs2/#lock-contention-monitoring","text":"# Monitor glock stats cat /proc/fs/gfs2/cluster_name/glocks # Monitor lock dumps gfs2_tool lockdump /mnt/gfs2 # Monitor DLM activity dwatch -v glock /proc/fs/gfs2 Nifty : Real-time lock performance analysis","title":"Lock Contention Monitoring"},{"location":"storage/gfs2/#clvm-integration","text":"","title":"CLVM Integration"},{"location":"storage/gfs2/#clvm-setup","text":"# Install CLVM apt-get install lvm2-cluster # Configure cluster pvcreate /dev/sdb vgcreate -Ay -cy mycluster_vg /dev/sdb lvcreate -L 100G -n data_lv mycluster_vg # Start clvmd systemctl enable clvmd systemctl start clvmd # Create GFS2 on CLVM volume mkfs.gfs2 -p lock_dlm -t mycluster:myfs -j 3 \\ /dev/mycluster_vg/data_lv","title":"CLVM Setup"},{"location":"storage/gfs2/#clvm-operations","text":"# Create volume lvcreate -L 50G -n my_lv mycluster_vg # Extend volume lvextend -L +20G mycluster_vg/my_lv resize_gfs2 /dev/mycluster_vg/my_lv # Reduce volume lvreduce -L -10G mycluster_vg/my_lv resize_gfs2 /dev/mycluster_vg/my_lv # Create snapshot lvcreate -L 5G -s -n snap_lv mycluster_vg/my_lv # Merge snapshot lvconvert --merge mycluster_vg/snap_lv","title":"CLVM Operations"},{"location":"storage/gfs2/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"storage/gfs2/#mount-options","text":"# Optimized for performance mount -t gfs2 -o noatime,nodiratime,data=writeback \\ /dev/drbd/by-res/data /mnt/gfs2 # Optimized for safety mount -t gfs2 -o data=ordered,barrier=1 \\ /dev/drbd/by-res/data /mnt/gfs2 # Reduce lock contention mount -t gfs2 -o localflocks \\ /dev/drbd/by-res/data /mnt/gfs2 Option Description noatime Don't update access time nodiratime Don't update directory access time data=ordered Metadata before data (safe) data=writeback Data before metadata (fast) barrier Enable write barriers (safe) localflocks Use local lockd instead of DLM quota Enable quotas","title":"Mount Options"},{"location":"storage/gfs2/#system-tuning","text":"# Increase journal commit interval sysctl -w fs.gfs2.logd_secs=30 # Increase lock cache size sysctl -w fs.gfs2.quota_scale=100 # Tune DLM sysctl -w fs.dlm.tcp_port_size=4096","title":"System Tuning"},{"location":"storage/gfs2/#file-system-tuning","text":"# Tune GFS2 parameters gfs2_tool settune /mnt/gfs2 logd_secs 30 gfs2_tool settune /mnt/gfs2 quota_scale 100 gfs2_tool settune /mnt/gfs2 new_files_directio 1 # Tune inode cache gfs2_tool settune /mnt/gfs2 greedy_max 2000","title":"File System Tuning"},{"location":"storage/gfs2/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"storage/gfs2/#stale-locks","text":"# Check for stale locks dlm_tool dump | grep -i stale # Clear stale locks dlm_tool unlock <resource> # Restart DLM if needed systemctl restart dlm systemctl restart clvmd","title":"Stale Locks"},{"location":"storage/gfs2/#mount-issues","text":"# Check DLM status systemctl status dlm # Check cluster quorum corosync-quorumtool -s # Check fence status pcs status stonith # Remount filesystem umount /mnt/gfs2 mount -t gfs2 /dev/drbd/by-res/data /mnt/gfs2","title":"Mount Issues"},{"location":"storage/gfs2/#performance-issues","text":"# Monitor lock contention cat /proc/fs/gfs2/cluster_name/glocks # Monitor DLM stats dwatch -v dlm /proc/fs/dlm # Check journal size gfs2_tool jindex /mnt/gfs2 # Tune mount options mount -o remount,noatime,nodiratime /mnt/gfs2","title":"Performance Issues"},{"location":"storage/gfs2/#file-system-corruption","text":"# Unmount filesystem umount /mnt/gfs2 # Run fsck gfs2_fsck -y /dev/drbd/by-res/data # Check superblock gfs2_tool sb /dev/drbd/by-res/data all # Rebuild journals if needed gfs2_jadd -j 3 /dev/drbd/by-res/data","title":"File System Corruption"},{"location":"storage/gfs2/#best-practices","text":"Use separate journals for each node Monitor lock contention regularly Use CLVM for volume management Set appropriate mount options for workload Enable quotas for user management Test failover scenarios Monitor DLM performance Use snapshots for backups Keep journal sizes adequate Document cluster configuration","title":"Best Practices"},{"location":"storage/gfs2/#use-cases","text":"","title":"Use Cases"},{"location":"storage/gfs2/#shared-web-content","text":"# Mount shared web root mount -t gfs2 /dev/drbd/by-res/webroot /var/www # Configure web servers on all nodes # All nodes can read/write same files # Automatic lock management","title":"Shared Web Content"},{"location":"storage/gfs2/#shared-application-data","text":"# Mount application data mount -t gfs2 /dev/drbd/by-res/appdata /opt/app/data # Application runs on multiple nodes # Concurrent access to shared data # POSIX semantics guaranteed","title":"Shared Application Data"},{"location":"storage/gfs2/#database-clustering","text":"# Mount database data mount -t gfs2 -o data=writeback /dev/drbd/by-res/dbdata /var/lib/mysql # MySQL cluster with shared storage # Note: Use with caution, consider Galera for MySQL","title":"Database Clustering"},{"location":"storage/gfs2/#comparison-with-other-file-systems","text":"Feature GFS2 OCFS2 NFS CephFS POSIX Yes Yes Yes Yes Block Device Yes Yes No No Direct Access Yes Yes No No Cluster Size Medium Medium Large Large Scalability Medium Medium High High Complexity High Medium Low High","title":"Comparison with Other File Systems"},{"location":"storage/gfs2/#source-code","text":"Location in Linux kernel : fs/gfs2/ Repository : https://github.com/torvalds/linux/tree/master/fs/gfs2 Documentation : /usr/share/doc/gfs2-utils/","title":"Source Code"},{"location":"storage/gfs2/#key-source-locations","text":"Component Location Description Main module fs/gfs2/main.c GFS2 initialization Superblock fs/gfs2/super.c Superblock operations Inode fs/gfs2/inode.c Inode operations Dirthy fs/gfs2/dir.c Directory operations Glock fs/gfs2/glock.c Lock management Rgrp fs/gfs2/rgrp.c Resource group Log fs/gfs2/log.c Journaling","title":"Key Source Locations"},{"location":"storage/iscsi/","text":"Internet Small Computer System Interface for block-level storage access over TCP/IP networks. Architecture graph TB subgraph \"Initiator Side\" A[SCSI Initiator] B[SCSI Layer] C[Block Layer] D[Filesystem] end subgraph \"Network\" E[TCP/IP] F[Network Interface] end subgraph \"Target Side\" G[SCSI Target] H[LIO Target Framework] I[TCMU Backend] J[Target Driver] end subgraph \"Storage Backend\" K[RBD] L[LVM] M[File] N[RAMdisk] end A --> B B --> C C --> D B --> E G --> E E --> F G --> H H --> I I --> J J --> K J --> L J --> M J --> N style G fill:#c8e6c9 style H fill:#ffecb3 style J fill:#e1f5ff style K fill:#fff3e0 Core Components LIO (Linux IO Target) SCSI target framework providing target functionality. LIO Components : | Component | Description | |-----------|-------------| | tcmu | Userspace target core | | tcm_loop | Loopback target | | tcm_qla2xxx | QLogic target | | tcm_iblock | Block device backend | | tcm_fileio | File backend | | tcm_pscsi | Passthrough SCSI | LIO Architecture : graph TB A[SCSI Initiator] --> B[TCP/IP] B --> C[tcmu-runner] C --> D[TCMU Handler] D --> E[Storage Backend] F[ConfigFS] --> D G[Target CLI] --> F style C fill:#c8e6c9 style D fill:#ffecb3 style E fill:#e1f5ff TCMU (Target Core Module Userspace) Userspace target core for flexible backends. TCMU Operation : sequenceDiagram participant I as Initiator participant T as TCMU participant R as tcmu-runner participant B as Backend I->>T: SCSI Command T->>R: Submit to userspace R->>B: Process command B->>R: Response R->>T: Complete command T->>I: SCSI Response style R fill:#c8e6c9 style B fill:#e1f5ff TCMU Benefits : - Userspace implementation - Flexible backends - No kernel modifications - Easier debugging Target Configuration LIO Target Setup # Install LIO target apt-get install targetcli-fb # Start target service systemctl enable target systemctl start target # Open firewall firewall-cmd --permanent --add-port=3260/tcp firewall-cmd --reload Creating Targets # Enter targetcli targetcli # Create backstore # Block device backend backstores/block create name=block_backend dev=/dev/vg0/vm1 # File backend backstores/fileio create name=file_backend \\ path=/var/lib/iscsi/disk.img size=10G # Create target iscsi/ create iqn.2026-01.com.example:storage.target1 # Create LUN iscsi/iqn.2026-01.com.example:storage.target1/tpg1/luns create \\ /backstores/block/block_backend # Create ACL iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls create \\ iqn.2026-01.com.example:initiator1 # Save configuration saveconfig exit Target Configuration Structure / \u251c\u2500\u2500 backstores/ \u2502 \u251c\u2500\u2500 block/ \u2502 \u251c\u2500\u2500 fileio/ \u2502 \u251c\u2500\u2500 pscsi/ \u2502 \u2514\u2500\u2500 ramdisk/ \u251c\u2500\u2500 iscsi/ \u2502 \u2514\u2500\u2500 iqn.2026-01.com.example:storage.target1/ \u2502 \u251c\u2500\u2500 tpg1/ \u2502 \u2502 \u251c\u2500\u2500 acls/ \u2502 \u2502 \u251c\u2500\u2500 luns/ \u2502 \u2502 \u251c\u2500\u2500 portals/ \u2502 \u2502 \u2514\u2500\u2500 params/ \u2502 \u2514\u2500\u2500 tpg2/ \u251c\u2500\u2500 loopback/ \u2514\u2500\u2500 core/ Initiator Configuration iSCSI Initiator Setup # Install iSCSI initiator apt-get install open-iscsi # Configure initiator name vim /etc/iscsi/initiatorname.iscsi # InitiatorName=iqn.2026-01.com.example:initiator1 # Start initiator service systemctl enable iscsid systemctl start iscsid Discovery # Discover targets iscsiadm -m discovery -t st -p 192.168.1.100 # Discover with CHAP iscsiadm -m discovery -t st -p 192.168.1.100 \\ -o new -u username -w password # Discover targets on subnet iscsiadm -m discovery -t st -p 192.168.1.0/24 Connection # Login to target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 -l # Login to all discovered targets iscsiadm -m node -l # Logout from target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 -u # Logout from all targets iscsiadm -m node -u Persistent Login # Automatic login iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 --op update -n node.startup -v automatic # Manual login iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 --op update -n node.startup -v manual CHAP Authentication One-Way CHAP Target Configuration : # Set CHAP username and password iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ userid=username iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ password=password123 # Enable authentication iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ authentication=1 Initiator Configuration : # Set CHAP credentials vim /etc/iscsi/nodes/iqn.2026-01.com.example:storage.target1/192.168.1.100,3260/default node.session.auth.authmethod = CHAP node.session.auth.username = username node.session.auth.password = password123 Mutual CHAP Target Configuration : # Set mutual CHAP iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ userid=username iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ password=password123 iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ mutual_userid=mutual_username iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ mutual_password=mutual_password123 Initiator Configuration : # Set mutual CHAP vim /etc/iscsi/nodes/iqn.2026-01.com.example:storage.target1/192.168.1.100,3260/default node.session.auth.authmethod = CHAP node.session.auth.username = username node.session.auth.password = password123 node.session.auth.username_in = mutual_username node.session.auth.password_in = mutual_password123 Multipath I/O Multipath Setup # Install multipath tools apt-get install multipath-tools # Configure multipath vim /etc/multipath.conf defaults { user_friendly_names yes find_multipaths yes } Multipath Configuration # /etc/multipath.conf defaults { user_friendly_names yes find_multipaths yes path_selector \"round-robin 0\" path_grouping_policy multibus failback immediate no_path_retry queue rr_min_io 1000 rr_min_io_rq 1 } devices { device { vendor \"LIO-ORG\" product \"TCMU device\" path_grouping_policy \"multibus\" getuid_callout \"/lib/udev/scsi_id --whitelisted --replace-whitespace --device=/dev/%n\" path_checker \"tur\" } } blacklist { devnode \"^sd[a-z]+\" } Multipath Management # Scan for multipath devices multipath -F multipath -v2 # List multipath devices multipath -ll # Show device maps multipath -ll # Show device paths multipath -l # Flush multipath devices multipath -F Performance Tuning Target Tuning # Set queue depth iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ max_cmd_sn=65536 # Set session parameters iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ nodeacl.auth.username=username # Set queue parameters iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ cmdsn_depth=128 Initiator Tuning # Set queue depth vim /etc/iscsi/iscsid.conf node.session.cmds_max=128 # Set initial login retries node.session.initial_login_retry_max=8 # Set timeout values node.session.timeo.replacement_timeout=120 node.conn[0].timeo.login_timeout=15 node.conn[0].timeo.logout_timeout=15 # Enable large receive offload node.conn[0].iscsi.HeaderDigest=None node.conn[0].iscsi.DataDigest=None node.conn[0].iscsi.IFMarker=No node.conn[0].iscsi.OFMarker=No High Availability HA Gateway Setup # Configure multiple portals iscsi/iqn.2026-01.com.example:storage.target1/tpg1/portals/ create \\ 192.168.1.100:3260 iscsi/iqn.2026-01.com.example:storage.target1/tpg1/portals/ create \\ 192.168.1.101:3260 # Create target with multiple portals iscsi/ create iqn.2026-01.com.example:storage.target2 iscsi/iqn.2026-01.com.example:storage.target2/tpg1/portals/ create \\ 192.168.1.102:3260 iscsi/iqn.2026-01.com.example:storage.target2/tpg1/portals/ create \\ 192.168.1.103:3260 Load Balancing # Round-robin path selection path_selector \"round-robin 0\" # Least queue depth path_selector \"queue-depth 0\" # Service time path_selector \"service-time 0\" Ceph iSCSI Gateway Ceph iSCSI Gateway Setup # Install Ceph iSCSI gateway apt-get install ceph-iscsi apt-get install tcmu-runner apt-get install targetcli-fb # Create RBD image rbd create rbd/disk1 --size 100G # Create iSCSI gateway configuration cat > gateway.yml <<EOF service_type: iscsi service_id: iscsi placement: hosts: - gw1 - gw2 spec: pools: - rbd api_user: admin api_secret: AQATjVdhSx3pGBAA2C7C1C9DjFwB2bY8x0l6g== trusted_ip_list: 192.168.1.100,192.168.1.101 EOF # Apply gateway configuration ceph orch apply -i gateway.yml Gateway CLI # Connect to gateway gwcli.py # Create target target create iqn.2026-01.com.example:ceph.target1 # Create LUN lun create iqn.2026-01.com.example:ceph.target1 0 \\ --pool rbd --image disk1 # Create client client create iqn.2026-01.com.example:initiator1 # Add client to target client add iqn.2026-01.com.example:initiator1 \\ --lun 0 # Set CHAP client auth iqn.2026-01.com.example:initiator1 \\ --user username --password password Troubleshooting Discovery Issues # Check target service systemctl status target # Check network ping 192.168.1.100 telnet 192.168.1.100 3260 # Check firewall iptables -L -n | grep 3260 # Check initiator service systemctl status iscsid # Discover targets iscsiadm -m discovery -t st -p 192.168.1.100 -d Connection Issues # Check initiator logs journalctl -u iscsid -f # Check target logs journalctl -u target -f # Check session status iscsiadm -m session # Check node status iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 # Test connection iscsiadm -m session -P 3 Performance Issues # Check multipath multipath -ll # Check I/O iostat -x 1 # Check network sar -n DEV 1 # Check initiator configuration cat /etc/iscsi/iscsid.conf # Check target configuration targetcli ls Best Practices Use CHAP authentication for security Enable multipath for redundancy Configure HA gateways for high availability Use dedicated network for iSCSI Monitor iSCSI connections regularly Test failover scenarios Tune queue depth for performance Use LUN masking for access control Document configuration thoroughly Regular backups of configuration Quick Commands Target Commands # Enter targetcli targetcli # Save configuration saveconfig # Restore configuration restoreconfig /etc/target/saveconfig.json # View configuration ls cd / ls Initiator Commands # Discover targets iscsiadm -m discovery -t st -p 192.168.1.100 # Login to target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 -p 192.168.1.100 -l # Logout from target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 -p 192.168.1.100 -u # List sessions iscsiadm -m session # Delete node iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 -p 192.168.1.100 --op delete Multipath Commands # Scan devices multipath -F multipath -v2 # List devices multipath -ll # Show device status multipath -ll mpatha # Resize multipath multipathd -k'resize map mpatha' Source Code Repository : https://github.com/linux-rdma/lio-core Ceph iSCSI : https://github.com/ceph/ceph/tree/master/ceph-iscsi Documentation : https://linux-iscsi.org/ LIO Documentation : https://github.com/open-iscsi/targetcli-fb Key Components Component Location Description LIO Kernel drivers/target/ Kernel target tcmu-runner tcmu-runner/ Userspace runner targetcli targetcli/ Management tool open-iscsi open-iscsi/ Initiator multipath multipath-tools/ Multipath IO","title":"iSCSI"},{"location":"storage/iscsi/#architecture","text":"graph TB subgraph \"Initiator Side\" A[SCSI Initiator] B[SCSI Layer] C[Block Layer] D[Filesystem] end subgraph \"Network\" E[TCP/IP] F[Network Interface] end subgraph \"Target Side\" G[SCSI Target] H[LIO Target Framework] I[TCMU Backend] J[Target Driver] end subgraph \"Storage Backend\" K[RBD] L[LVM] M[File] N[RAMdisk] end A --> B B --> C C --> D B --> E G --> E E --> F G --> H H --> I I --> J J --> K J --> L J --> M J --> N style G fill:#c8e6c9 style H fill:#ffecb3 style J fill:#e1f5ff style K fill:#fff3e0","title":"Architecture"},{"location":"storage/iscsi/#core-components","text":"","title":"Core Components"},{"location":"storage/iscsi/#lio-linux-io-target","text":"SCSI target framework providing target functionality. LIO Components : | Component | Description | |-----------|-------------| | tcmu | Userspace target core | | tcm_loop | Loopback target | | tcm_qla2xxx | QLogic target | | tcm_iblock | Block device backend | | tcm_fileio | File backend | | tcm_pscsi | Passthrough SCSI | LIO Architecture : graph TB A[SCSI Initiator] --> B[TCP/IP] B --> C[tcmu-runner] C --> D[TCMU Handler] D --> E[Storage Backend] F[ConfigFS] --> D G[Target CLI] --> F style C fill:#c8e6c9 style D fill:#ffecb3 style E fill:#e1f5ff","title":"LIO (Linux IO Target)"},{"location":"storage/iscsi/#tcmu-target-core-module-userspace","text":"Userspace target core for flexible backends. TCMU Operation : sequenceDiagram participant I as Initiator participant T as TCMU participant R as tcmu-runner participant B as Backend I->>T: SCSI Command T->>R: Submit to userspace R->>B: Process command B->>R: Response R->>T: Complete command T->>I: SCSI Response style R fill:#c8e6c9 style B fill:#e1f5ff TCMU Benefits : - Userspace implementation - Flexible backends - No kernel modifications - Easier debugging","title":"TCMU (Target Core Module Userspace)"},{"location":"storage/iscsi/#target-configuration","text":"","title":"Target Configuration"},{"location":"storage/iscsi/#lio-target-setup","text":"# Install LIO target apt-get install targetcli-fb # Start target service systemctl enable target systemctl start target # Open firewall firewall-cmd --permanent --add-port=3260/tcp firewall-cmd --reload","title":"LIO Target Setup"},{"location":"storage/iscsi/#creating-targets","text":"# Enter targetcli targetcli # Create backstore # Block device backend backstores/block create name=block_backend dev=/dev/vg0/vm1 # File backend backstores/fileio create name=file_backend \\ path=/var/lib/iscsi/disk.img size=10G # Create target iscsi/ create iqn.2026-01.com.example:storage.target1 # Create LUN iscsi/iqn.2026-01.com.example:storage.target1/tpg1/luns create \\ /backstores/block/block_backend # Create ACL iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls create \\ iqn.2026-01.com.example:initiator1 # Save configuration saveconfig exit","title":"Creating Targets"},{"location":"storage/iscsi/#target-configuration-structure","text":"/ \u251c\u2500\u2500 backstores/ \u2502 \u251c\u2500\u2500 block/ \u2502 \u251c\u2500\u2500 fileio/ \u2502 \u251c\u2500\u2500 pscsi/ \u2502 \u2514\u2500\u2500 ramdisk/ \u251c\u2500\u2500 iscsi/ \u2502 \u2514\u2500\u2500 iqn.2026-01.com.example:storage.target1/ \u2502 \u251c\u2500\u2500 tpg1/ \u2502 \u2502 \u251c\u2500\u2500 acls/ \u2502 \u2502 \u251c\u2500\u2500 luns/ \u2502 \u2502 \u251c\u2500\u2500 portals/ \u2502 \u2502 \u2514\u2500\u2500 params/ \u2502 \u2514\u2500\u2500 tpg2/ \u251c\u2500\u2500 loopback/ \u2514\u2500\u2500 core/","title":"Target Configuration Structure"},{"location":"storage/iscsi/#initiator-configuration","text":"","title":"Initiator Configuration"},{"location":"storage/iscsi/#iscsi-initiator-setup","text":"# Install iSCSI initiator apt-get install open-iscsi # Configure initiator name vim /etc/iscsi/initiatorname.iscsi # InitiatorName=iqn.2026-01.com.example:initiator1 # Start initiator service systemctl enable iscsid systemctl start iscsid","title":"iSCSI Initiator Setup"},{"location":"storage/iscsi/#discovery","text":"# Discover targets iscsiadm -m discovery -t st -p 192.168.1.100 # Discover with CHAP iscsiadm -m discovery -t st -p 192.168.1.100 \\ -o new -u username -w password # Discover targets on subnet iscsiadm -m discovery -t st -p 192.168.1.0/24","title":"Discovery"},{"location":"storage/iscsi/#connection","text":"# Login to target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 -l # Login to all discovered targets iscsiadm -m node -l # Logout from target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 -u # Logout from all targets iscsiadm -m node -u","title":"Connection"},{"location":"storage/iscsi/#persistent-login","text":"# Automatic login iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 --op update -n node.startup -v automatic # Manual login iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 \\ -p 192.168.1.100 --op update -n node.startup -v manual","title":"Persistent Login"},{"location":"storage/iscsi/#chap-authentication","text":"","title":"CHAP Authentication"},{"location":"storage/iscsi/#one-way-chap","text":"Target Configuration : # Set CHAP username and password iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ userid=username iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ password=password123 # Enable authentication iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ authentication=1 Initiator Configuration : # Set CHAP credentials vim /etc/iscsi/nodes/iqn.2026-01.com.example:storage.target1/192.168.1.100,3260/default node.session.auth.authmethod = CHAP node.session.auth.username = username node.session.auth.password = password123","title":"One-Way CHAP"},{"location":"storage/iscsi/#mutual-chap","text":"Target Configuration : # Set mutual CHAP iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ userid=username iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ password=password123 iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ mutual_userid=mutual_username iscsi/iqn.2026-01.com.example:storage.target1/tpg1/acls/ \\ iqn.2026-01.com.example:initiator1 set auth \\ mutual_password=mutual_password123 Initiator Configuration : # Set mutual CHAP vim /etc/iscsi/nodes/iqn.2026-01.com.example:storage.target1/192.168.1.100,3260/default node.session.auth.authmethod = CHAP node.session.auth.username = username node.session.auth.password = password123 node.session.auth.username_in = mutual_username node.session.auth.password_in = mutual_password123","title":"Mutual CHAP"},{"location":"storage/iscsi/#multipath-io","text":"","title":"Multipath I/O"},{"location":"storage/iscsi/#multipath-setup","text":"# Install multipath tools apt-get install multipath-tools # Configure multipath vim /etc/multipath.conf defaults { user_friendly_names yes find_multipaths yes }","title":"Multipath Setup"},{"location":"storage/iscsi/#multipath-configuration","text":"# /etc/multipath.conf defaults { user_friendly_names yes find_multipaths yes path_selector \"round-robin 0\" path_grouping_policy multibus failback immediate no_path_retry queue rr_min_io 1000 rr_min_io_rq 1 } devices { device { vendor \"LIO-ORG\" product \"TCMU device\" path_grouping_policy \"multibus\" getuid_callout \"/lib/udev/scsi_id --whitelisted --replace-whitespace --device=/dev/%n\" path_checker \"tur\" } } blacklist { devnode \"^sd[a-z]+\" }","title":"Multipath Configuration"},{"location":"storage/iscsi/#multipath-management","text":"# Scan for multipath devices multipath -F multipath -v2 # List multipath devices multipath -ll # Show device maps multipath -ll # Show device paths multipath -l # Flush multipath devices multipath -F","title":"Multipath Management"},{"location":"storage/iscsi/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"storage/iscsi/#target-tuning","text":"# Set queue depth iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ max_cmd_sn=65536 # Set session parameters iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ nodeacl.auth.username=username # Set queue parameters iscsi/iqn.2026-01.com.example:storage.target1/tpg1 set attribute \\ cmdsn_depth=128","title":"Target Tuning"},{"location":"storage/iscsi/#initiator-tuning","text":"# Set queue depth vim /etc/iscsi/iscsid.conf node.session.cmds_max=128 # Set initial login retries node.session.initial_login_retry_max=8 # Set timeout values node.session.timeo.replacement_timeout=120 node.conn[0].timeo.login_timeout=15 node.conn[0].timeo.logout_timeout=15 # Enable large receive offload node.conn[0].iscsi.HeaderDigest=None node.conn[0].iscsi.DataDigest=None node.conn[0].iscsi.IFMarker=No node.conn[0].iscsi.OFMarker=No","title":"Initiator Tuning"},{"location":"storage/iscsi/#high-availability","text":"","title":"High Availability"},{"location":"storage/iscsi/#ha-gateway-setup","text":"# Configure multiple portals iscsi/iqn.2026-01.com.example:storage.target1/tpg1/portals/ create \\ 192.168.1.100:3260 iscsi/iqn.2026-01.com.example:storage.target1/tpg1/portals/ create \\ 192.168.1.101:3260 # Create target with multiple portals iscsi/ create iqn.2026-01.com.example:storage.target2 iscsi/iqn.2026-01.com.example:storage.target2/tpg1/portals/ create \\ 192.168.1.102:3260 iscsi/iqn.2026-01.com.example:storage.target2/tpg1/portals/ create \\ 192.168.1.103:3260","title":"HA Gateway Setup"},{"location":"storage/iscsi/#load-balancing","text":"# Round-robin path selection path_selector \"round-robin 0\" # Least queue depth path_selector \"queue-depth 0\" # Service time path_selector \"service-time 0\"","title":"Load Balancing"},{"location":"storage/iscsi/#ceph-iscsi-gateway","text":"","title":"Ceph iSCSI Gateway"},{"location":"storage/iscsi/#ceph-iscsi-gateway-setup","text":"# Install Ceph iSCSI gateway apt-get install ceph-iscsi apt-get install tcmu-runner apt-get install targetcli-fb # Create RBD image rbd create rbd/disk1 --size 100G # Create iSCSI gateway configuration cat > gateway.yml <<EOF service_type: iscsi service_id: iscsi placement: hosts: - gw1 - gw2 spec: pools: - rbd api_user: admin api_secret: AQATjVdhSx3pGBAA2C7C1C9DjFwB2bY8x0l6g== trusted_ip_list: 192.168.1.100,192.168.1.101 EOF # Apply gateway configuration ceph orch apply -i gateway.yml","title":"Ceph iSCSI Gateway Setup"},{"location":"storage/iscsi/#gateway-cli","text":"# Connect to gateway gwcli.py # Create target target create iqn.2026-01.com.example:ceph.target1 # Create LUN lun create iqn.2026-01.com.example:ceph.target1 0 \\ --pool rbd --image disk1 # Create client client create iqn.2026-01.com.example:initiator1 # Add client to target client add iqn.2026-01.com.example:initiator1 \\ --lun 0 # Set CHAP client auth iqn.2026-01.com.example:initiator1 \\ --user username --password password","title":"Gateway CLI"},{"location":"storage/iscsi/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"storage/iscsi/#discovery-issues","text":"# Check target service systemctl status target # Check network ping 192.168.1.100 telnet 192.168.1.100 3260 # Check firewall iptables -L -n | grep 3260 # Check initiator service systemctl status iscsid # Discover targets iscsiadm -m discovery -t st -p 192.168.1.100 -d","title":"Discovery Issues"},{"location":"storage/iscsi/#connection-issues","text":"# Check initiator logs journalctl -u iscsid -f # Check target logs journalctl -u target -f # Check session status iscsiadm -m session # Check node status iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 # Test connection iscsiadm -m session -P 3","title":"Connection Issues"},{"location":"storage/iscsi/#performance-issues","text":"# Check multipath multipath -ll # Check I/O iostat -x 1 # Check network sar -n DEV 1 # Check initiator configuration cat /etc/iscsi/iscsid.conf # Check target configuration targetcli ls","title":"Performance Issues"},{"location":"storage/iscsi/#best-practices","text":"Use CHAP authentication for security Enable multipath for redundancy Configure HA gateways for high availability Use dedicated network for iSCSI Monitor iSCSI connections regularly Test failover scenarios Tune queue depth for performance Use LUN masking for access control Document configuration thoroughly Regular backups of configuration","title":"Best Practices"},{"location":"storage/iscsi/#quick-commands","text":"","title":"Quick Commands"},{"location":"storage/iscsi/#target-commands","text":"# Enter targetcli targetcli # Save configuration saveconfig # Restore configuration restoreconfig /etc/target/saveconfig.json # View configuration ls cd / ls","title":"Target Commands"},{"location":"storage/iscsi/#initiator-commands","text":"# Discover targets iscsiadm -m discovery -t st -p 192.168.1.100 # Login to target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 -p 192.168.1.100 -l # Logout from target iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 -p 192.168.1.100 -u # List sessions iscsiadm -m session # Delete node iscsiadm -m node -T iqn.2026-01.com.example:storage.target1 -p 192.168.1.100 --op delete","title":"Initiator Commands"},{"location":"storage/iscsi/#multipath-commands","text":"# Scan devices multipath -F multipath -v2 # List devices multipath -ll # Show device status multipath -ll mpatha # Resize multipath multipathd -k'resize map mpatha'","title":"Multipath Commands"},{"location":"storage/iscsi/#source-code","text":"Repository : https://github.com/linux-rdma/lio-core Ceph iSCSI : https://github.com/ceph/ceph/tree/master/ceph-iscsi Documentation : https://linux-iscsi.org/ LIO Documentation : https://github.com/open-iscsi/targetcli-fb","title":"Source Code"},{"location":"storage/iscsi/#key-components","text":"Component Location Description LIO Kernel drivers/target/ Kernel target tcmu-runner tcmu-runner/ Userspace runner targetcli targetcli/ Management tool open-iscsi open-iscsi/ Initiator multipath multipath-tools/ Multipath IO","title":"Key Components"},{"location":"storage/nvme-of/","text":"High-performance block storage protocol extending NVMe over network fabrics like TCP, RDMA, and FC. Architecture graph TB subgraph \"NVMe Host\" A[NVMe Driver] B[NVMe Controller] C[Namespace] D[Block Layer] E[Filesystem] end subgraph \"Network Fabric\" F[NVMe/TCP] G[NVMe/RoCE] H[NVMe/FC] I[RDMA Network] J[TCP/IP Network] K[Fibre Channel] end subgraph \"NVMe-oF Target\" L[NVMe Target] M[SPDK] N[Target Subsystem] O[Namespace] end subgraph \"Storage Backend\" P[RBD] Q[LVM] R[File] S[SPDK Backend] end A --> B B --> C C --> D D --> E A --> F A --> G A --> H F --> J G --> I H --> K L --> F L --> G L --> H L --> M M --> N N --> O O --> P O --> Q O --> R O --> S style L fill:#c8e6c9 style M fill:#ffecb3 style N fill:#e1f5ff style P fill:#fff3e0 Core Components NVMe Host NVMe host driver for connecting to NVMe-oF targets. NVMe Host Components : | Component | Description | |-----------|-------------| | NVMe Core | NVMe protocol driver | | NVMe TCP | NVMe over TCP transport | | NVMe RDMA | NVMe over RDMA transport | | NVMe FC | NVMe over Fibre Channel | | Namespace | NVMe namespace (block device) | | Controller | NVMe controller instance | NVMe Operation Flow : sequenceDiagram participant H as Host participant T as Transport participant S as Storage H->>T: NVMe Command T->>S: I/O Request S->>S: Process I/O S->>T: I/O Response T->>H: NVMe Completion H->>H: Handle Completion style S fill:#c8e6c9 style T fill:#e1f5ff NVMe-oF Target Target implementing NVMe-oF protocol for block storage. NVMe-oF Target Components : | Component | Description | |-----------|-------------| | Target | NVMe-oF target daemon | | Subsystem | NVMe subsystem (collection of namespaces) | | Namespace | Block storage unit | | Listener | Network listener for connections | | Host | Allowed NVMe host connections | NVMe-oF Target Types : | Type | Description | Performance | |------|-------------|------------| | NVMe/TCP | NVMe over TCP | Good | | NVMe/RoCE | NVMe over RDMA | Excellent | | NVMe/FC | NVMe over Fibre Channel | Excellent | Transport Types NVMe/TCP NVMe over standard TCP/IP network. NVMe/TCP Characteristics : | Feature | Description | |---------|-------------| | Standard TCP | Works over any IP network | | MTU Support | Up to 9000 bytes (Jumbo frames) | | No RDMA required | Works without RDMA hardware | | Lower performance | Compared to RDMA transports | NVMe/TCP Setup : # Install NVMe/TCP target apt-get install nvme-stas apt-get install nvmetcli # Create TCP listener nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --device=/dev/vg0/vm1 nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 NVMe/TCP Host Connection : # Connect to target nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # View connections nvme list # Disconnect nvme disconnect -n nqn.2026-01.com.example:storage.target1 NVMe/RoCE NVMe over RDMA Converged Ethernet. NVMe/RoCE Characteristics : | Feature | Description | |---------|-------------| | RDMA | Remote Direct Memory Access | | High Performance | Low latency, high throughput | | Requires RDMA | Mellanox, Intel NICs | | MTU Support | Up to 9000 bytes | NVMe/RoCE Setup : # Install RDMA packages apt-get install rdma-core apt-get install ibutils apt-get install infiniband-diags # Configure RDMA modprobe rdma_cm modprobe ib_core modprobe ib_uverbs # Create RDMA listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=rdma --traddr=192.168.1.100 --trsvcid=4420 NVMe/RoCE Host Connection : # Connect with RDMA nvme connect -t rdma -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # View RDMA connections nvme list ibv_devinfo -v NVMe/FC NVMe over Fibre Channel. NVMe/FC Characteristics : | Feature | Description | |---------|-------------| | Fibre Channel | Enterprise storage networking | | High Availability | Redundant paths | | Zoning | Fibre Channel zoning | | WWN Identification | World Wide Name | NVMe/FC Setup : # Install NVMe/FC packages apt-get install nvme-fc apt-get install lsscsi apt-get install sg3-utils # Configure Fibre Channel modprobe lpfc modprobe qla2xxx # Create FC listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=fc --traddr=10:00:00:00:00:00:01 NVMe/FC Host Connection : # Discover FC targets nvme discover -t fc -w 10:00:00:00:00:00:01 # Connect to FC target nvme connect -t fc -n nqn.2026-01.com.example:storage.target1 \\ -w 10:00:00:00:00:00:01 # View FC connections nvme list lsscsi Target Configuration Creating Subsystem # Create subsystem nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 # Create subsystem with allow-all hosts nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --allow-all-hosts Creating Namespace # Create namespace with block device nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --device=/dev/vg0/vm1 --nsid=1 # Create namespace with file backend nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --path=/var/lib/nvme/disk.img --size=10G --nsid=1 Creating Listener # Create TCP listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 # Create RDMA listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=rdma --traddr=192.168.1.100 --trsvcid=4420 # Create FC listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=fc --traddr=10:00:00:00:00:00:01 Host Configuration # Allow specific host nvme host add --subsystem=nqn.2026-01.com.example:storage.target1 \\ --host=nqn.2026-01.com.example:initiator1 # Delete host nvme host delete --subsystem=nqn.2026-01.com.example:storage.target1 \\ --host=nqn.2026-01.com.example:initiator1 Discovery Discovery Methods # Discovery via transport nvme discover -t tcp -a 192.168.1.100 -s 4420 # Discovery via NQN nvme discover -n nqn.2026-01.com.example:storage.target1 # Discovery with all hosts nvme discover -t tcp -a 192.168.1.100 -s 4420 --hostid Discovery Details # View discovery log cat /etc/nvme/discovery.conf # Manual discovery entry # Discovery Entry for nqn.2026-01.com.example:storage.target1 -transport=tcp -traddr=192.168.1.100 -trsvcid=4420 -n nqn.2026-01.com.example:storage.target1 Connection Management Connection Operations # Connect to target nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # Connect with specific host NQN nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 -q nqn.2026-01.com.example:initiator1 # Connect with multiple queues nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 -c 32 # Disconnect from target nvme disconnect -n nqn.2026-01.com.example:storage.target1 # Disconnect all nvme disconnect-all Persistent Connections # Add to discovery.conf echo \"-transport=tcp -traddr=192.168.1.100 -trsvcid=4420 \\ -n nqn.2026-01.com.example:storage.target1\" >> /etc/nvme/discovery.conf # Enable auto-connect systemctl enable nvme-autoconnect systemctl start nvme-autoconnect Performance Tuning Host Tuning # Increase queue depth nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 -c 32 -i 32 # Enable write buffering echo 1 > /sys/block/nvme0n1/queue/write_cache # Set scheduler echo none > /sys/block/nvme0n1/queue/scheduler # Set I/O scheduler echo mq-deadline > /sys/block/nvme0n1/queue/scheduler # Enable jumbo frames ip link set eth0 mtu 9000 Target Tuning # Set I/O queues nvme subsystem modify --subsystem=nqn.2026-01.com.example:storage.target1 \\ --queue-size=128 # Set number of queues nvme subsystem modify --subsystem=nqn.2026-01.com.example:storage.target1 \\ --nr-io-queues=32 # Set keep-alive interval nvme subsystem modify --subsystem=nqn.2026-01.com.example:storage.target1 \\ --kato=10 Network Tuning # Enable jumbo frames ip link set eth0 mtu 9000 # Enable TCP offload ethtool -K eth0 tso on ethtool -K eth0 gso on ethtool -K eth0 lro on # Increase TCP buffer sysctl -w net.ipv4.tcp_window_scaling=1 sysctl -w net.ipv4.tcp_timestamps=1 sysctl -w net.ipv4.tcp_rmem='4096 65536 16777216' sysctl -w net.ipv4.tcp_wmem='4096 65536 16777216' High Availability HA Gateway Configuration # Create multiple listeners nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.101 --trsvcid=4420 # Configure multipath vim /etc/multipath.conf defaults { user_friendly_names yes find_multipaths yes } devices { device { vendor \"Linux\" product \"NVMe\" } } Load Balancing # Round-robin path selection path_selector \"round-robin 0\" # Load balancing nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.101 -s 4420 Ceph NVMe-oF Gateway Ceph NVMe-oF Setup # Install Ceph NVMe-oF gateway apt-get install ceph-nvmeof apt-get install rbd-nvmeof apt-get install spdk # Create RBD image rbd create rbd/disk1 --size 100G # Create NVMe-oF gateway configuration cat > gateway.yml <<EOF service_type: nvmeof service_id: nvmeof placement: hosts: - gw1 - gw2 spec: pools: - rbd api_user: admin api_secret: AQATjVdhSx3pGBAA2C7C1C9DjFwB2bY8x0l6g== trusted_ip_list: 192.168.1.100,192.168.1.101 EOF # Apply gateway configuration ceph orch apply -i gateway.yml Gateway Management # Create subsystem gwcli.py subsystem create nqn.2026-01.com.example:ceph.target1 # Create namespace gwcli.py namespace create nqn.2026-01.com.example:ceph.target1 1 \\ --pool rbd --image disk1 # Create listener gwcli.py listener create nqn.2026-01.com.example:ceph.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 # Add host gwcli.py host add nqn.2026-01.com.example:ceph.target1 \\ nqn.2026-01.com.example:initiator1 SPDK Integration SPDK Target # Install SPDK apt-get install spdk apt-get install dpdk apt-get install rdma-core # Configure SPDK modprobe vfio-pci modprobe uio_pci_generic # Create SPDK target nvmf_tgt # Create subsystem rpc.py nvmf_create_subsystem nqn.2026-01.com.example:spdk.target1 # Create namespace rpc.py bdev_rbd_create rbd=rbd/disk1 rpc.py nvmf_subsystem_add_ns nqn.2026-01.com.example:spdk.target1 \\ bdev_name=Rbd0 # Create listener rpc.py nvmf_subsystem_add_listener nqn.2026-01.com.example:spdk.target1 \\ -t TCP -a 192.168.1.100 -s 4420 Troubleshooting Discovery Issues # Check target status nvme list-subsys # Check network ping 192.168.1.100 telnet 192.168.1.100 4420 # Check NVMe driver lsmod | grep nvme dmesg | grep nvme # Check discovery nvme discover -t tcp -a 192.168.1.100 -s 4420 -d Connection Issues # Check connections nvme list nvme list-subsys # Check logs journalctl -u nvme -f dmesg | grep nvme # Check subsystems nvme list-subsys # Check namespaces nvme list Performance Issues # Check I/O iostat -x 1 # Check network sar -n DEV 1 # Check NVMe stats nvme list -v cat /sys/class/nvme/nvme0/device/statistics # Check RDMA ibv_devinfo -v Best Practices Use RDMA for high performance Enable jumbo frames for better throughput Configure multipath for HA Use dedicated network for NVMe-oF Monitor connections regularly Test failover scenarios Tune queue depth for workload Use proper authentication Document configuration thoroughly Regular backups of setup Quick Commands Target Commands # Create subsystem nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 # Create namespace nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --device=/dev/vg0/vm1 # Create listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 # Delete subsystem nvme subsystem delete --subsystem=nqn.2026-01.com.example:storage.target1 Host Commands # Discover targets nvme discover -t tcp -a 192.168.1.100 -s 4420 # Connect to target nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # Disconnect nvme disconnect -n nqn.2026-01.com.example:storage.target1 # List subsystems nvme list-subsys # List namespaces nvme list Multipath Commands # Configure multipath multipath -F multipath -v2 # List multipath devices multipath -ll Source Code Repository : https://github.com/linux-nvme/nvme-cli Ceph NVMe-oF : https://github.com/ceph/ceph/tree/master/src/monitoring SPDK : https://github.com/spdk/spdk Documentation : https://nvmexpress.org/ Key Components Component Location Description NVMe Core drivers/nvme/ NVMe driver NVMe TCP drivers/nvme/host/tcp.c NVMe/TCP transport NVMe RDMA drivers/nvme/host/rdma.c NVMe/RDMA transport NVMe FC drivers/nvme/host/fc.c NVMe/FC transport SPDK spdk/ Userspace NVMe","title":"NVMe-oF"},{"location":"storage/nvme-of/#architecture","text":"graph TB subgraph \"NVMe Host\" A[NVMe Driver] B[NVMe Controller] C[Namespace] D[Block Layer] E[Filesystem] end subgraph \"Network Fabric\" F[NVMe/TCP] G[NVMe/RoCE] H[NVMe/FC] I[RDMA Network] J[TCP/IP Network] K[Fibre Channel] end subgraph \"NVMe-oF Target\" L[NVMe Target] M[SPDK] N[Target Subsystem] O[Namespace] end subgraph \"Storage Backend\" P[RBD] Q[LVM] R[File] S[SPDK Backend] end A --> B B --> C C --> D D --> E A --> F A --> G A --> H F --> J G --> I H --> K L --> F L --> G L --> H L --> M M --> N N --> O O --> P O --> Q O --> R O --> S style L fill:#c8e6c9 style M fill:#ffecb3 style N fill:#e1f5ff style P fill:#fff3e0","title":"Architecture"},{"location":"storage/nvme-of/#core-components","text":"","title":"Core Components"},{"location":"storage/nvme-of/#nvme-host","text":"NVMe host driver for connecting to NVMe-oF targets. NVMe Host Components : | Component | Description | |-----------|-------------| | NVMe Core | NVMe protocol driver | | NVMe TCP | NVMe over TCP transport | | NVMe RDMA | NVMe over RDMA transport | | NVMe FC | NVMe over Fibre Channel | | Namespace | NVMe namespace (block device) | | Controller | NVMe controller instance | NVMe Operation Flow : sequenceDiagram participant H as Host participant T as Transport participant S as Storage H->>T: NVMe Command T->>S: I/O Request S->>S: Process I/O S->>T: I/O Response T->>H: NVMe Completion H->>H: Handle Completion style S fill:#c8e6c9 style T fill:#e1f5ff","title":"NVMe Host"},{"location":"storage/nvme-of/#nvme-of-target","text":"Target implementing NVMe-oF protocol for block storage. NVMe-oF Target Components : | Component | Description | |-----------|-------------| | Target | NVMe-oF target daemon | | Subsystem | NVMe subsystem (collection of namespaces) | | Namespace | Block storage unit | | Listener | Network listener for connections | | Host | Allowed NVMe host connections | NVMe-oF Target Types : | Type | Description | Performance | |------|-------------|------------| | NVMe/TCP | NVMe over TCP | Good | | NVMe/RoCE | NVMe over RDMA | Excellent | | NVMe/FC | NVMe over Fibre Channel | Excellent |","title":"NVMe-oF Target"},{"location":"storage/nvme-of/#transport-types","text":"","title":"Transport Types"},{"location":"storage/nvme-of/#nvmetcp","text":"NVMe over standard TCP/IP network. NVMe/TCP Characteristics : | Feature | Description | |---------|-------------| | Standard TCP | Works over any IP network | | MTU Support | Up to 9000 bytes (Jumbo frames) | | No RDMA required | Works without RDMA hardware | | Lower performance | Compared to RDMA transports | NVMe/TCP Setup : # Install NVMe/TCP target apt-get install nvme-stas apt-get install nvmetcli # Create TCP listener nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --device=/dev/vg0/vm1 nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 NVMe/TCP Host Connection : # Connect to target nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # View connections nvme list # Disconnect nvme disconnect -n nqn.2026-01.com.example:storage.target1","title":"NVMe/TCP"},{"location":"storage/nvme-of/#nvmeroce","text":"NVMe over RDMA Converged Ethernet. NVMe/RoCE Characteristics : | Feature | Description | |---------|-------------| | RDMA | Remote Direct Memory Access | | High Performance | Low latency, high throughput | | Requires RDMA | Mellanox, Intel NICs | | MTU Support | Up to 9000 bytes | NVMe/RoCE Setup : # Install RDMA packages apt-get install rdma-core apt-get install ibutils apt-get install infiniband-diags # Configure RDMA modprobe rdma_cm modprobe ib_core modprobe ib_uverbs # Create RDMA listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=rdma --traddr=192.168.1.100 --trsvcid=4420 NVMe/RoCE Host Connection : # Connect with RDMA nvme connect -t rdma -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # View RDMA connections nvme list ibv_devinfo -v","title":"NVMe/RoCE"},{"location":"storage/nvme-of/#nvmefc","text":"NVMe over Fibre Channel. NVMe/FC Characteristics : | Feature | Description | |---------|-------------| | Fibre Channel | Enterprise storage networking | | High Availability | Redundant paths | | Zoning | Fibre Channel zoning | | WWN Identification | World Wide Name | NVMe/FC Setup : # Install NVMe/FC packages apt-get install nvme-fc apt-get install lsscsi apt-get install sg3-utils # Configure Fibre Channel modprobe lpfc modprobe qla2xxx # Create FC listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=fc --traddr=10:00:00:00:00:00:01 NVMe/FC Host Connection : # Discover FC targets nvme discover -t fc -w 10:00:00:00:00:00:01 # Connect to FC target nvme connect -t fc -n nqn.2026-01.com.example:storage.target1 \\ -w 10:00:00:00:00:00:01 # View FC connections nvme list lsscsi","title":"NVMe/FC"},{"location":"storage/nvme-of/#target-configuration","text":"","title":"Target Configuration"},{"location":"storage/nvme-of/#creating-subsystem","text":"# Create subsystem nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 # Create subsystem with allow-all hosts nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --allow-all-hosts","title":"Creating Subsystem"},{"location":"storage/nvme-of/#creating-namespace","text":"# Create namespace with block device nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --device=/dev/vg0/vm1 --nsid=1 # Create namespace with file backend nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --path=/var/lib/nvme/disk.img --size=10G --nsid=1","title":"Creating Namespace"},{"location":"storage/nvme-of/#creating-listener","text":"# Create TCP listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 # Create RDMA listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=rdma --traddr=192.168.1.100 --trsvcid=4420 # Create FC listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=fc --traddr=10:00:00:00:00:00:01","title":"Creating Listener"},{"location":"storage/nvme-of/#host-configuration","text":"# Allow specific host nvme host add --subsystem=nqn.2026-01.com.example:storage.target1 \\ --host=nqn.2026-01.com.example:initiator1 # Delete host nvme host delete --subsystem=nqn.2026-01.com.example:storage.target1 \\ --host=nqn.2026-01.com.example:initiator1","title":"Host Configuration"},{"location":"storage/nvme-of/#discovery","text":"","title":"Discovery"},{"location":"storage/nvme-of/#discovery-methods","text":"# Discovery via transport nvme discover -t tcp -a 192.168.1.100 -s 4420 # Discovery via NQN nvme discover -n nqn.2026-01.com.example:storage.target1 # Discovery with all hosts nvme discover -t tcp -a 192.168.1.100 -s 4420 --hostid","title":"Discovery Methods"},{"location":"storage/nvme-of/#discovery-details","text":"# View discovery log cat /etc/nvme/discovery.conf # Manual discovery entry # Discovery Entry for nqn.2026-01.com.example:storage.target1 -transport=tcp -traddr=192.168.1.100 -trsvcid=4420 -n nqn.2026-01.com.example:storage.target1","title":"Discovery Details"},{"location":"storage/nvme-of/#connection-management","text":"","title":"Connection Management"},{"location":"storage/nvme-of/#connection-operations","text":"# Connect to target nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # Connect with specific host NQN nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 -q nqn.2026-01.com.example:initiator1 # Connect with multiple queues nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 -c 32 # Disconnect from target nvme disconnect -n nqn.2026-01.com.example:storage.target1 # Disconnect all nvme disconnect-all","title":"Connection Operations"},{"location":"storage/nvme-of/#persistent-connections","text":"# Add to discovery.conf echo \"-transport=tcp -traddr=192.168.1.100 -trsvcid=4420 \\ -n nqn.2026-01.com.example:storage.target1\" >> /etc/nvme/discovery.conf # Enable auto-connect systemctl enable nvme-autoconnect systemctl start nvme-autoconnect","title":"Persistent Connections"},{"location":"storage/nvme-of/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"storage/nvme-of/#host-tuning","text":"# Increase queue depth nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 -c 32 -i 32 # Enable write buffering echo 1 > /sys/block/nvme0n1/queue/write_cache # Set scheduler echo none > /sys/block/nvme0n1/queue/scheduler # Set I/O scheduler echo mq-deadline > /sys/block/nvme0n1/queue/scheduler # Enable jumbo frames ip link set eth0 mtu 9000","title":"Host Tuning"},{"location":"storage/nvme-of/#target-tuning","text":"# Set I/O queues nvme subsystem modify --subsystem=nqn.2026-01.com.example:storage.target1 \\ --queue-size=128 # Set number of queues nvme subsystem modify --subsystem=nqn.2026-01.com.example:storage.target1 \\ --nr-io-queues=32 # Set keep-alive interval nvme subsystem modify --subsystem=nqn.2026-01.com.example:storage.target1 \\ --kato=10","title":"Target Tuning"},{"location":"storage/nvme-of/#network-tuning","text":"# Enable jumbo frames ip link set eth0 mtu 9000 # Enable TCP offload ethtool -K eth0 tso on ethtool -K eth0 gso on ethtool -K eth0 lro on # Increase TCP buffer sysctl -w net.ipv4.tcp_window_scaling=1 sysctl -w net.ipv4.tcp_timestamps=1 sysctl -w net.ipv4.tcp_rmem='4096 65536 16777216' sysctl -w net.ipv4.tcp_wmem='4096 65536 16777216'","title":"Network Tuning"},{"location":"storage/nvme-of/#high-availability","text":"","title":"High Availability"},{"location":"storage/nvme-of/#ha-gateway-configuration","text":"# Create multiple listeners nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.101 --trsvcid=4420 # Configure multipath vim /etc/multipath.conf defaults { user_friendly_names yes find_multipaths yes } devices { device { vendor \"Linux\" product \"NVMe\" } }","title":"HA Gateway Configuration"},{"location":"storage/nvme-of/#load-balancing","text":"# Round-robin path selection path_selector \"round-robin 0\" # Load balancing nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.101 -s 4420","title":"Load Balancing"},{"location":"storage/nvme-of/#ceph-nvme-of-gateway","text":"","title":"Ceph NVMe-oF Gateway"},{"location":"storage/nvme-of/#ceph-nvme-of-setup","text":"# Install Ceph NVMe-oF gateway apt-get install ceph-nvmeof apt-get install rbd-nvmeof apt-get install spdk # Create RBD image rbd create rbd/disk1 --size 100G # Create NVMe-oF gateway configuration cat > gateway.yml <<EOF service_type: nvmeof service_id: nvmeof placement: hosts: - gw1 - gw2 spec: pools: - rbd api_user: admin api_secret: AQATjVdhSx3pGBAA2C7C1C9DjFwB2bY8x0l6g== trusted_ip_list: 192.168.1.100,192.168.1.101 EOF # Apply gateway configuration ceph orch apply -i gateway.yml","title":"Ceph NVMe-oF Setup"},{"location":"storage/nvme-of/#gateway-management","text":"# Create subsystem gwcli.py subsystem create nqn.2026-01.com.example:ceph.target1 # Create namespace gwcli.py namespace create nqn.2026-01.com.example:ceph.target1 1 \\ --pool rbd --image disk1 # Create listener gwcli.py listener create nqn.2026-01.com.example:ceph.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 # Add host gwcli.py host add nqn.2026-01.com.example:ceph.target1 \\ nqn.2026-01.com.example:initiator1","title":"Gateway Management"},{"location":"storage/nvme-of/#spdk-integration","text":"","title":"SPDK Integration"},{"location":"storage/nvme-of/#spdk-target","text":"# Install SPDK apt-get install spdk apt-get install dpdk apt-get install rdma-core # Configure SPDK modprobe vfio-pci modprobe uio_pci_generic # Create SPDK target nvmf_tgt # Create subsystem rpc.py nvmf_create_subsystem nqn.2026-01.com.example:spdk.target1 # Create namespace rpc.py bdev_rbd_create rbd=rbd/disk1 rpc.py nvmf_subsystem_add_ns nqn.2026-01.com.example:spdk.target1 \\ bdev_name=Rbd0 # Create listener rpc.py nvmf_subsystem_add_listener nqn.2026-01.com.example:spdk.target1 \\ -t TCP -a 192.168.1.100 -s 4420","title":"SPDK Target"},{"location":"storage/nvme-of/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"storage/nvme-of/#discovery-issues","text":"# Check target status nvme list-subsys # Check network ping 192.168.1.100 telnet 192.168.1.100 4420 # Check NVMe driver lsmod | grep nvme dmesg | grep nvme # Check discovery nvme discover -t tcp -a 192.168.1.100 -s 4420 -d","title":"Discovery Issues"},{"location":"storage/nvme-of/#connection-issues","text":"# Check connections nvme list nvme list-subsys # Check logs journalctl -u nvme -f dmesg | grep nvme # Check subsystems nvme list-subsys # Check namespaces nvme list","title":"Connection Issues"},{"location":"storage/nvme-of/#performance-issues","text":"# Check I/O iostat -x 1 # Check network sar -n DEV 1 # Check NVMe stats nvme list -v cat /sys/class/nvme/nvme0/device/statistics # Check RDMA ibv_devinfo -v","title":"Performance Issues"},{"location":"storage/nvme-of/#best-practices","text":"Use RDMA for high performance Enable jumbo frames for better throughput Configure multipath for HA Use dedicated network for NVMe-oF Monitor connections regularly Test failover scenarios Tune queue depth for workload Use proper authentication Document configuration thoroughly Regular backups of setup","title":"Best Practices"},{"location":"storage/nvme-of/#quick-commands","text":"","title":"Quick Commands"},{"location":"storage/nvme-of/#target-commands","text":"# Create subsystem nvme subsystem create --subsystem=nqn.2026-01.com.example:storage.target1 # Create namespace nvme namespace create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --device=/dev/vg0/vm1 # Create listener nvme listener create --subsystem=nqn.2026-01.com.example:storage.target1 \\ --transport=tcp --traddr=192.168.1.100 --trsvcid=4420 # Delete subsystem nvme subsystem delete --subsystem=nqn.2026-01.com.example:storage.target1","title":"Target Commands"},{"location":"storage/nvme-of/#host-commands","text":"# Discover targets nvme discover -t tcp -a 192.168.1.100 -s 4420 # Connect to target nvme connect -t tcp -n nqn.2026-01.com.example:storage.target1 \\ -a 192.168.1.100 -s 4420 # Disconnect nvme disconnect -n nqn.2026-01.com.example:storage.target1 # List subsystems nvme list-subsys # List namespaces nvme list","title":"Host Commands"},{"location":"storage/nvme-of/#multipath-commands","text":"# Configure multipath multipath -F multipath -v2 # List multipath devices multipath -ll","title":"Multipath Commands"},{"location":"storage/nvme-of/#source-code","text":"Repository : https://github.com/linux-nvme/nvme-cli Ceph NVMe-oF : https://github.com/ceph/ceph/tree/master/src/monitoring SPDK : https://github.com/spdk/spdk Documentation : https://nvmexpress.org/","title":"Source Code"},{"location":"storage/nvme-of/#key-components","text":"Component Location Description NVMe Core drivers/nvme/ NVMe driver NVMe TCP drivers/nvme/host/tcp.c NVMe/TCP transport NVMe RDMA drivers/nvme/host/rdma.c NVMe/RDMA transport NVMe FC drivers/nvme/host/fc.c NVMe/FC transport SPDK spdk/ Userspace NVMe","title":"Key Components"},{"location":"virtualization/","text":"Complete reference guide for QEMU, KVM, libvirt, virsh, and Open vSwitch virtualization technologies. QEMU Generic and open source machine emulator providing full-system and user-mode emulation. Architecture graph TB A[QEMU Process] --> B[VCPU Threads] A --> C[IO Thread] A --> D[Timer Thread] B --> E[TCG Translator] B --> F[KVM Acceleration] E --> G[Dynamic Binary Translation] F --> H[VM Exit/Entry] Key Features Full-system emulation (x86, ARM, PowerPC, etc.) User-mode emulation TCG dynamic translation KVM integration for hardware acceleration Device emulation (network, storage, graphics) Quick Commands # Create VM qemu-system-x86_64 -name vm1 -m 2048 -smp 2 \\ -drive file=disk.qcow2,format=qcow2 \\ -netdev user,id=net0,hostfwd=tcp::2222-:22 # Enable KVM qemu-system-x86_64 -enable-kvm -name vm1 -m 4096 -smp 4 # Disk management qemu-img create -f qcow2 disk.qcow2 20G qemu-img info disk.qcow2 qemu-img resize disk.qcow2 +10G Source Code Repository : qemu-project/qemu Documentation : qemu.org KVM Kernel-based virtual machine providing hardware-assisted virtualization. Architecture graph TB A[KVM Module] --> B[VMX/SVM Support] A --> C[KVM Kernel Modules] B --> D[QEMU Userspace] C --> E[Virtual Machine Monitor] E --> F[Guest VM] Key Features Hardware virtualization (Intel VT-x, AMD-V) Near-native performance EPT/NPT memory management Virtio I/O virtualization Live migration support Quick Commands # Check support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo # Load modules modprobe kvm-intel # Intel modprobe kvm-amd # AMD # Verify kvm-ok Source Code Location : virt/kvm/ in Linux kernel Repository : torvalds/linux Documentation : linux-kvm.org libvirt Virtualization API and daemon for managing virtualization capabilities. Architecture graph TB A[libvirtd Daemon] --> B[QEMU/KVM] A --> C[LXC Containers] A --> D[Xen] E[Management Tools] --> F[virsh CLI] E --> G[virt-manager GUI] F --> A G --> A Key Features Unified API for multiple hypervisors XML-based VM configuration Network and storage management Live migration support Secure API with authentication Quick Commands # List VMs virsh list --all # VM operations virsh start vm1 virsh shutdown vm1 virsh destroy vm1 virsh dumpxml vm1 # Network management virsh net-list virsh net-define network.xml virsh net-start default Source Code Repository : libvirt/libvirt Documentation : libvirt.org virsh Command-line interface for managing libvirt virtualization. Quick Commands # VM Management virsh list # List running VMs virsh list --all # List all VMs virsh start <vm-name> # Start VM virsh shutdown <vm-name> # Shutdown VM virsh destroy <vm-name> # Force stop VM virsh undefine <vm-name> # Remove VM # VM Information virsh dominfo <vm-name> # Domain info virsh dumpxml <vm-name> # XML config virsh vcpuinfo <vm-name> # VCPU info virsh dommemstat <vm-name> # Memory stats # Snapshot Management virsh snapshot-create <vm-name> <snap-name> virsh snapshot-list <vm-name> virsh snapshot-revert <vm-name> <snap-name> # Console Access virsh console <vm-name> virsh qemu-monitor-command <vm-name> \"info status\" Open vSwitch Open source multilayer virtual switch for managing VM networking. Architecture graph TB A[VM1 eth0] --> B[OVS Bridge br0] C[VM2 eth0] --> B D[VM3 eth0] --> B B --> E[OVS Database] E --> F[Controller] G[Host eth0] --> H[OVS Bridge br0] I[Management Tools] --> E Key Features Standard Linux bridge compatibility Flow-based switching VLAN isolation Bonding and trunking support Remote management protocol Quick Commands # Bridge Management ovs-vsctl add-br br0 ovs-vsctl del-br br0 ovs-vsctl list-br # Port Management ovs-vsctl add-port br0 eth0 ovs-vsctl del-port br0 eth0 ovs-vsctl list-ports br0 # Configuration ovs-vsctl show ovs-ofctl show br0 ovs-vsctls show Source Code Repository : openvswitch/ovs Documentation : openvswitch.org Deployment Workflow 1. Hardware Verification # Check virtualization support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo 2. Software Installation # Install packages apt-get install qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch # Enable services systemctl enable libvirtd systemctl start libvirtd systemctl enable openvswitch systemctl start openvswitch # Verify virsh version ovs-vsctl show 3. Network Setup # Create OVS bridge ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 eth1 4. Create VM # Using virt-install virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path=/var/lib/libvirt/images/vm1.qcow2,size=20 \\ --network bridge=br0 # Or using virsh virsh define vm1.xml virsh start vm1 Troubleshooting For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment . Common Issues Issue Solution VM slow performance Enable KVM with -enable-kvm Network issues Check OVS bridges with ovs-vsctl show Storage issues Use virtio drivers for better I/O Resource constraints Adjust memory/CPU allocation Security Considerations Use AppArmor/SELinux profiles Use virtio drivers for better isolation Enable seccomp filters Use IOMMU for device passthrough Source Code References Technology Repository Documentation QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org Open vSwitch openvswitch/ovs openvswitch.org","title":"Overview"},{"location":"virtualization/#qemu","text":"Generic and open source machine emulator providing full-system and user-mode emulation.","title":"QEMU"},{"location":"virtualization/#architecture","text":"graph TB A[QEMU Process] --> B[VCPU Threads] A --> C[IO Thread] A --> D[Timer Thread] B --> E[TCG Translator] B --> F[KVM Acceleration] E --> G[Dynamic Binary Translation] F --> H[VM Exit/Entry]","title":"Architecture"},{"location":"virtualization/#key-features","text":"Full-system emulation (x86, ARM, PowerPC, etc.) User-mode emulation TCG dynamic translation KVM integration for hardware acceleration Device emulation (network, storage, graphics)","title":"Key Features"},{"location":"virtualization/#quick-commands","text":"# Create VM qemu-system-x86_64 -name vm1 -m 2048 -smp 2 \\ -drive file=disk.qcow2,format=qcow2 \\ -netdev user,id=net0,hostfwd=tcp::2222-:22 # Enable KVM qemu-system-x86_64 -enable-kvm -name vm1 -m 4096 -smp 4 # Disk management qemu-img create -f qcow2 disk.qcow2 20G qemu-img info disk.qcow2 qemu-img resize disk.qcow2 +10G","title":"Quick Commands"},{"location":"virtualization/#source-code","text":"Repository : qemu-project/qemu Documentation : qemu.org","title":"Source Code"},{"location":"virtualization/#kvm","text":"Kernel-based virtual machine providing hardware-assisted virtualization.","title":"KVM"},{"location":"virtualization/#architecture_1","text":"graph TB A[KVM Module] --> B[VMX/SVM Support] A --> C[KVM Kernel Modules] B --> D[QEMU Userspace] C --> E[Virtual Machine Monitor] E --> F[Guest VM]","title":"Architecture"},{"location":"virtualization/#key-features_1","text":"Hardware virtualization (Intel VT-x, AMD-V) Near-native performance EPT/NPT memory management Virtio I/O virtualization Live migration support","title":"Key Features"},{"location":"virtualization/#quick-commands_1","text":"# Check support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo # Load modules modprobe kvm-intel # Intel modprobe kvm-amd # AMD # Verify kvm-ok","title":"Quick Commands"},{"location":"virtualization/#source-code_1","text":"Location : virt/kvm/ in Linux kernel Repository : torvalds/linux Documentation : linux-kvm.org","title":"Source Code"},{"location":"virtualization/#libvirt","text":"Virtualization API and daemon for managing virtualization capabilities.","title":"libvirt"},{"location":"virtualization/#architecture_2","text":"graph TB A[libvirtd Daemon] --> B[QEMU/KVM] A --> C[LXC Containers] A --> D[Xen] E[Management Tools] --> F[virsh CLI] E --> G[virt-manager GUI] F --> A G --> A","title":"Architecture"},{"location":"virtualization/#key-features_2","text":"Unified API for multiple hypervisors XML-based VM configuration Network and storage management Live migration support Secure API with authentication","title":"Key Features"},{"location":"virtualization/#quick-commands_2","text":"# List VMs virsh list --all # VM operations virsh start vm1 virsh shutdown vm1 virsh destroy vm1 virsh dumpxml vm1 # Network management virsh net-list virsh net-define network.xml virsh net-start default","title":"Quick Commands"},{"location":"virtualization/#source-code_2","text":"Repository : libvirt/libvirt Documentation : libvirt.org","title":"Source Code"},{"location":"virtualization/#virsh","text":"Command-line interface for managing libvirt virtualization.","title":"virsh"},{"location":"virtualization/#quick-commands_3","text":"# VM Management virsh list # List running VMs virsh list --all # List all VMs virsh start <vm-name> # Start VM virsh shutdown <vm-name> # Shutdown VM virsh destroy <vm-name> # Force stop VM virsh undefine <vm-name> # Remove VM # VM Information virsh dominfo <vm-name> # Domain info virsh dumpxml <vm-name> # XML config virsh vcpuinfo <vm-name> # VCPU info virsh dommemstat <vm-name> # Memory stats # Snapshot Management virsh snapshot-create <vm-name> <snap-name> virsh snapshot-list <vm-name> virsh snapshot-revert <vm-name> <snap-name> # Console Access virsh console <vm-name> virsh qemu-monitor-command <vm-name> \"info status\"","title":"Quick Commands"},{"location":"virtualization/#open-vswitch","text":"Open source multilayer virtual switch for managing VM networking.","title":"Open vSwitch"},{"location":"virtualization/#architecture_3","text":"graph TB A[VM1 eth0] --> B[OVS Bridge br0] C[VM2 eth0] --> B D[VM3 eth0] --> B B --> E[OVS Database] E --> F[Controller] G[Host eth0] --> H[OVS Bridge br0] I[Management Tools] --> E","title":"Architecture"},{"location":"virtualization/#key-features_3","text":"Standard Linux bridge compatibility Flow-based switching VLAN isolation Bonding and trunking support Remote management protocol","title":"Key Features"},{"location":"virtualization/#quick-commands_4","text":"# Bridge Management ovs-vsctl add-br br0 ovs-vsctl del-br br0 ovs-vsctl list-br # Port Management ovs-vsctl add-port br0 eth0 ovs-vsctl del-port br0 eth0 ovs-vsctl list-ports br0 # Configuration ovs-vsctl show ovs-ofctl show br0 ovs-vsctls show","title":"Quick Commands"},{"location":"virtualization/#source-code_3","text":"Repository : openvswitch/ovs Documentation : openvswitch.org","title":"Source Code"},{"location":"virtualization/#deployment-workflow","text":"","title":"Deployment Workflow"},{"location":"virtualization/#1-hardware-verification","text":"# Check virtualization support lscpu | grep Virtualization egrep -c '(vmx|svm)' /proc/cpuinfo","title":"1. Hardware Verification"},{"location":"virtualization/#2-software-installation","text":"# Install packages apt-get install qemu-kvm libvirt-daemon-system \\ libvirt-clients virt-manager openvswitch # Enable services systemctl enable libvirtd systemctl start libvirtd systemctl enable openvswitch systemctl start openvswitch # Verify virsh version ovs-vsctl show","title":"2. Software Installation"},{"location":"virtualization/#3-network-setup","text":"# Create OVS bridge ovs-vsctl add-br br0 ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 eth1","title":"3. Network Setup"},{"location":"virtualization/#4-create-vm","text":"# Using virt-install virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path=/var/lib/libvirt/images/vm1.qcow2,size=20 \\ --network bridge=br0 # Or using virsh virsh define vm1.xml virsh start vm1","title":"4. Create VM"},{"location":"virtualization/#troubleshooting","text":"For in-depth troubleshooting focused on code behavior and diagnostics, see Deployment .","title":"Troubleshooting"},{"location":"virtualization/#common-issues","text":"Issue Solution VM slow performance Enable KVM with -enable-kvm Network issues Check OVS bridges with ovs-vsctl show Storage issues Use virtio drivers for better I/O Resource constraints Adjust memory/CPU allocation","title":"Common Issues"},{"location":"virtualization/#security-considerations","text":"Use AppArmor/SELinux profiles Use virtio drivers for better isolation Enable seccomp filters Use IOMMU for device passthrough","title":"Security Considerations"},{"location":"virtualization/#source-code-references","text":"Technology Repository Documentation QEMU qemu-project/qemu qemu.org KVM torvalds/linux linux-kvm.org libvirt libvirt/libvirt libvirt.org Open vSwitch openvswitch/ovs openvswitch.org","title":"Source Code References"},{"location":"virtualization/kvm/","text":"Kernel-based virtual machine for Linux providing hardware-assisted virtualization with near-native performance. Architecture graph TB subgraph \"User Space\" A[QEMU Process] B[Guest Userspace] C[QEMU Device Model] end subgraph \"Kernel Space\" D[KVM Module] E[VMCS/VMCB] F[EPT/NPT] G[IOMMU] end subgraph \"Hardware\" H[CPU - VT-x/AMD-V] I[MMU] J[PCI Devices] end A --> D B --> D C --> D D --> E D --> F D --> G E --> H F --> I G --> J style D fill:#c8e6c9 style E fill:#ffecb3 style F fill:#e1f5ff style H fill:#fff3e0 Core Components KVM Module Kernel module providing virtualization support. KVM Device File : # KVM device node /dev/kvm /dev/kvm-vm (after VM creation) KVM IOCTL Operations : | IOCTL | Description | |--------|-------------| | KVM_CREATE_VM | Create new VM | | KVM_CREATE_VCPU | Create virtual CPU | | KVM_RUN | Run VCPU | | KVM_GET_REGS | Get CPU registers | | KVM_SET_REGS | Set CPU registers | | KVM_GET_SREGS | Get special registers | | KVM_SET_SREGS | Set special registers | | KVM_SET_USER_MEMORY_REGION | Set guest memory region | | KVM_GET_IRQCHIP | Get interrupt controller | | KVM_SET_IRQCHIP | Set interrupt controller | | KVM_IRQFD | Setup interrupt file descriptor | | KVM_IOEVENTFD | Setup I/O event file descriptor | VMCS/VMCB Virtual Machine Control Structure (Intel) or VMCB (AMD) managing guest state. VMCS Fields (Intel VT-x) : | Field | Type | Description | |-------|------|-------------| | CR0, CR3, CR4 | Control | Control registers | | RIP, RSP | Guest State | Instruction pointer, stack pointer | | RFLAGS | Guest State | Flags register | | RAX-RCX, RDX-RBX, RSI-RDI, R8-R15 | Guest State | General purpose registers | | CS, DS, ES, FS, GS, SS | Guest State | Segment registers | | GDTR, IDTR | Guest State | Global/Interrupt descriptor tables | | LDTR, TR | Guest State | Local descriptor table, Task register | | EFER | Guest State | Extended feature enable register | | VM_EXIT_CONTROLS | Control | VM exit controls | | VM_ENTRY_CONTROLS | Control | VM entry controls | VMCB Fields (AMD SVM) : | Field | Type | Description | |-------|------|-------------| | CR0, CR2, CR3, CR4 | Control | Control registers | | RIP, RSP | Guest State | Instruction pointer, stack pointer | | RFLAGS | Guest State | Flags register | | EAX-R15 | Guest State | General purpose registers | | CS, DS, ES, FS, GS, SS | Guest State | Segment registers | | GDTR, IDTR | Guest State | Global/Interrupt descriptor tables | | EXITCODE, EXITINFO1, EXITINFO2 | Exit | VM exit information | EPT/NPT Extended Page Tables (Intel) or Nested Page Tables (AMD) for memory virtualization. EPT Operation : graph TB A[Guest Physical Address] --> B[EPT Page Walk] B --> C[EPT Entry 1] C --> D[EPT Entry 2] D --> E[EPT Entry 3] E --> F[Host Physical Address] G[EPT Violation] --> H[VM Exit] H --> I[EPT Violation Handler] style C fill:#c8e6c9 style F fill:#ffecb3 style H fill:#e1f5ff EPT Memory Types : | Type | Description | |------|-------------| | UC | Uncacheable | | WC | Write combining | | WT | Write-through | | WP | Write-protected | | WB | Write-back (default) | NPT Operation : - Similar to EPT for AMD-V - Guest CR3 points to nested page tables - Hardware performs 2-level translation IOMMU I/O Memory Management Unit for device passthrough. IOMMU Operation : graph TB A[Guest Physical Address] --> B[VFIO Driver] B --> C[IOMMU] C --> D[Device IOTLB] D --> E[Host Physical Address] F[DMA Request] --> B G[Device Interrupt] --> B style B fill:#c8e6c9 style C fill:#ffecb3 style E fill:#e1f5ff IOMMU Benefits : - Direct device access for guest - Memory isolation - DMA remapping - Interrupt remapping (IRQ remapping) Key Features Hardware Virtualization Extensions Intel VT-x Features : | Feature | Description | |---------|-------------| | VMX | Virtual Machine Extensions | | EPT | Extended Page Tables | | VPID | Virtual Processor ID | | APICv | Virtual APIC | | Posted Interrupts | Posted interrupt delivery | | Flexible VMX | Flexible VMX control | AMD-V Features : | Feature | Description | |---------|-------------| | SVM | Secure Virtual Machine | | NPT | Nested Page Tables | | AVIC | Advanced Virtual Interrupt Controller | | Virtualized GIF | Virtualized Global Interrupt Flag | | Nested Virtualization | Guest hypervisor support | CPU Virtualization Guest Code Execution : stateDiagram-v2 [*] --> VMEntry VMEntry --> GuestExecution: VMLAUNCH/VMRESUME GuestExecution --> VMExit: VM Exit VMExit --> HostExecution: Handle Exit HostExecution --> VMEntry: VMRESUME VM Exit Reasons : | Category | Reasons | Frequency | |----------|---------|----------| | I/O | Port I/O, MMIO, PCI config | High | | Interrupts | External interrupt, APIC access | High | | Exceptions | Page fault, general protection | Medium | | System Management | CPUID, RDTSC, RDMSR, WRMSR | Low | | Control | CR access, DR access, EPT/NPT | Low | | VMX/SVM | VMXOFF, VMCALL, Triple Fault | Very Low | Memory Management Guest Memory Translation : graph TB A[Guest Virtual Address] --> B[Guest Page Tables] B --> C[Guest Physical Address] C --> D[EPT/NPT] D --> E[Host Physical Address] F[Page Fault] --> B G[EPT Violation] --> D style B fill:#c8e6c9 style D fill:#ffecb3 style E fill:#e1f5ff Memory Management Features : - EPT/NPT for 2-level translation - Huge page support (2MB, 1GB) - Memory ballooning - Page sharing (KSM) - Dirty page tracking I/O Virtualization Virtio I/O : sequenceDiagram participant G as Guest participant V as Virtqueue participant Q as QEMU participant D as Host Device G->>V: Submit I/O V->>Q: Notification Q->>D: Host I/O D->>Q: Completion Q->>V: Interrupt V->>G: Complete style V fill:#c8e6c9 style D fill:#e1f5ff Device Passthrough : sequenceDiagram participant G as Guest participant VFIO as VFIO Driver participant IOMMU as IOMMU participant D as Host Device G->>VFIO: MMIO/PIO Access VFIO->>IOMMU: DMA Request IOMMU->>D: Direct DMA D->>IOMMU: Completion IOMMU->>VFIO: Interrupt VFIO->>G: Inject Interrupt style VFIO fill:#c8e6c9 style IOMMU fill:#ffecb3 style D fill:#e1f5ff Quick Commands Check KVM Support # Check CPU virtualization support lscpu | grep -i virtualization # Check KVM module lsmod | grep kvm # Check /dev/kvm ls -l /dev/kvm Create VM with KVM # Basic KVM VM qemu-system-x86_64 -enable-kvm \\ -name vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 # KVM with host CPU qemu-system-x86_64 -enable-kvm -cpu host \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,format=qcow2 # KVM with NUMA qemu-system-x86_64 -enable-kvm \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -numa node,memdev=mem \\ -object memory-backend-file,id=mem,size=4G,mem-path=/dev/hugepages Device Passthrough # PCI passthrough qemu-system-x86_64 -enable-kvm \\ -device pci-assign,host=01:00.0 # VFIO passthrough qemu-system-x86_64 -enable-kvm \\ -device vfio-pci,host=01:00.0 # Multiple devices qemu-system-x86_64 -enable-kvm \\ -device vfio-pci,host=01:00.0 \\ -device vfio-pci,host=01:00.1 Memory Configuration # Hugepages qemu-system-x86_64 -enable-kvm \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # KSM optimization echo 1 > /sys/kernel/mm/ksm/run echo 1000 > /sys/kernel/mm/ksm/pages_to_scan # Memory ballooning qemu-system-x86_64 -enable-kvm \\ -m 4096 \\ -device virtio-balloon-pci Nifty Behaviors Nested Virtualization # Enable nested virtualization modprobe kvm_intel nested=1 # or modprobe kvm_amd nested=1 # Check nested support cat /sys/module/kvm_intel/parameters/nested # or cat /sys/module/kvm_amd/parameters/nested # Create nested VM qemu-system-x86_64 -enable-kvm -cpu host,+vmx \\ -name nested-vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 Nifty : Run hypervisor inside hypervisor VPID # VPID reduces VM exits qemu-system-x86_64 -enable-kvm -cpu host,+vpid \\ -name vm1 -m 2048 -smp 2 Nifty : Improves VM context switching performance APICv # APICv virtualizes interrupt controller qemu-system-x86_64 -enable-kvm -cpu host,+apicv \\ -name vm1 -m 2048 -smp 2 Nifty : Reduces interrupt delivery overhead Posted Interrupts # Posted interrupts improve interrupt delivery qemu-system-x86_64 -enable-kvm -cpu host,+posted-interrupt \\ -name vm1 -m 2048 -smp 2 Nifty : Zero-latency interrupt delivery Performance Tuning CPU Configuration # Host CPU passthrough qemu-system-x86_64 -enable-kvm -cpu host # CPU features qemu-system-x86_64 -enable-kvm -cpu host,+vmx,+pdpe1gb # CPU pinning qemu-system-x86_64 -enable-kvm \\ -smp 4 \\ -numa node,memdev=mem # CPU topology qemu-system-x86_64 -enable-kvm \\ -smp 4,sockets=2,cores=2,threads=1 Memory Configuration # Hugepages echo 2048 > /proc/sys/vm/nr_hugepages qemu-system-x86_64 -enable-kvm \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # Transparent hugepages echo always > /sys/kernel/mm/transparent_hugepage/enabled # Memory locking qemu-system-x86_64 -enable-kvm -mlock all I/O Configuration # Vhost for virtio devices qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio \\ -net tap,vhost=on # IO thread for storage qemu-system-x86_64 -enable-kvm \\ -object iothread,id=io1 \\ -drive file=disk.qcow2,if=virtio,iothread=io1 # Native AIO qemu-system-x86_64 -enable-kvm \\ -drive file=disk.img,if=virtio,aio=native Network Configuration # Vhost-net qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio \\ -net tap,vhost=on # Multiqueue qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on # Gro/LRO qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio \\ -net tap,gro=on Troubleshooting VM Not Starting # Check KVM module lsmod | grep kvm # Check /dev/kvm ls -l /dev/kvm # Check CPU support lscpu | grep -i virtualization # Check KVM log dmesg | grep -i kvm Performance Issues # Check CPU usage top -H -p $(pidof qemu-system-x86_64) # Check memory free -h cat /proc/meminfo | grep -i huge # Check I/O iostat -x 1 # Check network sar -n DEV 1 Migration Issues # Check migration compatibility qemu-system-x86_64 -cpu host,check # Check NUMA configuration numactl --hardware # Check hugepages cat /proc/meminfo | grep Huge Best Practices Always use KVM for x86 guests on supported hardware Use -cpu host for best performance Enable hugepages for memory-intensive workloads Use virtio devices for optimal I/O performance Configure NUMA for large VMs Enable KSM for memory overcommitment Use vhost-net for network performance Pin VCPUs for CPU-intensive workloads Monitor VM performance regularly Test nested virtualization if needed Source Code Location in Linux kernel : virt/kvm/ Repository : https://github.com/torvalds/linux/tree/master/virt/kvm Documentation : https://www.linux-kvm.org/ Key Source Locations Component Location Description Main module virt/kvm/kvm_main.c Main KVM module Intel VMX arch/x86/kvm/vmx.c Intel VT-x implementation AMD SVM arch/x86/kvm/svm.c AMD-V implementation MMU arch/x86/kvm/mmu.c Memory management EPT arch/x86/kvm/vmx.c Extended Page Tables IOMMU virt/kvm/vfio.c VFIO passthrough Interrupts arch/x86/kvm/irq.c Interrupt handling PIC arch/x86/kvm/i8259.c Legacy PIC IOAPIC arch/x86/kvm/ioapic.c I/O APIC LAPIC arch/x86/kvm/lapic.c Local APIC","title":"KVM"},{"location":"virtualization/kvm/#architecture","text":"graph TB subgraph \"User Space\" A[QEMU Process] B[Guest Userspace] C[QEMU Device Model] end subgraph \"Kernel Space\" D[KVM Module] E[VMCS/VMCB] F[EPT/NPT] G[IOMMU] end subgraph \"Hardware\" H[CPU - VT-x/AMD-V] I[MMU] J[PCI Devices] end A --> D B --> D C --> D D --> E D --> F D --> G E --> H F --> I G --> J style D fill:#c8e6c9 style E fill:#ffecb3 style F fill:#e1f5ff style H fill:#fff3e0","title":"Architecture"},{"location":"virtualization/kvm/#core-components","text":"","title":"Core Components"},{"location":"virtualization/kvm/#kvm-module","text":"Kernel module providing virtualization support. KVM Device File : # KVM device node /dev/kvm /dev/kvm-vm (after VM creation) KVM IOCTL Operations : | IOCTL | Description | |--------|-------------| | KVM_CREATE_VM | Create new VM | | KVM_CREATE_VCPU | Create virtual CPU | | KVM_RUN | Run VCPU | | KVM_GET_REGS | Get CPU registers | | KVM_SET_REGS | Set CPU registers | | KVM_GET_SREGS | Get special registers | | KVM_SET_SREGS | Set special registers | | KVM_SET_USER_MEMORY_REGION | Set guest memory region | | KVM_GET_IRQCHIP | Get interrupt controller | | KVM_SET_IRQCHIP | Set interrupt controller | | KVM_IRQFD | Setup interrupt file descriptor | | KVM_IOEVENTFD | Setup I/O event file descriptor |","title":"KVM Module"},{"location":"virtualization/kvm/#vmcsvmcb","text":"Virtual Machine Control Structure (Intel) or VMCB (AMD) managing guest state. VMCS Fields (Intel VT-x) : | Field | Type | Description | |-------|------|-------------| | CR0, CR3, CR4 | Control | Control registers | | RIP, RSP | Guest State | Instruction pointer, stack pointer | | RFLAGS | Guest State | Flags register | | RAX-RCX, RDX-RBX, RSI-RDI, R8-R15 | Guest State | General purpose registers | | CS, DS, ES, FS, GS, SS | Guest State | Segment registers | | GDTR, IDTR | Guest State | Global/Interrupt descriptor tables | | LDTR, TR | Guest State | Local descriptor table, Task register | | EFER | Guest State | Extended feature enable register | | VM_EXIT_CONTROLS | Control | VM exit controls | | VM_ENTRY_CONTROLS | Control | VM entry controls | VMCB Fields (AMD SVM) : | Field | Type | Description | |-------|------|-------------| | CR0, CR2, CR3, CR4 | Control | Control registers | | RIP, RSP | Guest State | Instruction pointer, stack pointer | | RFLAGS | Guest State | Flags register | | EAX-R15 | Guest State | General purpose registers | | CS, DS, ES, FS, GS, SS | Guest State | Segment registers | | GDTR, IDTR | Guest State | Global/Interrupt descriptor tables | | EXITCODE, EXITINFO1, EXITINFO2 | Exit | VM exit information |","title":"VMCS/VMCB"},{"location":"virtualization/kvm/#eptnpt","text":"Extended Page Tables (Intel) or Nested Page Tables (AMD) for memory virtualization. EPT Operation : graph TB A[Guest Physical Address] --> B[EPT Page Walk] B --> C[EPT Entry 1] C --> D[EPT Entry 2] D --> E[EPT Entry 3] E --> F[Host Physical Address] G[EPT Violation] --> H[VM Exit] H --> I[EPT Violation Handler] style C fill:#c8e6c9 style F fill:#ffecb3 style H fill:#e1f5ff EPT Memory Types : | Type | Description | |------|-------------| | UC | Uncacheable | | WC | Write combining | | WT | Write-through | | WP | Write-protected | | WB | Write-back (default) | NPT Operation : - Similar to EPT for AMD-V - Guest CR3 points to nested page tables - Hardware performs 2-level translation","title":"EPT/NPT"},{"location":"virtualization/kvm/#iommu","text":"I/O Memory Management Unit for device passthrough. IOMMU Operation : graph TB A[Guest Physical Address] --> B[VFIO Driver] B --> C[IOMMU] C --> D[Device IOTLB] D --> E[Host Physical Address] F[DMA Request] --> B G[Device Interrupt] --> B style B fill:#c8e6c9 style C fill:#ffecb3 style E fill:#e1f5ff IOMMU Benefits : - Direct device access for guest - Memory isolation - DMA remapping - Interrupt remapping (IRQ remapping)","title":"IOMMU"},{"location":"virtualization/kvm/#key-features","text":"","title":"Key Features"},{"location":"virtualization/kvm/#hardware-virtualization-extensions","text":"Intel VT-x Features : | Feature | Description | |---------|-------------| | VMX | Virtual Machine Extensions | | EPT | Extended Page Tables | | VPID | Virtual Processor ID | | APICv | Virtual APIC | | Posted Interrupts | Posted interrupt delivery | | Flexible VMX | Flexible VMX control | AMD-V Features : | Feature | Description | |---------|-------------| | SVM | Secure Virtual Machine | | NPT | Nested Page Tables | | AVIC | Advanced Virtual Interrupt Controller | | Virtualized GIF | Virtualized Global Interrupt Flag | | Nested Virtualization | Guest hypervisor support |","title":"Hardware Virtualization Extensions"},{"location":"virtualization/kvm/#cpu-virtualization","text":"Guest Code Execution : stateDiagram-v2 [*] --> VMEntry VMEntry --> GuestExecution: VMLAUNCH/VMRESUME GuestExecution --> VMExit: VM Exit VMExit --> HostExecution: Handle Exit HostExecution --> VMEntry: VMRESUME VM Exit Reasons : | Category | Reasons | Frequency | |----------|---------|----------| | I/O | Port I/O, MMIO, PCI config | High | | Interrupts | External interrupt, APIC access | High | | Exceptions | Page fault, general protection | Medium | | System Management | CPUID, RDTSC, RDMSR, WRMSR | Low | | Control | CR access, DR access, EPT/NPT | Low | | VMX/SVM | VMXOFF, VMCALL, Triple Fault | Very Low |","title":"CPU Virtualization"},{"location":"virtualization/kvm/#memory-management","text":"Guest Memory Translation : graph TB A[Guest Virtual Address] --> B[Guest Page Tables] B --> C[Guest Physical Address] C --> D[EPT/NPT] D --> E[Host Physical Address] F[Page Fault] --> B G[EPT Violation] --> D style B fill:#c8e6c9 style D fill:#ffecb3 style E fill:#e1f5ff Memory Management Features : - EPT/NPT for 2-level translation - Huge page support (2MB, 1GB) - Memory ballooning - Page sharing (KSM) - Dirty page tracking","title":"Memory Management"},{"location":"virtualization/kvm/#io-virtualization","text":"Virtio I/O : sequenceDiagram participant G as Guest participant V as Virtqueue participant Q as QEMU participant D as Host Device G->>V: Submit I/O V->>Q: Notification Q->>D: Host I/O D->>Q: Completion Q->>V: Interrupt V->>G: Complete style V fill:#c8e6c9 style D fill:#e1f5ff Device Passthrough : sequenceDiagram participant G as Guest participant VFIO as VFIO Driver participant IOMMU as IOMMU participant D as Host Device G->>VFIO: MMIO/PIO Access VFIO->>IOMMU: DMA Request IOMMU->>D: Direct DMA D->>IOMMU: Completion IOMMU->>VFIO: Interrupt VFIO->>G: Inject Interrupt style VFIO fill:#c8e6c9 style IOMMU fill:#ffecb3 style D fill:#e1f5ff","title":"I/O Virtualization"},{"location":"virtualization/kvm/#quick-commands","text":"","title":"Quick Commands"},{"location":"virtualization/kvm/#check-kvm-support","text":"# Check CPU virtualization support lscpu | grep -i virtualization # Check KVM module lsmod | grep kvm # Check /dev/kvm ls -l /dev/kvm","title":"Check KVM Support"},{"location":"virtualization/kvm/#create-vm-with-kvm","text":"# Basic KVM VM qemu-system-x86_64 -enable-kvm \\ -name vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 # KVM with host CPU qemu-system-x86_64 -enable-kvm -cpu host \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,format=qcow2 # KVM with NUMA qemu-system-x86_64 -enable-kvm \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -numa node,memdev=mem \\ -object memory-backend-file,id=mem,size=4G,mem-path=/dev/hugepages","title":"Create VM with KVM"},{"location":"virtualization/kvm/#device-passthrough","text":"# PCI passthrough qemu-system-x86_64 -enable-kvm \\ -device pci-assign,host=01:00.0 # VFIO passthrough qemu-system-x86_64 -enable-kvm \\ -device vfio-pci,host=01:00.0 # Multiple devices qemu-system-x86_64 -enable-kvm \\ -device vfio-pci,host=01:00.0 \\ -device vfio-pci,host=01:00.1","title":"Device Passthrough"},{"location":"virtualization/kvm/#memory-configuration","text":"# Hugepages qemu-system-x86_64 -enable-kvm \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # KSM optimization echo 1 > /sys/kernel/mm/ksm/run echo 1000 > /sys/kernel/mm/ksm/pages_to_scan # Memory ballooning qemu-system-x86_64 -enable-kvm \\ -m 4096 \\ -device virtio-balloon-pci","title":"Memory Configuration"},{"location":"virtualization/kvm/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"virtualization/kvm/#nested-virtualization","text":"# Enable nested virtualization modprobe kvm_intel nested=1 # or modprobe kvm_amd nested=1 # Check nested support cat /sys/module/kvm_intel/parameters/nested # or cat /sys/module/kvm_amd/parameters/nested # Create nested VM qemu-system-x86_64 -enable-kvm -cpu host,+vmx \\ -name nested-vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 Nifty : Run hypervisor inside hypervisor","title":"Nested Virtualization"},{"location":"virtualization/kvm/#vpid","text":"# VPID reduces VM exits qemu-system-x86_64 -enable-kvm -cpu host,+vpid \\ -name vm1 -m 2048 -smp 2 Nifty : Improves VM context switching performance","title":"VPID"},{"location":"virtualization/kvm/#apicv","text":"# APICv virtualizes interrupt controller qemu-system-x86_64 -enable-kvm -cpu host,+apicv \\ -name vm1 -m 2048 -smp 2 Nifty : Reduces interrupt delivery overhead","title":"APICv"},{"location":"virtualization/kvm/#posted-interrupts","text":"# Posted interrupts improve interrupt delivery qemu-system-x86_64 -enable-kvm -cpu host,+posted-interrupt \\ -name vm1 -m 2048 -smp 2 Nifty : Zero-latency interrupt delivery","title":"Posted Interrupts"},{"location":"virtualization/kvm/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"virtualization/kvm/#cpu-configuration","text":"# Host CPU passthrough qemu-system-x86_64 -enable-kvm -cpu host # CPU features qemu-system-x86_64 -enable-kvm -cpu host,+vmx,+pdpe1gb # CPU pinning qemu-system-x86_64 -enable-kvm \\ -smp 4 \\ -numa node,memdev=mem # CPU topology qemu-system-x86_64 -enable-kvm \\ -smp 4,sockets=2,cores=2,threads=1","title":"CPU Configuration"},{"location":"virtualization/kvm/#memory-configuration_1","text":"# Hugepages echo 2048 > /proc/sys/vm/nr_hugepages qemu-system-x86_64 -enable-kvm \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # Transparent hugepages echo always > /sys/kernel/mm/transparent_hugepage/enabled # Memory locking qemu-system-x86_64 -enable-kvm -mlock all","title":"Memory Configuration"},{"location":"virtualization/kvm/#io-configuration","text":"# Vhost for virtio devices qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio \\ -net tap,vhost=on # IO thread for storage qemu-system-x86_64 -enable-kvm \\ -object iothread,id=io1 \\ -drive file=disk.qcow2,if=virtio,iothread=io1 # Native AIO qemu-system-x86_64 -enable-kvm \\ -drive file=disk.img,if=virtio,aio=native","title":"I/O Configuration"},{"location":"virtualization/kvm/#network-configuration","text":"# Vhost-net qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio \\ -net tap,vhost=on # Multiqueue qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on # Gro/LRO qemu-system-x86_64 -enable-kvm \\ -net nic,model=virtio \\ -net tap,gro=on","title":"Network Configuration"},{"location":"virtualization/kvm/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"virtualization/kvm/#vm-not-starting","text":"# Check KVM module lsmod | grep kvm # Check /dev/kvm ls -l /dev/kvm # Check CPU support lscpu | grep -i virtualization # Check KVM log dmesg | grep -i kvm","title":"VM Not Starting"},{"location":"virtualization/kvm/#performance-issues","text":"# Check CPU usage top -H -p $(pidof qemu-system-x86_64) # Check memory free -h cat /proc/meminfo | grep -i huge # Check I/O iostat -x 1 # Check network sar -n DEV 1","title":"Performance Issues"},{"location":"virtualization/kvm/#migration-issues","text":"# Check migration compatibility qemu-system-x86_64 -cpu host,check # Check NUMA configuration numactl --hardware # Check hugepages cat /proc/meminfo | grep Huge","title":"Migration Issues"},{"location":"virtualization/kvm/#best-practices","text":"Always use KVM for x86 guests on supported hardware Use -cpu host for best performance Enable hugepages for memory-intensive workloads Use virtio devices for optimal I/O performance Configure NUMA for large VMs Enable KSM for memory overcommitment Use vhost-net for network performance Pin VCPUs for CPU-intensive workloads Monitor VM performance regularly Test nested virtualization if needed","title":"Best Practices"},{"location":"virtualization/kvm/#source-code","text":"Location in Linux kernel : virt/kvm/ Repository : https://github.com/torvalds/linux/tree/master/virt/kvm Documentation : https://www.linux-kvm.org/","title":"Source Code"},{"location":"virtualization/kvm/#key-source-locations","text":"Component Location Description Main module virt/kvm/kvm_main.c Main KVM module Intel VMX arch/x86/kvm/vmx.c Intel VT-x implementation AMD SVM arch/x86/kvm/svm.c AMD-V implementation MMU arch/x86/kvm/mmu.c Memory management EPT arch/x86/kvm/vmx.c Extended Page Tables IOMMU virt/kvm/vfio.c VFIO passthrough Interrupts arch/x86/kvm/irq.c Interrupt handling PIC arch/x86/kvm/i8259.c Legacy PIC IOAPIC arch/x86/kvm/ioapic.c I/O APIC LAPIC arch/x86/kvm/lapic.c Local APIC","title":"Key Source Locations"},{"location":"virtualization/libvirt/","text":"Virtualization API and daemon for managing virtualization capabilities with unified hypervisor support. Architecture graph TB subgraph \"Hypervisors\" A[QEMU/KVM] B[LXC] C[Xen] D[VMware ESX] E[Hyper-V] end subgraph \"libvirtd Daemon\" F[libvirtd Main] G[QEMU Driver] H[LXC Driver] I[Xen Driver] J[Network Driver] K[Storage Driver] L[Node Driver] M[Secret Driver] end subgraph \"API Layer\" N[libvirt API] O[QEMU Protocol] P[Network Protocol] end subgraph \"Clients\" Q[virsh CLI] R[virt-manager GUI] S[Python Bindings] T[C Bindings] U[Java Bindings] end F --> A F --> B F --> C F --> D F --> E F --> G F --> H F --> I F --> J F --> K F --> L F --> M G --> A H --> B I --> C F --> N F --> O F --> P Q --> N R --> N S --> N T --> N U --> N style F fill:#c8e6c9 style N fill:#ffecb3 style A fill:#e1f5ff style Q fill:#fff3e0 Core Components libvirtd Daemon Main daemon managing virtualization resources. libvirtd Responsibilities : - Manage VM lifecycle - Handle client connections - Execute VM operations - Manage network and storage - Provide security and authentication libvirtd Configuration : <!-- /etc/libvirt/libvirtd.conf --> listen_tls = 1 listen_tcp = 0 auth_unix_ro = \"none\" auth_unix_rw = \"none\" auth_tls = \"none\" log_level = 3 log_outputs = \"3:file:/var/log/libvirt/libvirtd.log\" libvirtd Services : | Service | Description | |---------|-------------| | libvirtd | Main daemon | | virtlogd | Log manager | | virtlockd | Lock manager | | virtqemud | QEMU driver daemon | Drivers libvirt supports multiple hypervisor drivers. Driver Types : | Driver | Hypervisor | Status | |--------|------------|--------| | qemu | QEMU/KVM | Stable | | lxc | Linux Containers | Stable | | xen | Xen | Stable | | vmware | VMware ESX | Stable | | vbox | VirtualBox | Experimental | | bhyve | BSD Hypervisor | Experimental | | vz | Virtuozzo | Stable | Driver Architecture : graph TB A[libvirt API] --> B[Driver Manager] B --> C[QEMU Driver] B --> D[LXC Driver] B --> E[Xen Driver] C --> F[QEMU Process] D --> G[LXC Container] E --> H[Xen Domain] style B fill:#c8e6c9 style C fill:#ffecb3 style D fill:#e1f5ff Domain Configuration XML Domain Structure Complete VM configuration in XML format. Basic Domain XML : <domain type='kvm'> <name>vm1</name> <memory unit='KiB'>2097152</memory> <vcpu placement='static'>2</vcpu> <os> <type arch='x86_64' machine='pc-i440fx-2.9'>hvm</type> <boot dev='hd'/> </os> <devices> <emulator>/usr/bin/qemu-system-x86_64</emulator> <disk type='file' device='disk'> <driver name='qemu' type='qcow2'/> <source file='/var/lib/libvirt/images/vm1.qcow2'/> <target dev='vda' bus='virtio'/> </disk> <interface type='network'> <mac address='52:54:00:71:b1:b6'/> <source network='default'/> <model type='virtio'/> </interface> </devices> </domain> CPU Configuration CPU Features : <cpu mode='host-passthrough'> <feature policy='require' name='vmx'/> <topology sockets='2' cores='2' threads='1'/> </cpu> CPU Modes : | Mode | Description | |------|-------------| | host-passthrough | Pass all CPU features | | host-model | Emulate host CPU model | | custom | Custom CPU configuration | CPU Features : | Feature | Description | |---------|-------------| | vmx | Intel VT-x | | svm | AMD-V | | pdpe1gb | 1GB pages | | avx | Advanced Vector Extensions | | avx2 | Advanced Vector Extensions 2 | Memory Configuration Basic Memory : <memory unit='KiB'>2097152</memory> <currentMemory unit='KiB'>2097152</currentMemory> NUMA Memory : <cpu mode='host-passthrough' numa='1'> <numa> <cell id='0' cpus='0-1' memory='2097152' unit='KiB'/> </numa> </cpu> <memoryBacking> <hugepages> <page size='2048' unit='KiB' nodeset='0'/> </hugepages> </memoryBacking> Memory Locking : <memoryBacking> <locked/> <hugepages/> </memoryBacking> Storage Configuration Disk Configuration : <disk type='file' device='disk'> <driver name='qemu' type='qcow2' cache='writeback'/> <source file='/var/lib/libvirt/images/vm1.qcow2'/> <target dev='vda' bus='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/> </disk> Disk Types : | Type | Description | Performance | |------|-------------|------------| | qcow2 | QEMU copy-on-write | Good | | raw | Raw image | Excellent | | qed | QED format | Good | | vmdk | VMware format | Good | | vdi | VirtualBox format | Good | Disk Formats : <!-- Qcow2 --> <disk type='file' device='disk'> <driver name='qemu' type='qcow2'/> <source file='/var/lib/libvirt/images/vm1.qcow2'/> </disk> <!-- Raw --> <disk type='block' device='disk'> <driver name='qemu' type='raw' cache='none'/> <source dev='/dev/vg0/vm1'/> </disk> <!-- iSCSI --> <disk type='block' device='disk'> <driver name='qemu' type='raw'/> <source dev='/dev/disk/by-path/ip-192.168.1.100:3260-iscsi-iqn.2026-01.com.example:storage.target1-lun-1'/> </disk> <!-- RBD --> <disk type='network' device='disk'> <driver name='qemu' type='raw'/> <source protocol='rbd' name='pool/image'> <host name='ceph-mon.example.com' port='6789'/> </source> </disk> Network Configuration Bridge Network : <interface type='bridge'> <mac address='52:54:00:71:b1:b6'/> <source bridge='br0'/> <model type='virtio'/> <driver name='vhost'/> </interface> Virtual Network : <interface type='network'> <mac address='52:54:00:71:b1:b6'/> <source network='default'/> <model type='virtio'/> <driver name='vhost'/> </interface> Network Types : | Type | Description | Use Case | |------|-------------|----------| | bridge | Physical bridge | Production networking | | network | Virtual network | Isolated VMs | | direct | SR-IOV | High performance | | macvtap | MAC VLAN | Direct access | | tap | TAP device | Custom networking | Network Features : <interface type='bridge'> <mac address='52:54:00:71:b1:b6'/> <source bridge='br0'/> <model type='virtio'/> <driver name='vhost' queues='4'/> <mtu size='9000'/> <bandwidth> <inbound average='1000' peak='5000' burst='1024'/> <outbound average='1000' peak='5000' burst='1024'/> </bandwidth> <link state='up'/> </interface> Quick Commands Domain Management # List VMs virsh list virsh list --all virsh list --state-running virsh list --state-shutoff # Define VM virsh define vm1.xml # Undefine VM virsh undefine vm1 # Start VM virsh start vm1 # Stop VM virsh shutdown vm1 # Force stop VM virsh destroy vm1 # Restart VM virsh reboot vm1 Domain Configuration # Dump XML virsh dumpxml vm1 # Edit XML virsh edit vm1 # Get/set XML elements virsh dumpxml vm1 | grep memory virsh dumpxml vm1 | xpath //memory # Autostart VM virsh autostart vm1 virsh autostart --disable vm1 Domain Monitoring # VM status virsh dominfo vm1 # VCPU status virsh vcpuinfo vm1 # Memory statistics virsh dommemstat vm1 # Block statistics virsh domblklist vm1 virsh domblkinfo vm1 vda # Network statistics virsh domiflist vm1 virsh domifstat vm1 vnet0 Migration # Live migration virsh migrate --live vm1 qemu+ssh://dest/system # Migration with compression virsh migrate --live --compress vm1 qemu+ssh://dest/system # Migration with tunnel virsh migrate --live --tunnelled vm1 qemu+ssh://dest/system # Cold migration virsh migrate vm1 qemu+ssh://dest/system Network Configuration Virtual Networks Define Network : <network> <name>default</name> <forward mode='nat'/> <bridge name='virbr0' stp='on' delay='0'/> <ip address='192.168.122.1' netmask='255.255.255.0'> <dhcp> <range start='192.168.122.2' end='192.168.122.254'/> </dhcp> </ip> </network> Create Network : # Define network virsh net-define network.xml # Start network virsh net-start default # Autostart network virsh net-autostart default # List networks virsh net-list --all # Delete network virsh net-destroy default virsh net-undefine default Network Forward Modes : | Mode | Description | |------|-------------| | nat | Network address translation | | route | Simple routing | | bridge | Direct bridge | | open | No forwarding | | hostdev | PCI passthrough | | macvtap | MAC VLAN | Storage Configuration Storage Pools Define Storage Pool : <pool type='dir'> <name>default</name> <target> <path>/var/lib/libvirt/images</path> </target> </pool> Create Storage Pool : # Define pool virsh pool-define pool.xml # Build pool virsh pool-build default # Start pool virsh pool-start default # Autostart pool virsh pool-autostart default # List pools virsh pool-list --all Storage Pool Types : | Type | Description | Use Case | |------|-------------|----------| | dir | Directory | Simple storage | | fs | Filesystem | Local filesystem | | logical | LVM | Volume management | | iscsi | iSCSI | SAN storage | | netfs | NFS | Network file system | | rbd | Ceph RBD | Distributed storage | | disk | Physical disk | Disk partition | | zfs | ZFS | ZFS storage | Storage Volumes Create Volume : # Create volume virsh vol-create-as default vm1.qcow2 20G --format qcow2 # Upload image virsh vol-upload default vm1.qcow2 disk.qcow2 # Download image virsh vol-download default vm1.qcow2 disk.qcow2 # Clone volume virsh vol-clone default vm1.qcow2 vm2.qcow2 # Delete volume virsh vol-delete default vm1.qcow2 API Bindings Python API #!/usr/bin/env python3 import libvirt # Connect to libvirt conn = libvirt.open('qemu:///system') # Get domain dom = conn.lookupByName('vm1') # Start domain dom.create() # Get domain info info = dom.info() print(f\"State: {info[0]}\") print(f\"Memory: {info[1]}\") print(f\"CPUs: {info[3]}\") # Close connection conn.close() C API #include <libvirt/libvirt.h> int main() { virConnectPtr conn; virDomainPtr dom; // Connect to libvirt conn = virConnectOpen(\"qemu:///system\"); if (conn == NULL) { fprintf(stderr, \"Failed to open connection to qemu:///system\\n\"); return 1; } // Get domain dom = virDomainLookupByName(conn, \"vm1\"); if (dom == NULL) { fprintf(stderr, \"Failed to find domain vm1\\n\"); virConnectClose(conn); return 1; } // Start domain virDomainCreate(dom); // Cleanup virDomainFree(dom); virConnectClose(conn); return 0; } Security Authentication SASL Authentication : <!-- /etc/libvirt/libvirtd.conf --> auth_unix_ro = \"sasl\" auth_unix_rw = \"sasl\" TLS/SSL : <!-- /etc/libvirt/libvirtd.conf --> listen_tls = 1 listen_tcp = 0 <!-- Certificate locations --> cert_file = \"/etc/pki/libvirt/servercert.pem\" key_file = \"/etc/pki/libvirt/private/serverkey.pem\" ca_file = \"/etc/pki/CA/cacert.pem\" AppArmor/SELinux AppArmor Profile : # Check AppArmor status aa-status # View AppArmor profile cat /etc/apparmor.d/usr.sbin.libvirtd SELinux Context : # Check SELinux status sestatus # Set SELinux context chcon -R --reference=/var/lib/libvirt/images /path/to/new/images Troubleshooting Connection Issues # Check libvirtd status systemctl status libvirtd # Check logs journalctl -u libvirtd -f # Check configuration virsh version virsh uri VM Issues # Check VM logs tail -f /var/log/libvirt/qemu/vm1.log # Check libvirt logs tail -f /var/log/libvirt/libvirtd.log # Check VM configuration virsh dumpxml vm1 virsh dominfo vm1 Performance Issues # Check CPU usage top -H -p $(pidof qemu-kvm) # Check memory usage free -h # Check I/O iostat -x 1 # Check network sar -n DEV 1 Best Practices Use libvirt XML for VM configuration Enable virtio devices for performance Use storage pools for volume management Configure networks properly Enable live migration for high availability Set up monitoring and alerting Use proper security (TLS, SELinux) Test backup/restore procedures Document VM configurations Regular updates of libvirt and QEMU Source Code Repository : https://gitlab.com/libvirt/libvirt Documentation : https://libvirt.org/docs/ Key Source Locations Component Location Description libvirtd src/libvirtd.c Main daemon QEMU Driver src/qemu/ QEMU/KVM driver LXC Driver src/lxc/ LXC driver Xen Driver src/xen/ Xen driver Network src/network/ Network management Storage src/storage/ Storage management API src/libvirt.c Public API XML src/conf/ XML parsing Documentation Links libvirt API Reference libvirt XML Format libvirt Drivers libvirt Security","title":"libvirt"},{"location":"virtualization/libvirt/#architecture","text":"graph TB subgraph \"Hypervisors\" A[QEMU/KVM] B[LXC] C[Xen] D[VMware ESX] E[Hyper-V] end subgraph \"libvirtd Daemon\" F[libvirtd Main] G[QEMU Driver] H[LXC Driver] I[Xen Driver] J[Network Driver] K[Storage Driver] L[Node Driver] M[Secret Driver] end subgraph \"API Layer\" N[libvirt API] O[QEMU Protocol] P[Network Protocol] end subgraph \"Clients\" Q[virsh CLI] R[virt-manager GUI] S[Python Bindings] T[C Bindings] U[Java Bindings] end F --> A F --> B F --> C F --> D F --> E F --> G F --> H F --> I F --> J F --> K F --> L F --> M G --> A H --> B I --> C F --> N F --> O F --> P Q --> N R --> N S --> N T --> N U --> N style F fill:#c8e6c9 style N fill:#ffecb3 style A fill:#e1f5ff style Q fill:#fff3e0","title":"Architecture"},{"location":"virtualization/libvirt/#core-components","text":"","title":"Core Components"},{"location":"virtualization/libvirt/#libvirtd-daemon","text":"Main daemon managing virtualization resources. libvirtd Responsibilities : - Manage VM lifecycle - Handle client connections - Execute VM operations - Manage network and storage - Provide security and authentication libvirtd Configuration : <!-- /etc/libvirt/libvirtd.conf --> listen_tls = 1 listen_tcp = 0 auth_unix_ro = \"none\" auth_unix_rw = \"none\" auth_tls = \"none\" log_level = 3 log_outputs = \"3:file:/var/log/libvirt/libvirtd.log\" libvirtd Services : | Service | Description | |---------|-------------| | libvirtd | Main daemon | | virtlogd | Log manager | | virtlockd | Lock manager | | virtqemud | QEMU driver daemon |","title":"libvirtd Daemon"},{"location":"virtualization/libvirt/#drivers","text":"libvirt supports multiple hypervisor drivers. Driver Types : | Driver | Hypervisor | Status | |--------|------------|--------| | qemu | QEMU/KVM | Stable | | lxc | Linux Containers | Stable | | xen | Xen | Stable | | vmware | VMware ESX | Stable | | vbox | VirtualBox | Experimental | | bhyve | BSD Hypervisor | Experimental | | vz | Virtuozzo | Stable | Driver Architecture : graph TB A[libvirt API] --> B[Driver Manager] B --> C[QEMU Driver] B --> D[LXC Driver] B --> E[Xen Driver] C --> F[QEMU Process] D --> G[LXC Container] E --> H[Xen Domain] style B fill:#c8e6c9 style C fill:#ffecb3 style D fill:#e1f5ff","title":"Drivers"},{"location":"virtualization/libvirt/#domain-configuration","text":"","title":"Domain Configuration"},{"location":"virtualization/libvirt/#xml-domain-structure","text":"Complete VM configuration in XML format. Basic Domain XML : <domain type='kvm'> <name>vm1</name> <memory unit='KiB'>2097152</memory> <vcpu placement='static'>2</vcpu> <os> <type arch='x86_64' machine='pc-i440fx-2.9'>hvm</type> <boot dev='hd'/> </os> <devices> <emulator>/usr/bin/qemu-system-x86_64</emulator> <disk type='file' device='disk'> <driver name='qemu' type='qcow2'/> <source file='/var/lib/libvirt/images/vm1.qcow2'/> <target dev='vda' bus='virtio'/> </disk> <interface type='network'> <mac address='52:54:00:71:b1:b6'/> <source network='default'/> <model type='virtio'/> </interface> </devices> </domain>","title":"XML Domain Structure"},{"location":"virtualization/libvirt/#cpu-configuration","text":"CPU Features : <cpu mode='host-passthrough'> <feature policy='require' name='vmx'/> <topology sockets='2' cores='2' threads='1'/> </cpu> CPU Modes : | Mode | Description | |------|-------------| | host-passthrough | Pass all CPU features | | host-model | Emulate host CPU model | | custom | Custom CPU configuration | CPU Features : | Feature | Description | |---------|-------------| | vmx | Intel VT-x | | svm | AMD-V | | pdpe1gb | 1GB pages | | avx | Advanced Vector Extensions | | avx2 | Advanced Vector Extensions 2 |","title":"CPU Configuration"},{"location":"virtualization/libvirt/#memory-configuration","text":"Basic Memory : <memory unit='KiB'>2097152</memory> <currentMemory unit='KiB'>2097152</currentMemory> NUMA Memory : <cpu mode='host-passthrough' numa='1'> <numa> <cell id='0' cpus='0-1' memory='2097152' unit='KiB'/> </numa> </cpu> <memoryBacking> <hugepages> <page size='2048' unit='KiB' nodeset='0'/> </hugepages> </memoryBacking> Memory Locking : <memoryBacking> <locked/> <hugepages/> </memoryBacking>","title":"Memory Configuration"},{"location":"virtualization/libvirt/#storage-configuration","text":"Disk Configuration : <disk type='file' device='disk'> <driver name='qemu' type='qcow2' cache='writeback'/> <source file='/var/lib/libvirt/images/vm1.qcow2'/> <target dev='vda' bus='virtio'/> <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/> </disk> Disk Types : | Type | Description | Performance | |------|-------------|------------| | qcow2 | QEMU copy-on-write | Good | | raw | Raw image | Excellent | | qed | QED format | Good | | vmdk | VMware format | Good | | vdi | VirtualBox format | Good | Disk Formats : <!-- Qcow2 --> <disk type='file' device='disk'> <driver name='qemu' type='qcow2'/> <source file='/var/lib/libvirt/images/vm1.qcow2'/> </disk> <!-- Raw --> <disk type='block' device='disk'> <driver name='qemu' type='raw' cache='none'/> <source dev='/dev/vg0/vm1'/> </disk> <!-- iSCSI --> <disk type='block' device='disk'> <driver name='qemu' type='raw'/> <source dev='/dev/disk/by-path/ip-192.168.1.100:3260-iscsi-iqn.2026-01.com.example:storage.target1-lun-1'/> </disk> <!-- RBD --> <disk type='network' device='disk'> <driver name='qemu' type='raw'/> <source protocol='rbd' name='pool/image'> <host name='ceph-mon.example.com' port='6789'/> </source> </disk>","title":"Storage Configuration"},{"location":"virtualization/libvirt/#network-configuration","text":"Bridge Network : <interface type='bridge'> <mac address='52:54:00:71:b1:b6'/> <source bridge='br0'/> <model type='virtio'/> <driver name='vhost'/> </interface> Virtual Network : <interface type='network'> <mac address='52:54:00:71:b1:b6'/> <source network='default'/> <model type='virtio'/> <driver name='vhost'/> </interface> Network Types : | Type | Description | Use Case | |------|-------------|----------| | bridge | Physical bridge | Production networking | | network | Virtual network | Isolated VMs | | direct | SR-IOV | High performance | | macvtap | MAC VLAN | Direct access | | tap | TAP device | Custom networking | Network Features : <interface type='bridge'> <mac address='52:54:00:71:b1:b6'/> <source bridge='br0'/> <model type='virtio'/> <driver name='vhost' queues='4'/> <mtu size='9000'/> <bandwidth> <inbound average='1000' peak='5000' burst='1024'/> <outbound average='1000' peak='5000' burst='1024'/> </bandwidth> <link state='up'/> </interface>","title":"Network Configuration"},{"location":"virtualization/libvirt/#quick-commands","text":"","title":"Quick Commands"},{"location":"virtualization/libvirt/#domain-management","text":"# List VMs virsh list virsh list --all virsh list --state-running virsh list --state-shutoff # Define VM virsh define vm1.xml # Undefine VM virsh undefine vm1 # Start VM virsh start vm1 # Stop VM virsh shutdown vm1 # Force stop VM virsh destroy vm1 # Restart VM virsh reboot vm1","title":"Domain Management"},{"location":"virtualization/libvirt/#domain-configuration_1","text":"# Dump XML virsh dumpxml vm1 # Edit XML virsh edit vm1 # Get/set XML elements virsh dumpxml vm1 | grep memory virsh dumpxml vm1 | xpath //memory # Autostart VM virsh autostart vm1 virsh autostart --disable vm1","title":"Domain Configuration"},{"location":"virtualization/libvirt/#domain-monitoring","text":"# VM status virsh dominfo vm1 # VCPU status virsh vcpuinfo vm1 # Memory statistics virsh dommemstat vm1 # Block statistics virsh domblklist vm1 virsh domblkinfo vm1 vda # Network statistics virsh domiflist vm1 virsh domifstat vm1 vnet0","title":"Domain Monitoring"},{"location":"virtualization/libvirt/#migration","text":"# Live migration virsh migrate --live vm1 qemu+ssh://dest/system # Migration with compression virsh migrate --live --compress vm1 qemu+ssh://dest/system # Migration with tunnel virsh migrate --live --tunnelled vm1 qemu+ssh://dest/system # Cold migration virsh migrate vm1 qemu+ssh://dest/system","title":"Migration"},{"location":"virtualization/libvirt/#network-configuration_1","text":"","title":"Network Configuration"},{"location":"virtualization/libvirt/#virtual-networks","text":"Define Network : <network> <name>default</name> <forward mode='nat'/> <bridge name='virbr0' stp='on' delay='0'/> <ip address='192.168.122.1' netmask='255.255.255.0'> <dhcp> <range start='192.168.122.2' end='192.168.122.254'/> </dhcp> </ip> </network> Create Network : # Define network virsh net-define network.xml # Start network virsh net-start default # Autostart network virsh net-autostart default # List networks virsh net-list --all # Delete network virsh net-destroy default virsh net-undefine default Network Forward Modes : | Mode | Description | |------|-------------| | nat | Network address translation | | route | Simple routing | | bridge | Direct bridge | | open | No forwarding | | hostdev | PCI passthrough | | macvtap | MAC VLAN |","title":"Virtual Networks"},{"location":"virtualization/libvirt/#storage-configuration_1","text":"","title":"Storage Configuration"},{"location":"virtualization/libvirt/#storage-pools","text":"Define Storage Pool : <pool type='dir'> <name>default</name> <target> <path>/var/lib/libvirt/images</path> </target> </pool> Create Storage Pool : # Define pool virsh pool-define pool.xml # Build pool virsh pool-build default # Start pool virsh pool-start default # Autostart pool virsh pool-autostart default # List pools virsh pool-list --all Storage Pool Types : | Type | Description | Use Case | |------|-------------|----------| | dir | Directory | Simple storage | | fs | Filesystem | Local filesystem | | logical | LVM | Volume management | | iscsi | iSCSI | SAN storage | | netfs | NFS | Network file system | | rbd | Ceph RBD | Distributed storage | | disk | Physical disk | Disk partition | | zfs | ZFS | ZFS storage |","title":"Storage Pools"},{"location":"virtualization/libvirt/#storage-volumes","text":"Create Volume : # Create volume virsh vol-create-as default vm1.qcow2 20G --format qcow2 # Upload image virsh vol-upload default vm1.qcow2 disk.qcow2 # Download image virsh vol-download default vm1.qcow2 disk.qcow2 # Clone volume virsh vol-clone default vm1.qcow2 vm2.qcow2 # Delete volume virsh vol-delete default vm1.qcow2","title":"Storage Volumes"},{"location":"virtualization/libvirt/#api-bindings","text":"","title":"API Bindings"},{"location":"virtualization/libvirt/#python-api","text":"#!/usr/bin/env python3 import libvirt # Connect to libvirt conn = libvirt.open('qemu:///system') # Get domain dom = conn.lookupByName('vm1') # Start domain dom.create() # Get domain info info = dom.info() print(f\"State: {info[0]}\") print(f\"Memory: {info[1]}\") print(f\"CPUs: {info[3]}\") # Close connection conn.close()","title":"Python API"},{"location":"virtualization/libvirt/#c-api","text":"#include <libvirt/libvirt.h> int main() { virConnectPtr conn; virDomainPtr dom; // Connect to libvirt conn = virConnectOpen(\"qemu:///system\"); if (conn == NULL) { fprintf(stderr, \"Failed to open connection to qemu:///system\\n\"); return 1; } // Get domain dom = virDomainLookupByName(conn, \"vm1\"); if (dom == NULL) { fprintf(stderr, \"Failed to find domain vm1\\n\"); virConnectClose(conn); return 1; } // Start domain virDomainCreate(dom); // Cleanup virDomainFree(dom); virConnectClose(conn); return 0; }","title":"C API"},{"location":"virtualization/libvirt/#security","text":"","title":"Security"},{"location":"virtualization/libvirt/#authentication","text":"SASL Authentication : <!-- /etc/libvirt/libvirtd.conf --> auth_unix_ro = \"sasl\" auth_unix_rw = \"sasl\" TLS/SSL : <!-- /etc/libvirt/libvirtd.conf --> listen_tls = 1 listen_tcp = 0 <!-- Certificate locations --> cert_file = \"/etc/pki/libvirt/servercert.pem\" key_file = \"/etc/pki/libvirt/private/serverkey.pem\" ca_file = \"/etc/pki/CA/cacert.pem\"","title":"Authentication"},{"location":"virtualization/libvirt/#apparmorselinux","text":"AppArmor Profile : # Check AppArmor status aa-status # View AppArmor profile cat /etc/apparmor.d/usr.sbin.libvirtd SELinux Context : # Check SELinux status sestatus # Set SELinux context chcon -R --reference=/var/lib/libvirt/images /path/to/new/images","title":"AppArmor/SELinux"},{"location":"virtualization/libvirt/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"virtualization/libvirt/#connection-issues","text":"# Check libvirtd status systemctl status libvirtd # Check logs journalctl -u libvirtd -f # Check configuration virsh version virsh uri","title":"Connection Issues"},{"location":"virtualization/libvirt/#vm-issues","text":"# Check VM logs tail -f /var/log/libvirt/qemu/vm1.log # Check libvirt logs tail -f /var/log/libvirt/libvirtd.log # Check VM configuration virsh dumpxml vm1 virsh dominfo vm1","title":"VM Issues"},{"location":"virtualization/libvirt/#performance-issues","text":"# Check CPU usage top -H -p $(pidof qemu-kvm) # Check memory usage free -h # Check I/O iostat -x 1 # Check network sar -n DEV 1","title":"Performance Issues"},{"location":"virtualization/libvirt/#best-practices","text":"Use libvirt XML for VM configuration Enable virtio devices for performance Use storage pools for volume management Configure networks properly Enable live migration for high availability Set up monitoring and alerting Use proper security (TLS, SELinux) Test backup/restore procedures Document VM configurations Regular updates of libvirt and QEMU","title":"Best Practices"},{"location":"virtualization/libvirt/#source-code","text":"Repository : https://gitlab.com/libvirt/libvirt Documentation : https://libvirt.org/docs/","title":"Source Code"},{"location":"virtualization/libvirt/#key-source-locations","text":"Component Location Description libvirtd src/libvirtd.c Main daemon QEMU Driver src/qemu/ QEMU/KVM driver LXC Driver src/lxc/ LXC driver Xen Driver src/xen/ Xen driver Network src/network/ Network management Storage src/storage/ Storage management API src/libvirt.c Public API XML src/conf/ XML parsing","title":"Key Source Locations"},{"location":"virtualization/libvirt/#documentation-links","text":"libvirt API Reference libvirt XML Format libvirt Drivers libvirt Security","title":"Documentation Links"},{"location":"virtualization/ovs/","text":"Production-grade multilayer virtual switch for managing VM networking with OpenFlow support and advanced features. Architecture graph TB subgraph \"Virtual Machines\" A[VM1 eth0] B[VM2 eth0] C[VM3 eth0] end subgraph \"OVS Components\" D[OVS Bridge br0] E[OVS Database ovsdb-server] F[OVS Daemon ovs-vswitchd] G[Flow Tables] H[Port Table] I[Controller] J[OpenFlow Protocol] end subgraph \"Physical Network\" K[Host eth0] L[Host eth1] M[Physical Switch] end subgraph \"Management\" N[ovs-vsctl CLI] O[ovs-ofctl CLI] P[Remote Controller] end A --> D B --> D C --> D K --> D L --> D D --> E D --> F D --> G D --> H D --> I I --> J I --> P N --> E N --> F O --> G D --> M style D fill:#c8e6c9 style E fill:#ffecb3 style G fill:#e1f5ff style P fill:#fff3e0 Core Components ovs-vswitchd Main daemon implementing the switch datapath and OpenFlow protocol. Responsibilities : - Manage bridge datapath - Process OpenFlow rules - Forward packets between ports - Collect statistics - Communicate with controller Datapath Types : - Netlink : Linux kernel datapath (default) - DPDK : Userspace datapath for high performance - AF_XDP : High-performance Linux datapath Datapath Flow : graph LR A[Packet In] --> B[Packet Parser] B --> C[Flow Table Lookup] C --> D{Match Found?} D -->|Yes| E[Execute Actions] D -->|No| F[Send to Controller] E --> G[Forward/MOD/DROP] F --> H[Controller Decision] H --> I[Install Flow] I --> E style B fill:#c8e6c9 style E fill:#e1f5ff style F fill:#ffecb3 ovsdb-server Configuration database daemon storing OVS state. Database Structure : - Open_vSwitch : Switch configuration - Bridge : Bridge definitions - Port : Port configurations - Interface : Interface settings - Controller : Controller connections - Flow_Table : Flow table settings - QoS : Quality of Service - Queue : Queue configurations Database Operations : # Connect to database ovsdb-client connect unix:/var/run/openvswitch/db.sock # Monitor changes ovsdb-client monitor Open_vSwitch Bridge Port Interface # Dump database ovsdb-client dump OpenFlow Protocol Standard protocol for SDN controller communication. OpenFlow Versions : - OpenFlow 1.0: Basic features - OpenFlow 1.3: Extended features - OpenFlow 1.4: Group tables, meter tables - OpenFlow 1.5: Packet-out bundles OpenFlow Messages : - Controller-to-Switch : Features, Config, Modify, Packet-Out, Stats - Asynchronous : Packet-In, Flow-Removed, Port-Status - Symmetric : Hello, Echo, Error, Vendor Key Features Standard Linux Bridge Compatibility # Create OVS bridge (works like Linux bridge) ovs-vsctl add-br br0 # Add ports ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 vnet0 # Configure IP ip addr add 192.168.1.100/24 dev br0 ip link set br0 up Flow-Based Switching # Add flow rule ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_dst=192.168.1.100,actions=output:2\" # View flows ovs-ofctl dump-flows br0 # Delete flow ovs-ofctl del-flows br0 Network Isolation with VLANs # Add VLAN tagged port ovs-vsctl add-port br0 eth0 tag=10 # Add VLAN trunk port ovs-vsctl add-port br0 eth1 vlan_mode=trunk \\ trunks=10,20,30 # Add native VLAN port ovs-vsctl add-port br0 eth2 vlan_mode=native-tagged tag=100 Bonding and Trunking # Add LACP bond ovs-vsctl add-bond bond0 eth0 eth1 \\ lacp=active \\ bond_mode=balance-tcp # Add active-backup bond ovs-vsctl add-bond bond0 eth0 eth1 \\ bond_mode=active-backup # Add SLB bond ovs-vsctl add-bond bond0 eth0 eth1 \\ bond_mode=balance-slb Remote Management # Enable remote management ovs-vsctl set-manager ptcp:6640 # Connect with SSL ovs-vsctl set-protocol ssl ovs-vsctl set-ssl \\ /etc/openvswitch/privkey.pem \\ /etc/openvswitch/cert.pem \\ /etc/openvswitch/cacert.pem # Connect to controller ovs-vsctl set-controller br0 tcp:192.168.1.200:6653 Flow Tables Flow Table Structure graph TB A[Flow Table 0] -->|Miss| B[Flow Table 1] A -->|Match| C[Execute Actions] B -->|Miss| D[Flow Table 2] B -->|Match| C D -->|Miss| E[Controller] D -->|Match| C style A fill:#c8e6c9 style B fill:#e1f5ff style C fill:#ffecb3 style E fill:#fff3e0 Flow Rules Basic Flow # Simple flow: forward to output port ovs-ofctl add-flow br0 \\ \"priority=100,in_port=1,actions=output:2\" IP Flow # Forward specific IP ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_src=192.168.1.100,actions=output:3\" # Forward specific destination IP ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_dst=192.168.1.200,actions=output:2\" TCP/UDP Flow # Forward TCP traffic ovs-ofctl add-flow br0 \\ \"priority=100,tcp,tp_dst=80,actions=output:2\" # Forward UDP traffic ovs-ofctl add-flow br0 \\ \"priority=100,udp,tp_dst=53,actions=output:2\" VLAN Flow # Forward VLAN 10 ovs-ofctl add-flow br0 \\ \"priority=100,vlan_vid=10,actions=output:2\" # Strip VLAN and forward ovs-ofctl add-flow br0 \\ \"priority=100,vlan_vid=10,actions=strip_vlan,output:2\" Modifying Flows # Modify source IP ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_src=10.0.0.0/8,actions=mod_nw_src:192.168.1.100,output:2\" # Modify VLAN ovs-ofctl add-flow br0 \\ \"priority=100,actions=mod_vlan_vid:20,output:2\" Flow Actions Action Description Example output Send to port output:2 normal Forward normally normal flood Flood to all ports flood all Send to all ports all local Send to local bridge local controller Send to controller controller drop Drop packet drop mod_vlan_vid Modify VLAN mod_vlan_vid:10 strip_vlan Remove VLAN strip_vlan push_vlan Add VLAN push_vlan:0x8100 pop_vlan Remove VLAN tag pop_vlan mod_nw_src Modify source IP mod_nw_src:10.0.0.1 mod_nw_dst Modify dest IP mod_nw_dst:10.0.0.2 mod_tp_src Modify source port mod_tp_src:8080 mod_tp_dst Modify dest port mod_tp_dst:80 resubmit Resubmit to table resubmit:1 goto_table Jump to table goto_table:1 meter Apply meter meter:1 group Execute group group:1 learn Learn flow learn(table=1,...) exit Stop processing exit Flow Matching Fields Field Description Format in_port Input port in_port=1 dl_src Source MAC dl_src=aa:bb:cc:dd:ee:ff dl_dst Destination MAC dl_dst=00:11:22:33:44:55 dl_type Ethernet type dl_type=0x0800 vlan_vid VLAN ID vlan_vid=10 vlan_pcp VLAN priority vlan_pcp=3 ip IP protocol ip nw_src Source IP nw_src=192.168.1.100 nw_dst Destination IP nw_dst=192.168.1.200 nw_proto IP protocol nw_proto=6 nw_tos IP ToS nw_tos=0 tp_src Source port tp_src=80 tp_dst Destination port tp_dst=8080 icmp_type ICMP type icmp_type=8 icmp_code ICMP code icmp_code=0 Controllers Controller Configuration # Set controller ovs-vsctl set-controller br0 tcp:192.168.1.200:6653 # Set multiple controllers ovs-vsctl set-controller br0 \\ tcp:192.168.1.200:6653 \\ tcp:192.168.1.201:6653 # Set controller inactivity probe ovs-vsctl set controller br0 inactivity_probe=10 # Set controller fail mode ovs-vsctl set-fail-mode br0 secure Controller Fail Modes Mode Description standalone Switch forwards packets normally without controller secure Switch drops packets without controller OpenFlow Controller # Use OpenDaylight controller ovs-vsctl set-controller br0 tcp:192.168.1.200:6633 # Use ONOS controller ovs-vsctl set-controller br0 tcp:192.168.1.201:6653 # Use Open Virtual Network (OVN) ovs-vsctl set-controller br0 tcp:192.168.1.202:6642 Advanced Features Group Tables # Create group ovs-ofctl add-group br0 \\ \"group_id=1,type=select,bucket=output:2,bucket=output:3\" # Use group in flow ovs-ofctl add-flow br0 \\ \"priority=100,actions=group:1\" # Group types: select, all, indirect, fast_failover Meter Tables # Create meter ovs-ofctl add-meter br0 \\ \"meter=1,kbps,bands=type=drop,rate=1000\" # Use meter in flow ovs-ofctl add-flow br0 \\ \"priority=100,actions=meter:1,output:2\" Quality of Service (QoS) # Create QoS ovs-vsctl set port eth0 qos=@newqos \\ -- --id=@newqos create qos type=linux-htb \\ other-config:max-rate=1000000000 # Create queue ovs-vsctl set port eth0 qos=@newqos \\ -- --id=@newqos create qos type=linux-htb \\ queues:0=@q0 \\ -- --id=@q0 create queue other-config:max-rate=500000000 Mirroring # Create mirror ovs-vsctl -- \\ --id=@m create mirror name=mymirror select-dst-port=eth0 \\ -- set bridge br0 mirrors=@m # Add output port to mirror ovs-vsctl set mirror mymirror output-port=eth1 Tunneling GRE Tunnel # Create GRE tunnel ovs-vsctl add-port br0 gre0 -- \\ set interface gre0 type=gre options:remote_ip=192.168.1.200 # Create GRE tunnel with key ovs-vsctl add-port br0 gre0 -- \\ set interface gre0 type=gre \\ options:remote_ip=192.168.1.200 \\ options:key=100 VXLAN Tunnel # Create VXLAN tunnel ovs-vsctl add-port br0 vxlan0 -- \\ set interface vxlan0 type=vxlan \\ options:remote_ip=192.168.1.200 \\ options:key=flow # Create VXLAN with multicast ovs-vsctl add-port br0 vxlan0 -- \\ set interface vxlan0 type=vxlan \\ options:remote_ip=239.1.1.1 \\ options:key=100 Geneve Tunnel # Create Geneve tunnel ovs-vsctl add-port br0 geneve0 -- \\ set interface geneve0 type=geneve \\ options:remote_ip=192.168.1.200 \\ options:key=100 Quick Commands Bridge Management # Create bridge ovs-vsctl add-br br0 # Delete bridge ovs-vsctl del-br br0 # List bridges ovs-vsctl list-br # Show bridge ovs-vsctl show br0 Port Management # Add port ovs-vsctl add-port br0 eth0 # Delete port ovs-vsctl del-port br0 eth0 # List ports ovs-vsctl list-ports br0 # Show port ovs-vsctl show Flow Management # Add flow ovs-ofctl add-flow br0 \"priority=100,actions=output:2\" # Delete flow ovs-ofctl del-flows br0 # Dump flows ovs-ofctl dump-flows br0 # Monitor flows ovs-ofctl monitor br0 Configuration # Show configuration ovs-vsctl show # Show OVSDB ovs-vsctl list Open_vSwitch # Show bridge ovs-vsctl list Bridge # Show port ovs-vsctl list Port Nifty Behaviors Remote Management # Enable remote management ovs-vsctl set-manager ptcp:6640 # Connect with SSL ovs-vsctl set-protocol ssl Nifty : Secure remote OVS management Bond Configuration # Create LACP bond ovs-vsctl add-bond bond0 eth0 eth1 \\ lacp=active \\ bond_mode=balance-tcp Nifty : Network redundancy and load balancing VXLAN Overlay # Create VXLAN with key=flow ovs-vsctl add-port br0 vxlan0 -- \\ set interface vxlan0 type=vxlan \\ options:remote_ip=192.168.1.200 \\ options:key=flow Nifty : Network virtualization with per-flow keys Flow Learning # Add learning flow ovs-ofctl add-flow br0 \\ \"priority=100,actions=learn(table=1,idle_timeout=300,hard_timeout=1800,eth_src=dl_src,output=in_port),output:1\" Nifty : Automatic flow table population DPDK Datapath DPDK Setup # Create DPDK bridge ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev # Add DPDK port ovs-vsctl add-port br0 dpdk0 -- \\ set interface dpdk0 type=dpdk \\ options:dpdk-devargs=0000:01:00.0 DPDK Benefits Userspace packet processing Zero-copy networking High performance (10M+ PPS) Low latency (<1ms) DPDK Configuration # Set DPDK lcore ovs-vsctl set Open_vSwitch . other_config:dpdk-lcore-mask=0x1 # Set DPDK memory ovs-vsctl set Open_vSwitch . other_config:dpdk-socket-mem=1024 # Set DPDK PMD ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x2 OVN - Open Virtual Network OVN Architecture graph TB A[OVN Northbound DB] --> B[OVN Northbound Service] C[OVN Southbound DB] --> D[OVN Southbound Service] E[OVS] --> F[OVN Controller] B --> C D --> F F --> G[OVSDB] H[Applications] --> B I[Admins] --> B style A fill:#c8e6c9 style C fill:#e1f5ff style F fill:#ffecb3 OVN Setup # Install OVN apt-get install ovn-central ovn-host ovn-controller # Initialize OVN ovn-nbctl init ovn-sbctl init # Connect OVN to OVS ovs-vsctl set Open_vSwitch . \\ external_ids:ovn-remote=tcp:192.168.1.200:6642 \\ external_ids:ovn-encap-type=geneve \\ external_ids:ovn-encap-ip=192.168.1.10 Performance Tuning Flow Table Optimization # Set flow table size ovs-vsctl set bridge br0 \\ other-config:n-flow-entries=100000 # Set flow limits ovs-vsctl set bridge br0 \\ other-config:max-idle=300000 \\ other-config:n-backlog=1000 Datapath Optimization # Use hugepages ovs-vsctl set Open_vSwitch . \\ other_config:dpdk-socket-mem=1024,1024 # Enable batch processing ovs-vsctl set bridge br0 \\ other-config:hwaddr=02:00:00:00:00:01 Memory Optimization # Set memory limits ovs-vsctl set Open_vSwitch . \\ other_config:dpdk-alloc-mem=1024 # Set buffer size ovs-vsctl set bridge br0 \\ other-config:max-revalidator=4000 Troubleshooting Flow Issues # Check flows ovs-ofctl dump-flows br0 # Monitor flow additions ovs-ofctl monitor br0 # Check packet counters ovs-ofctl dump-ports br0 Connectivity Issues # Check bridge status ovs-vsctl show # Check port status ovs-vsctl list Port # Check interface status ovs-vsctl list Interface Controller Issues # Check controller status ovs-vsctl get controller br0 status # Check fail mode ovs-vsctl get fail-mode br0 # Test controller connectivity curl http://192.168.1.200:8080 Best Practices Use VLANs for network isolation Enable bonding for network redundancy Use flow tables for traffic control Monitor OVS for performance Secure OVSDB with SSL Test failover scenarios Document network topology Use tunnels for overlay networks Source Code Repository : https://github.com/openvswitch/ovs Documentation : https://docs.openvswitch.org/ Key Source Locations Component Location Description ovs-vswitchd vswitchd/ Main switch daemon ovsdb-server ovsdb/ Database server lib/ofp lib/ofp* OpenFlow library lib/dpif lib/dpif* Datapath interface lib/netdev lib/netdev* Network device library ofproto ofproto/ OpenFlow protocol","title":"Open vSwitch"},{"location":"virtualization/ovs/#architecture","text":"graph TB subgraph \"Virtual Machines\" A[VM1 eth0] B[VM2 eth0] C[VM3 eth0] end subgraph \"OVS Components\" D[OVS Bridge br0] E[OVS Database ovsdb-server] F[OVS Daemon ovs-vswitchd] G[Flow Tables] H[Port Table] I[Controller] J[OpenFlow Protocol] end subgraph \"Physical Network\" K[Host eth0] L[Host eth1] M[Physical Switch] end subgraph \"Management\" N[ovs-vsctl CLI] O[ovs-ofctl CLI] P[Remote Controller] end A --> D B --> D C --> D K --> D L --> D D --> E D --> F D --> G D --> H D --> I I --> J I --> P N --> E N --> F O --> G D --> M style D fill:#c8e6c9 style E fill:#ffecb3 style G fill:#e1f5ff style P fill:#fff3e0","title":"Architecture"},{"location":"virtualization/ovs/#core-components","text":"","title":"Core Components"},{"location":"virtualization/ovs/#ovs-vswitchd","text":"Main daemon implementing the switch datapath and OpenFlow protocol. Responsibilities : - Manage bridge datapath - Process OpenFlow rules - Forward packets between ports - Collect statistics - Communicate with controller Datapath Types : - Netlink : Linux kernel datapath (default) - DPDK : Userspace datapath for high performance - AF_XDP : High-performance Linux datapath Datapath Flow : graph LR A[Packet In] --> B[Packet Parser] B --> C[Flow Table Lookup] C --> D{Match Found?} D -->|Yes| E[Execute Actions] D -->|No| F[Send to Controller] E --> G[Forward/MOD/DROP] F --> H[Controller Decision] H --> I[Install Flow] I --> E style B fill:#c8e6c9 style E fill:#e1f5ff style F fill:#ffecb3","title":"ovs-vswitchd"},{"location":"virtualization/ovs/#ovsdb-server","text":"Configuration database daemon storing OVS state. Database Structure : - Open_vSwitch : Switch configuration - Bridge : Bridge definitions - Port : Port configurations - Interface : Interface settings - Controller : Controller connections - Flow_Table : Flow table settings - QoS : Quality of Service - Queue : Queue configurations Database Operations : # Connect to database ovsdb-client connect unix:/var/run/openvswitch/db.sock # Monitor changes ovsdb-client monitor Open_vSwitch Bridge Port Interface # Dump database ovsdb-client dump","title":"ovsdb-server"},{"location":"virtualization/ovs/#openflow-protocol","text":"Standard protocol for SDN controller communication. OpenFlow Versions : - OpenFlow 1.0: Basic features - OpenFlow 1.3: Extended features - OpenFlow 1.4: Group tables, meter tables - OpenFlow 1.5: Packet-out bundles OpenFlow Messages : - Controller-to-Switch : Features, Config, Modify, Packet-Out, Stats - Asynchronous : Packet-In, Flow-Removed, Port-Status - Symmetric : Hello, Echo, Error, Vendor","title":"OpenFlow Protocol"},{"location":"virtualization/ovs/#key-features","text":"","title":"Key Features"},{"location":"virtualization/ovs/#standard-linux-bridge-compatibility","text":"# Create OVS bridge (works like Linux bridge) ovs-vsctl add-br br0 # Add ports ovs-vsctl add-port br0 eth0 ovs-vsctl add-port br0 vnet0 # Configure IP ip addr add 192.168.1.100/24 dev br0 ip link set br0 up","title":"Standard Linux Bridge Compatibility"},{"location":"virtualization/ovs/#flow-based-switching","text":"# Add flow rule ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_dst=192.168.1.100,actions=output:2\" # View flows ovs-ofctl dump-flows br0 # Delete flow ovs-ofctl del-flows br0","title":"Flow-Based Switching"},{"location":"virtualization/ovs/#network-isolation-with-vlans","text":"# Add VLAN tagged port ovs-vsctl add-port br0 eth0 tag=10 # Add VLAN trunk port ovs-vsctl add-port br0 eth1 vlan_mode=trunk \\ trunks=10,20,30 # Add native VLAN port ovs-vsctl add-port br0 eth2 vlan_mode=native-tagged tag=100","title":"Network Isolation with VLANs"},{"location":"virtualization/ovs/#bonding-and-trunking","text":"# Add LACP bond ovs-vsctl add-bond bond0 eth0 eth1 \\ lacp=active \\ bond_mode=balance-tcp # Add active-backup bond ovs-vsctl add-bond bond0 eth0 eth1 \\ bond_mode=active-backup # Add SLB bond ovs-vsctl add-bond bond0 eth0 eth1 \\ bond_mode=balance-slb","title":"Bonding and Trunking"},{"location":"virtualization/ovs/#remote-management","text":"# Enable remote management ovs-vsctl set-manager ptcp:6640 # Connect with SSL ovs-vsctl set-protocol ssl ovs-vsctl set-ssl \\ /etc/openvswitch/privkey.pem \\ /etc/openvswitch/cert.pem \\ /etc/openvswitch/cacert.pem # Connect to controller ovs-vsctl set-controller br0 tcp:192.168.1.200:6653","title":"Remote Management"},{"location":"virtualization/ovs/#flow-tables","text":"","title":"Flow Tables"},{"location":"virtualization/ovs/#flow-table-structure","text":"graph TB A[Flow Table 0] -->|Miss| B[Flow Table 1] A -->|Match| C[Execute Actions] B -->|Miss| D[Flow Table 2] B -->|Match| C D -->|Miss| E[Controller] D -->|Match| C style A fill:#c8e6c9 style B fill:#e1f5ff style C fill:#ffecb3 style E fill:#fff3e0","title":"Flow Table Structure"},{"location":"virtualization/ovs/#flow-rules","text":"","title":"Flow Rules"},{"location":"virtualization/ovs/#basic-flow","text":"# Simple flow: forward to output port ovs-ofctl add-flow br0 \\ \"priority=100,in_port=1,actions=output:2\"","title":"Basic Flow"},{"location":"virtualization/ovs/#ip-flow","text":"# Forward specific IP ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_src=192.168.1.100,actions=output:3\" # Forward specific destination IP ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_dst=192.168.1.200,actions=output:2\"","title":"IP Flow"},{"location":"virtualization/ovs/#tcpudp-flow","text":"# Forward TCP traffic ovs-ofctl add-flow br0 \\ \"priority=100,tcp,tp_dst=80,actions=output:2\" # Forward UDP traffic ovs-ofctl add-flow br0 \\ \"priority=100,udp,tp_dst=53,actions=output:2\"","title":"TCP/UDP Flow"},{"location":"virtualization/ovs/#vlan-flow","text":"# Forward VLAN 10 ovs-ofctl add-flow br0 \\ \"priority=100,vlan_vid=10,actions=output:2\" # Strip VLAN and forward ovs-ofctl add-flow br0 \\ \"priority=100,vlan_vid=10,actions=strip_vlan,output:2\"","title":"VLAN Flow"},{"location":"virtualization/ovs/#modifying-flows","text":"# Modify source IP ovs-ofctl add-flow br0 \\ \"priority=100,ip,nw_src=10.0.0.0/8,actions=mod_nw_src:192.168.1.100,output:2\" # Modify VLAN ovs-ofctl add-flow br0 \\ \"priority=100,actions=mod_vlan_vid:20,output:2\"","title":"Modifying Flows"},{"location":"virtualization/ovs/#flow-actions","text":"Action Description Example output Send to port output:2 normal Forward normally normal flood Flood to all ports flood all Send to all ports all local Send to local bridge local controller Send to controller controller drop Drop packet drop mod_vlan_vid Modify VLAN mod_vlan_vid:10 strip_vlan Remove VLAN strip_vlan push_vlan Add VLAN push_vlan:0x8100 pop_vlan Remove VLAN tag pop_vlan mod_nw_src Modify source IP mod_nw_src:10.0.0.1 mod_nw_dst Modify dest IP mod_nw_dst:10.0.0.2 mod_tp_src Modify source port mod_tp_src:8080 mod_tp_dst Modify dest port mod_tp_dst:80 resubmit Resubmit to table resubmit:1 goto_table Jump to table goto_table:1 meter Apply meter meter:1 group Execute group group:1 learn Learn flow learn(table=1,...) exit Stop processing exit","title":"Flow Actions"},{"location":"virtualization/ovs/#flow-matching-fields","text":"Field Description Format in_port Input port in_port=1 dl_src Source MAC dl_src=aa:bb:cc:dd:ee:ff dl_dst Destination MAC dl_dst=00:11:22:33:44:55 dl_type Ethernet type dl_type=0x0800 vlan_vid VLAN ID vlan_vid=10 vlan_pcp VLAN priority vlan_pcp=3 ip IP protocol ip nw_src Source IP nw_src=192.168.1.100 nw_dst Destination IP nw_dst=192.168.1.200 nw_proto IP protocol nw_proto=6 nw_tos IP ToS nw_tos=0 tp_src Source port tp_src=80 tp_dst Destination port tp_dst=8080 icmp_type ICMP type icmp_type=8 icmp_code ICMP code icmp_code=0","title":"Flow Matching Fields"},{"location":"virtualization/ovs/#controllers","text":"","title":"Controllers"},{"location":"virtualization/ovs/#controller-configuration","text":"# Set controller ovs-vsctl set-controller br0 tcp:192.168.1.200:6653 # Set multiple controllers ovs-vsctl set-controller br0 \\ tcp:192.168.1.200:6653 \\ tcp:192.168.1.201:6653 # Set controller inactivity probe ovs-vsctl set controller br0 inactivity_probe=10 # Set controller fail mode ovs-vsctl set-fail-mode br0 secure","title":"Controller Configuration"},{"location":"virtualization/ovs/#controller-fail-modes","text":"Mode Description standalone Switch forwards packets normally without controller secure Switch drops packets without controller","title":"Controller Fail Modes"},{"location":"virtualization/ovs/#openflow-controller","text":"# Use OpenDaylight controller ovs-vsctl set-controller br0 tcp:192.168.1.200:6633 # Use ONOS controller ovs-vsctl set-controller br0 tcp:192.168.1.201:6653 # Use Open Virtual Network (OVN) ovs-vsctl set-controller br0 tcp:192.168.1.202:6642","title":"OpenFlow Controller"},{"location":"virtualization/ovs/#advanced-features","text":"","title":"Advanced Features"},{"location":"virtualization/ovs/#group-tables","text":"# Create group ovs-ofctl add-group br0 \\ \"group_id=1,type=select,bucket=output:2,bucket=output:3\" # Use group in flow ovs-ofctl add-flow br0 \\ \"priority=100,actions=group:1\" # Group types: select, all, indirect, fast_failover","title":"Group Tables"},{"location":"virtualization/ovs/#meter-tables","text":"# Create meter ovs-ofctl add-meter br0 \\ \"meter=1,kbps,bands=type=drop,rate=1000\" # Use meter in flow ovs-ofctl add-flow br0 \\ \"priority=100,actions=meter:1,output:2\"","title":"Meter Tables"},{"location":"virtualization/ovs/#quality-of-service-qos","text":"# Create QoS ovs-vsctl set port eth0 qos=@newqos \\ -- --id=@newqos create qos type=linux-htb \\ other-config:max-rate=1000000000 # Create queue ovs-vsctl set port eth0 qos=@newqos \\ -- --id=@newqos create qos type=linux-htb \\ queues:0=@q0 \\ -- --id=@q0 create queue other-config:max-rate=500000000","title":"Quality of Service (QoS)"},{"location":"virtualization/ovs/#mirroring","text":"# Create mirror ovs-vsctl -- \\ --id=@m create mirror name=mymirror select-dst-port=eth0 \\ -- set bridge br0 mirrors=@m # Add output port to mirror ovs-vsctl set mirror mymirror output-port=eth1","title":"Mirroring"},{"location":"virtualization/ovs/#tunneling","text":"","title":"Tunneling"},{"location":"virtualization/ovs/#gre-tunnel","text":"# Create GRE tunnel ovs-vsctl add-port br0 gre0 -- \\ set interface gre0 type=gre options:remote_ip=192.168.1.200 # Create GRE tunnel with key ovs-vsctl add-port br0 gre0 -- \\ set interface gre0 type=gre \\ options:remote_ip=192.168.1.200 \\ options:key=100","title":"GRE Tunnel"},{"location":"virtualization/ovs/#vxlan-tunnel","text":"# Create VXLAN tunnel ovs-vsctl add-port br0 vxlan0 -- \\ set interface vxlan0 type=vxlan \\ options:remote_ip=192.168.1.200 \\ options:key=flow # Create VXLAN with multicast ovs-vsctl add-port br0 vxlan0 -- \\ set interface vxlan0 type=vxlan \\ options:remote_ip=239.1.1.1 \\ options:key=100","title":"VXLAN Tunnel"},{"location":"virtualization/ovs/#geneve-tunnel","text":"# Create Geneve tunnel ovs-vsctl add-port br0 geneve0 -- \\ set interface geneve0 type=geneve \\ options:remote_ip=192.168.1.200 \\ options:key=100","title":"Geneve Tunnel"},{"location":"virtualization/ovs/#quick-commands","text":"","title":"Quick Commands"},{"location":"virtualization/ovs/#bridge-management","text":"# Create bridge ovs-vsctl add-br br0 # Delete bridge ovs-vsctl del-br br0 # List bridges ovs-vsctl list-br # Show bridge ovs-vsctl show br0","title":"Bridge Management"},{"location":"virtualization/ovs/#port-management","text":"# Add port ovs-vsctl add-port br0 eth0 # Delete port ovs-vsctl del-port br0 eth0 # List ports ovs-vsctl list-ports br0 # Show port ovs-vsctl show","title":"Port Management"},{"location":"virtualization/ovs/#flow-management","text":"# Add flow ovs-ofctl add-flow br0 \"priority=100,actions=output:2\" # Delete flow ovs-ofctl del-flows br0 # Dump flows ovs-ofctl dump-flows br0 # Monitor flows ovs-ofctl monitor br0","title":"Flow Management"},{"location":"virtualization/ovs/#configuration","text":"# Show configuration ovs-vsctl show # Show OVSDB ovs-vsctl list Open_vSwitch # Show bridge ovs-vsctl list Bridge # Show port ovs-vsctl list Port","title":"Configuration"},{"location":"virtualization/ovs/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"virtualization/ovs/#remote-management_1","text":"# Enable remote management ovs-vsctl set-manager ptcp:6640 # Connect with SSL ovs-vsctl set-protocol ssl Nifty : Secure remote OVS management","title":"Remote Management"},{"location":"virtualization/ovs/#bond-configuration","text":"# Create LACP bond ovs-vsctl add-bond bond0 eth0 eth1 \\ lacp=active \\ bond_mode=balance-tcp Nifty : Network redundancy and load balancing","title":"Bond Configuration"},{"location":"virtualization/ovs/#vxlan-overlay","text":"# Create VXLAN with key=flow ovs-vsctl add-port br0 vxlan0 -- \\ set interface vxlan0 type=vxlan \\ options:remote_ip=192.168.1.200 \\ options:key=flow Nifty : Network virtualization with per-flow keys","title":"VXLAN Overlay"},{"location":"virtualization/ovs/#flow-learning","text":"# Add learning flow ovs-ofctl add-flow br0 \\ \"priority=100,actions=learn(table=1,idle_timeout=300,hard_timeout=1800,eth_src=dl_src,output=in_port),output:1\" Nifty : Automatic flow table population","title":"Flow Learning"},{"location":"virtualization/ovs/#dpdk-datapath","text":"","title":"DPDK Datapath"},{"location":"virtualization/ovs/#dpdk-setup","text":"# Create DPDK bridge ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev # Add DPDK port ovs-vsctl add-port br0 dpdk0 -- \\ set interface dpdk0 type=dpdk \\ options:dpdk-devargs=0000:01:00.0","title":"DPDK Setup"},{"location":"virtualization/ovs/#dpdk-benefits","text":"Userspace packet processing Zero-copy networking High performance (10M+ PPS) Low latency (<1ms)","title":"DPDK Benefits"},{"location":"virtualization/ovs/#dpdk-configuration","text":"# Set DPDK lcore ovs-vsctl set Open_vSwitch . other_config:dpdk-lcore-mask=0x1 # Set DPDK memory ovs-vsctl set Open_vSwitch . other_config:dpdk-socket-mem=1024 # Set DPDK PMD ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x2","title":"DPDK Configuration"},{"location":"virtualization/ovs/#ovn-open-virtual-network","text":"","title":"OVN - Open Virtual Network"},{"location":"virtualization/ovs/#ovn-architecture","text":"graph TB A[OVN Northbound DB] --> B[OVN Northbound Service] C[OVN Southbound DB] --> D[OVN Southbound Service] E[OVS] --> F[OVN Controller] B --> C D --> F F --> G[OVSDB] H[Applications] --> B I[Admins] --> B style A fill:#c8e6c9 style C fill:#e1f5ff style F fill:#ffecb3","title":"OVN Architecture"},{"location":"virtualization/ovs/#ovn-setup","text":"# Install OVN apt-get install ovn-central ovn-host ovn-controller # Initialize OVN ovn-nbctl init ovn-sbctl init # Connect OVN to OVS ovs-vsctl set Open_vSwitch . \\ external_ids:ovn-remote=tcp:192.168.1.200:6642 \\ external_ids:ovn-encap-type=geneve \\ external_ids:ovn-encap-ip=192.168.1.10","title":"OVN Setup"},{"location":"virtualization/ovs/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"virtualization/ovs/#flow-table-optimization","text":"# Set flow table size ovs-vsctl set bridge br0 \\ other-config:n-flow-entries=100000 # Set flow limits ovs-vsctl set bridge br0 \\ other-config:max-idle=300000 \\ other-config:n-backlog=1000","title":"Flow Table Optimization"},{"location":"virtualization/ovs/#datapath-optimization","text":"# Use hugepages ovs-vsctl set Open_vSwitch . \\ other_config:dpdk-socket-mem=1024,1024 # Enable batch processing ovs-vsctl set bridge br0 \\ other-config:hwaddr=02:00:00:00:00:01","title":"Datapath Optimization"},{"location":"virtualization/ovs/#memory-optimization","text":"# Set memory limits ovs-vsctl set Open_vSwitch . \\ other_config:dpdk-alloc-mem=1024 # Set buffer size ovs-vsctl set bridge br0 \\ other-config:max-revalidator=4000","title":"Memory Optimization"},{"location":"virtualization/ovs/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"virtualization/ovs/#flow-issues","text":"# Check flows ovs-ofctl dump-flows br0 # Monitor flow additions ovs-ofctl monitor br0 # Check packet counters ovs-ofctl dump-ports br0","title":"Flow Issues"},{"location":"virtualization/ovs/#connectivity-issues","text":"# Check bridge status ovs-vsctl show # Check port status ovs-vsctl list Port # Check interface status ovs-vsctl list Interface","title":"Connectivity Issues"},{"location":"virtualization/ovs/#controller-issues","text":"# Check controller status ovs-vsctl get controller br0 status # Check fail mode ovs-vsctl get fail-mode br0 # Test controller connectivity curl http://192.168.1.200:8080","title":"Controller Issues"},{"location":"virtualization/ovs/#best-practices","text":"Use VLANs for network isolation Enable bonding for network redundancy Use flow tables for traffic control Monitor OVS for performance Secure OVSDB with SSL Test failover scenarios Document network topology Use tunnels for overlay networks","title":"Best Practices"},{"location":"virtualization/ovs/#source-code","text":"Repository : https://github.com/openvswitch/ovs Documentation : https://docs.openvswitch.org/","title":"Source Code"},{"location":"virtualization/ovs/#key-source-locations","text":"Component Location Description ovs-vswitchd vswitchd/ Main switch daemon ovsdb-server ovsdb/ Database server lib/ofp lib/ofp* OpenFlow library lib/dpif lib/dpif* Datapath interface lib/netdev lib/netdev* Network device library ofproto ofproto/ OpenFlow protocol","title":"Key Source Locations"},{"location":"virtualization/qemu/","text":"Generic and open source machine emulator and virtualizer providing full-system and user-mode emulation with hardware acceleration. Architecture graph TB subgraph \"QEMU Process\" A[Main Process] B[VCPU Threads] C[IO Thread] D[Timer Thread] E[Display Thread] end subgraph \"Accelerators\" F[TCG - Tiny Code Generator] G[KVM - Kernel Virtual Machine] H[Xen] end subgraph \"Device Emulation\" I[Virtio Devices] J[Network] K[Storage] L[Graphics] M[Input] end subgraph \"I/O Paths\" N[MMIO - Memory Mapped I/O] O[PIO - Port I/O] P[DMA - Direct Memory Access] Q[IRQ - Interrupt Request] end A --> B A --> C A --> D A --> E B --> F B --> G B --> H C --> I C --> J C --> K C --> L C --> M I --> N I --> O I --> P I --> Q J --> N J --> O K --> N K --> O style A fill:#c8e6c9 style G fill:#ffecb3 style F fill:#e1f5ff style I fill:#fff3e0 Core Components VCPU (Virtual CPU) Threads executing guest code in virtual CPU context. VCPU States : | State | Description | |-------|-------------| | Running | Currently executing guest code | | Blocked | Waiting for I/O or sleep | | Exited | VCPU exited | VCPU Configuration : # Set VCPU count qemu-system-x86_64 -smp 4 # Set CPU topology qemu-system-x86_64 -smp 4,sockets=2,cores=2,threads=1 # Pin VCPUs to physical CPUs qemu-system-x86_64 -smp 4 -numa node,memdev=mem TCG (Tiny Code Generator) Dynamic binary translation engine for software emulation. TCG Operation : graph LR A[Guest Binary Code] --> B[TCG Translator] B --> C[Host Code Block] C --> D[Translation Cache] D --> E[Code Execution] F[Cache Miss] --> B E --> G[Execution Result] style B fill:#c8e6c9 style D fill:#ffecb3 style E fill:#e1f5ff TCG Features : - Dynamic translation - Block chaining - Code cache management - Cross-architectural emulation - No hardware requirements TCG Performance : - Slower than KVM (~10-20% host speed) - Cross-platform compatibility - Good for non-virtualized hosts KVM Acceleration Hardware virtualization using Intel VT-x or AMD-V. KVM Operation : sequenceDiagram participant Q as QEMU participant K as KVM Module participant G as Guest Q->>K: Create VM K->>K: Allocate resources Q->>G: Start VCPU G->>G: Execute guest code G->>K: VM Exit (I/O, interrupt) K->>Q: Handle exit Q->>K: Resume guest style K fill:#c8e6c9 style G fill:#e1f5ff KVM Features : - Native CPU execution - Near-native performance (~95-98% host speed) - Virtualization extensions required - Paravirtualized I/O support VM Exit Reasons : | Reason | Description | Frequency | |--------|-------------|----------| | I/O Port Access | Guest accesses PIO | High | | MMIO Access | Guest accesses MMIO | Medium | | Interrupt | Hardware interrupt | High | | Interrupt Window | Interrupt delivery needed | Medium | | HLT | CPU halted | Low | | CR Access | Control register access | Low | IO Thread Dedicated thread for I/O operations to improve performance. IO Thread Benefits : - Non-blocking I/O for VCPUs - Improved I/O throughput - Better concurrency IO Thread Configuration : # Enable IO thread qemu-system-x86_64 -object iothread,id=io1 \\ -device virtio-blk-pci,iothread=io1 # Multiple IO threads qemu-system-x86_64 \\ -object iothread,id=io1 \\ -object iothread,id=io2 \\ -device virtio-blk-pci,iothread=io1 \\ -device virtio-blk-pci,iothread=io2 Device Emulation Virtio Devices Paravirtualized devices for optimal performance. Virtio Architecture : graph TB A[Guest Driver] --> B[Virtqueue] B --> C[Virtio Ring] C --> D[Host Device] E[Notification] --> B F[Completion] --> B style B fill:#c8e6c9 style D fill:#e1f5ff Virtio Device Types : | Device | Description | Performance | |--------|-------------|------------| | virtio-blk | Block device | Excellent | | virtio-net | Network device | Excellent | | virtio-scsi | SCSI controller | Very Good | | virtio-serial | Serial port | Good | | virtio-gpu | Graphics | Good | | virtio-rng | Random number | Good | Virtio Features : - Zero-copy I/O - Direct guest memory access - Efficient notification mechanism - Queue-based I/O Network Devices Virtio Network # Basic virtio network qemu-system-x86_64 -net nic,model=virtio \\ -net user,hostfwd=tcp::2222-:22 # Multiqueue virtio qemu-system-x86_64 -net nic,model=virtio,vectors=6 \\ -net tap,ifname=tap0,queues=4,vhost=on # Bridge network qemu-system-x86_64 -net nic,model=virtio \\ -net tap,script=/etc/qemu-ifup,downscript=/etc/qemu-ifdown Virtio Network Features : - Multiple queues (virtio multiqueue) - Vhost support (kernel-space driver) - Zero-copy transmission - Large packet offload Vhost Configuration : # Enable vhost-net qemu-system-x86_64 -net nic,model=virtio \\ -net tap,vhost=on,vhostforce=on # Multiple vhost queues qemu-system-x86_64 -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on Storage Devices Virtio Block # Virtio block device qemu-system-x86_64 -drive file=disk.qcow2,if=virtio,format=qcow2 # Virtio block with cache qemu-system-x86_64 \\ -drive file=disk.qcow2,if=virtio,format=qcow2,cache=writeback # Virtio block with multiple queues qemu-system-x86_64 \\ -object iothread,id=io1 \\ -drive file=disk.qcow2,if=virtio,format=qcow2,iothread=io1 Virtio Block Features : - Direct I/O support - Multiple queues with IO thread - Write barriers - Discard support - Write-zeroes support Virtio SCSI # Virtio SCSI qemu-system-x86_64 \\ -device virtio-scsi-pci \\ -drive file=disk1.qcow2,if=none,id=drive1 \\ -device scsi-hd,drive=drive1 # Multiple SCSI devices qemu-system-x86_64 \\ -device virtio-scsi-pci \\ -drive file=disk1.qcow2,if=none,id=drive1 \\ -device scsi-hd,drive=drive1 \\ -drive file=disk2.qcow2,if=none,id=drive2 \\ -device scsi-hd,drive=drive2 Virtio SCSI Features : - Multiple devices per controller - SCSI command support - Persistent reservations - Better than virtio-blk for many disks I/O Paths MMIO (Memory Mapped I/O) Device registers mapped to guest memory space. MMIO Access Flow : graph LR A[Guest CPU] --> B[Guest Address Space] B --> C[MMIO Region] C --> D[MMIO Handler] D --> E[Device Emulation] F[VM Exit] --> D E --> G[I/O Completion] style C fill:#c8e6c9 style E fill:#e1f5ff MMIO Characteristics : - Memory-mapped registers - Triggers VM exit on access - Used for device configuration - Slower than DMA PIO (Port I/O) Legacy x86 I/O port access. PIO Characteristics : - Legacy x86 I/O ports - Triggers VM exit on IN/OUT - Used for legacy device access - Slower than MMIO - Limited to 64K ports PIO Example : # Legacy IDE device qemu-system-x86_64 \\ -drive file=disk.img,if=ide # Legacy network (e1000) qemu-system-x86_64 \\ -net nic,model=e1000 DMA (Direct Memory Access) Direct guest memory access for I/O operations. DMA Operation : graph TB A[Guest Device] --> B[DMA Request] B --> C[Guest Physical Address] C --> D[IOMMU Translation] D --> E[Host Physical Address] E --> F[Host Device] F --> G[Data Transfer] G --> H[Completion Interrupt] style D fill:#c8e6c9 style F fill:#e1f5ff DMA Features : - Direct guest memory access - High performance - IOMMU support for isolation - Used by virtio devices IRQ (Interrupt Request) Hardware interrupt delivery to guest. IRQ Flow : sequenceDiagram participant G as Guest participant Q as QEMU participant K as KVM G->>Q: Complete I/O Q->>K: Inject IRQ K->>G: Deliver interrupt G->>G: Handle interrupt style K fill:#c8e6c9 style G fill:#e1f5ff IRQ Types : - PIC : Legacy 8259 PIC - IOAPIC : Advanced programmable interrupt controller - MSI : Message signaled interrupts - MSI-X : Enhanced MSI MSI Configuration : # Enable MSI-X qemu-system-x86_64 \\ -device virtio-net-pci,msix=on # Multiple MSI-X vectors qemu-system-x86_64 \\ -device virtio-net-pci,msix=on,vectors=6 Quick Commands Basic VM Creation # Simple VM qemu-system-x86_64 \\ -name vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 \\ -netdev user,id=net0,hostfwd=tcp::2222-:22 # VM with graphics qemu-system-x86_64 \\ -name vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 \\ -vnc :1 # VM with KVM qemu-system-x86_64 -enable-kvm \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,format=qcow2 Advanced Configuration # CPU configuration qemu-system-x86_64 \\ -cpu host \\ -smp 4,sockets=2,cores=2,threads=1 # Memory configuration qemu-system-x86_64 \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # NUMA topology qemu-system-x86_64 \\ -m 4096 \\ -numa node,memdev=mem \\ -object memory-backend-file,id=mem,size=4G,mem-path=/dev/hugepages,share=on Network Configuration # User networking qemu-system-x86_64 \\ -netdev user,id=net0,hostfwd=tcp::2222-:22 # TAP networking qemu-system-x86_64 \\ -netdev tap,id=net0,ifname=tap0,script=no,downscript=no # Bridge networking qemu-system-x86_64 \\ -netdev bridge,id=net0,br=br0 # Virtio multiqueue qemu-system-x86_64 \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on Storage Configuration # Qcow2 image qemu-system-x86_64 \\ -drive file=disk.qcow2,format=qcow2,if=virtio # Raw image qemu-system-x86_64 \\ -drive file=disk.img,format=raw,if=virtio # Multiple disks qemu-system-x86_64 \\ -drive file=disk1.qcow2,if=virtio \\ -drive file=disk2.qcow2,if=virtio # CD-ROM qemu-system-x86_64 \\ -drive file=ubuntu.iso,media=cdrom Nifty Behaviors QMP Programmatic Control # Start QEMU with QMP socket qemu-system-x86_64 -qmp tcp:localhost:4444,server,nowait # Connect to QMP echo '{\"execute\":\"qmp_capabilities\"}' | \\ socat TCP:localhost:4444 - # Query status echo '{\"execute\":\"query-status\"}' | \\ socat TCP:localhost:4444 - # Start VM echo '{\"execute\":\"cont\"}' | \\ socat TCP:localhost:4444 - Nifty : Programmatic control of running VMs Virtio Multiqueue # Create VM with multiqueue qemu-system-x86_64 \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on # Libvirt XML <interface type='network'> <model type='virtio'/> <driver name='vhost' queues='4'/> </interface> Nifty : Multi-queue network, better throughput Memory Ballooning # Attach balloon device qemu-system-x86_64 \\ -device virtio-balloon-pci # Adjust memory via QMP echo '{\"execute\":\"balloon\",\"arguments\":{\"size\":1073741824}}' | \\ socat TCP:localhost:4444 - # Libvirt balloon virsh setmem vm1 2G virsh setmem vm1 4G Nifty : Dynamically adjust VM memory Live Migration # Source VM qemu-system-x86_64 \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,if=virtio \\ -incoming tcp:0:4444 # Destination VM qemu-system-x86_64 \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,if=virtio \\ -monitor stdio # Trigger migration (in destination monitor) migrate -tcp:source:4444 Nifty : Zero-downtime VM migration Performance Optimization CPU Optimization # Host CPU passthrough qemu-system-x86_64 -cpu host # CPU features qemu-system-x86_64 -cpu host,+vmx,+pdpe1gb # CPU pinning qemu-system-x86_64 -smp 4 -numa node,memdev=mem Memory Optimization # Hugepages qemu-system-x86_64 \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # KSM (Kernel Same-page Merging) echo 1 > /sys/kernel/mm/ksm/run echo 1000 > /sys/kernel/mm/ksm/pages_to_scan # Memory balloon qemu-system-x86_64 -device virtio-balloon-pci I/O Optimization # IO thread qemu-system-x86_64 \\ -object iothread,id=io1 \\ -drive file=disk.qcow2,if=virtio,iothread=io1 # Multiple IO threads qemu-system-x86_64 \\ -object iothread,id=io1 \\ -object iothread,id=io2 \\ -drive file=disk1.qcow2,if=virtio,iothread=io1 \\ -drive file=disk2.qcow2,if=virtio,iothread=io2 # Cache options qemu-system-x86_64 \\ -drive file=disk.qcow2,if=virtio,cache=writeback,async=on # Native AIO qemu-system-x86_64 \\ -drive file=disk.img,if=virtio,aio=native Network Optimization # Vhost-net qemu-system-x86_64 \\ -net nic,model=virtio \\ -net tap,vhost=on # Multiple queues qemu-system-x86_64 \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on # Large receive offload qemu-system-x86_64 \\ -net nic,model=virtio \\ -net tap,gro=on Troubleshooting Performance Issues # Check if KVM is enabled qemu-system-x86_64 -accel kvm -version # Check VCPU load top -H -p $(pidof qemu-system-x86_64) # Check I/O iostat -x 1 # Check network sar -n DEV 1 Migration Issues # Check migration status echo '{\"execute\":\"query-migrate\"}' | \\ socat TCP:localhost:4444 - # Cancel migration echo '{\"execute\":\"migrate_cancel\"}' | \\ socat TCP:localhost:4444 - # Check compatibility echo '{\"execute\":\"query-migrate-capabilities\"}' | \\ socat TCP:localhost:4444 - Best Practices Always use KVM for x86 guests on supported hardware Use virtio devices for optimal performance Enable IO threads for storage-intensive workloads Use hugepages for memory-intensive workloads Enable vhost-net for network performance Configure NUMA for large VMs Use cache=writeback for better disk performance Test migration in production environment Monitor VM performance regularly Use snapshots for backup/rollback Source Code Repository : https://gitlab.com/qemu-project/qemu Documentation : https://www.qemu.org/documentation/ Key Source Locations Component Location Description Main loop cpus.c CPU execution loop TCG tcg/ Tiny Code Generator KVM accel/kvm/ KVM acceleration Memory exec.c Memory management I/O ioport.c Port I/O MMIO memory.c Memory Mapped I/O Virtio hw/virtio/ Virtio devices Block block/ Block devices Network net/ Network devices","title":"QEMU"},{"location":"virtualization/qemu/#architecture","text":"graph TB subgraph \"QEMU Process\" A[Main Process] B[VCPU Threads] C[IO Thread] D[Timer Thread] E[Display Thread] end subgraph \"Accelerators\" F[TCG - Tiny Code Generator] G[KVM - Kernel Virtual Machine] H[Xen] end subgraph \"Device Emulation\" I[Virtio Devices] J[Network] K[Storage] L[Graphics] M[Input] end subgraph \"I/O Paths\" N[MMIO - Memory Mapped I/O] O[PIO - Port I/O] P[DMA - Direct Memory Access] Q[IRQ - Interrupt Request] end A --> B A --> C A --> D A --> E B --> F B --> G B --> H C --> I C --> J C --> K C --> L C --> M I --> N I --> O I --> P I --> Q J --> N J --> O K --> N K --> O style A fill:#c8e6c9 style G fill:#ffecb3 style F fill:#e1f5ff style I fill:#fff3e0","title":"Architecture"},{"location":"virtualization/qemu/#core-components","text":"","title":"Core Components"},{"location":"virtualization/qemu/#vcpu-virtual-cpu","text":"Threads executing guest code in virtual CPU context. VCPU States : | State | Description | |-------|-------------| | Running | Currently executing guest code | | Blocked | Waiting for I/O or sleep | | Exited | VCPU exited | VCPU Configuration : # Set VCPU count qemu-system-x86_64 -smp 4 # Set CPU topology qemu-system-x86_64 -smp 4,sockets=2,cores=2,threads=1 # Pin VCPUs to physical CPUs qemu-system-x86_64 -smp 4 -numa node,memdev=mem","title":"VCPU (Virtual CPU)"},{"location":"virtualization/qemu/#tcg-tiny-code-generator","text":"Dynamic binary translation engine for software emulation. TCG Operation : graph LR A[Guest Binary Code] --> B[TCG Translator] B --> C[Host Code Block] C --> D[Translation Cache] D --> E[Code Execution] F[Cache Miss] --> B E --> G[Execution Result] style B fill:#c8e6c9 style D fill:#ffecb3 style E fill:#e1f5ff TCG Features : - Dynamic translation - Block chaining - Code cache management - Cross-architectural emulation - No hardware requirements TCG Performance : - Slower than KVM (~10-20% host speed) - Cross-platform compatibility - Good for non-virtualized hosts","title":"TCG (Tiny Code Generator)"},{"location":"virtualization/qemu/#kvm-acceleration","text":"Hardware virtualization using Intel VT-x or AMD-V. KVM Operation : sequenceDiagram participant Q as QEMU participant K as KVM Module participant G as Guest Q->>K: Create VM K->>K: Allocate resources Q->>G: Start VCPU G->>G: Execute guest code G->>K: VM Exit (I/O, interrupt) K->>Q: Handle exit Q->>K: Resume guest style K fill:#c8e6c9 style G fill:#e1f5ff KVM Features : - Native CPU execution - Near-native performance (~95-98% host speed) - Virtualization extensions required - Paravirtualized I/O support VM Exit Reasons : | Reason | Description | Frequency | |--------|-------------|----------| | I/O Port Access | Guest accesses PIO | High | | MMIO Access | Guest accesses MMIO | Medium | | Interrupt | Hardware interrupt | High | | Interrupt Window | Interrupt delivery needed | Medium | | HLT | CPU halted | Low | | CR Access | Control register access | Low |","title":"KVM Acceleration"},{"location":"virtualization/qemu/#io-thread","text":"Dedicated thread for I/O operations to improve performance. IO Thread Benefits : - Non-blocking I/O for VCPUs - Improved I/O throughput - Better concurrency IO Thread Configuration : # Enable IO thread qemu-system-x86_64 -object iothread,id=io1 \\ -device virtio-blk-pci,iothread=io1 # Multiple IO threads qemu-system-x86_64 \\ -object iothread,id=io1 \\ -object iothread,id=io2 \\ -device virtio-blk-pci,iothread=io1 \\ -device virtio-blk-pci,iothread=io2","title":"IO Thread"},{"location":"virtualization/qemu/#device-emulation","text":"","title":"Device Emulation"},{"location":"virtualization/qemu/#virtio-devices","text":"Paravirtualized devices for optimal performance. Virtio Architecture : graph TB A[Guest Driver] --> B[Virtqueue] B --> C[Virtio Ring] C --> D[Host Device] E[Notification] --> B F[Completion] --> B style B fill:#c8e6c9 style D fill:#e1f5ff Virtio Device Types : | Device | Description | Performance | |--------|-------------|------------| | virtio-blk | Block device | Excellent | | virtio-net | Network device | Excellent | | virtio-scsi | SCSI controller | Very Good | | virtio-serial | Serial port | Good | | virtio-gpu | Graphics | Good | | virtio-rng | Random number | Good | Virtio Features : - Zero-copy I/O - Direct guest memory access - Efficient notification mechanism - Queue-based I/O","title":"Virtio Devices"},{"location":"virtualization/qemu/#network-devices","text":"","title":"Network Devices"},{"location":"virtualization/qemu/#virtio-network","text":"# Basic virtio network qemu-system-x86_64 -net nic,model=virtio \\ -net user,hostfwd=tcp::2222-:22 # Multiqueue virtio qemu-system-x86_64 -net nic,model=virtio,vectors=6 \\ -net tap,ifname=tap0,queues=4,vhost=on # Bridge network qemu-system-x86_64 -net nic,model=virtio \\ -net tap,script=/etc/qemu-ifup,downscript=/etc/qemu-ifdown Virtio Network Features : - Multiple queues (virtio multiqueue) - Vhost support (kernel-space driver) - Zero-copy transmission - Large packet offload Vhost Configuration : # Enable vhost-net qemu-system-x86_64 -net nic,model=virtio \\ -net tap,vhost=on,vhostforce=on # Multiple vhost queues qemu-system-x86_64 -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on","title":"Virtio Network"},{"location":"virtualization/qemu/#storage-devices","text":"","title":"Storage Devices"},{"location":"virtualization/qemu/#virtio-block","text":"# Virtio block device qemu-system-x86_64 -drive file=disk.qcow2,if=virtio,format=qcow2 # Virtio block with cache qemu-system-x86_64 \\ -drive file=disk.qcow2,if=virtio,format=qcow2,cache=writeback # Virtio block with multiple queues qemu-system-x86_64 \\ -object iothread,id=io1 \\ -drive file=disk.qcow2,if=virtio,format=qcow2,iothread=io1 Virtio Block Features : - Direct I/O support - Multiple queues with IO thread - Write barriers - Discard support - Write-zeroes support","title":"Virtio Block"},{"location":"virtualization/qemu/#virtio-scsi","text":"# Virtio SCSI qemu-system-x86_64 \\ -device virtio-scsi-pci \\ -drive file=disk1.qcow2,if=none,id=drive1 \\ -device scsi-hd,drive=drive1 # Multiple SCSI devices qemu-system-x86_64 \\ -device virtio-scsi-pci \\ -drive file=disk1.qcow2,if=none,id=drive1 \\ -device scsi-hd,drive=drive1 \\ -drive file=disk2.qcow2,if=none,id=drive2 \\ -device scsi-hd,drive=drive2 Virtio SCSI Features : - Multiple devices per controller - SCSI command support - Persistent reservations - Better than virtio-blk for many disks","title":"Virtio SCSI"},{"location":"virtualization/qemu/#io-paths","text":"","title":"I/O Paths"},{"location":"virtualization/qemu/#mmio-memory-mapped-io","text":"Device registers mapped to guest memory space. MMIO Access Flow : graph LR A[Guest CPU] --> B[Guest Address Space] B --> C[MMIO Region] C --> D[MMIO Handler] D --> E[Device Emulation] F[VM Exit] --> D E --> G[I/O Completion] style C fill:#c8e6c9 style E fill:#e1f5ff MMIO Characteristics : - Memory-mapped registers - Triggers VM exit on access - Used for device configuration - Slower than DMA","title":"MMIO (Memory Mapped I/O)"},{"location":"virtualization/qemu/#pio-port-io","text":"Legacy x86 I/O port access. PIO Characteristics : - Legacy x86 I/O ports - Triggers VM exit on IN/OUT - Used for legacy device access - Slower than MMIO - Limited to 64K ports PIO Example : # Legacy IDE device qemu-system-x86_64 \\ -drive file=disk.img,if=ide # Legacy network (e1000) qemu-system-x86_64 \\ -net nic,model=e1000","title":"PIO (Port I/O)"},{"location":"virtualization/qemu/#dma-direct-memory-access","text":"Direct guest memory access for I/O operations. DMA Operation : graph TB A[Guest Device] --> B[DMA Request] B --> C[Guest Physical Address] C --> D[IOMMU Translation] D --> E[Host Physical Address] E --> F[Host Device] F --> G[Data Transfer] G --> H[Completion Interrupt] style D fill:#c8e6c9 style F fill:#e1f5ff DMA Features : - Direct guest memory access - High performance - IOMMU support for isolation - Used by virtio devices","title":"DMA (Direct Memory Access)"},{"location":"virtualization/qemu/#irq-interrupt-request","text":"Hardware interrupt delivery to guest. IRQ Flow : sequenceDiagram participant G as Guest participant Q as QEMU participant K as KVM G->>Q: Complete I/O Q->>K: Inject IRQ K->>G: Deliver interrupt G->>G: Handle interrupt style K fill:#c8e6c9 style G fill:#e1f5ff IRQ Types : - PIC : Legacy 8259 PIC - IOAPIC : Advanced programmable interrupt controller - MSI : Message signaled interrupts - MSI-X : Enhanced MSI MSI Configuration : # Enable MSI-X qemu-system-x86_64 \\ -device virtio-net-pci,msix=on # Multiple MSI-X vectors qemu-system-x86_64 \\ -device virtio-net-pci,msix=on,vectors=6","title":"IRQ (Interrupt Request)"},{"location":"virtualization/qemu/#quick-commands","text":"","title":"Quick Commands"},{"location":"virtualization/qemu/#basic-vm-creation","text":"# Simple VM qemu-system-x86_64 \\ -name vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 \\ -netdev user,id=net0,hostfwd=tcp::2222-:22 # VM with graphics qemu-system-x86_64 \\ -name vm1 \\ -m 2048 \\ -smp 2 \\ -drive file=disk.qcow2,format=qcow2 \\ -vnc :1 # VM with KVM qemu-system-x86_64 -enable-kvm \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,format=qcow2","title":"Basic VM Creation"},{"location":"virtualization/qemu/#advanced-configuration","text":"# CPU configuration qemu-system-x86_64 \\ -cpu host \\ -smp 4,sockets=2,cores=2,threads=1 # Memory configuration qemu-system-x86_64 \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # NUMA topology qemu-system-x86_64 \\ -m 4096 \\ -numa node,memdev=mem \\ -object memory-backend-file,id=mem,size=4G,mem-path=/dev/hugepages,share=on","title":"Advanced Configuration"},{"location":"virtualization/qemu/#network-configuration","text":"# User networking qemu-system-x86_64 \\ -netdev user,id=net0,hostfwd=tcp::2222-:22 # TAP networking qemu-system-x86_64 \\ -netdev tap,id=net0,ifname=tap0,script=no,downscript=no # Bridge networking qemu-system-x86_64 \\ -netdev bridge,id=net0,br=br0 # Virtio multiqueue qemu-system-x86_64 \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on","title":"Network Configuration"},{"location":"virtualization/qemu/#storage-configuration","text":"# Qcow2 image qemu-system-x86_64 \\ -drive file=disk.qcow2,format=qcow2,if=virtio # Raw image qemu-system-x86_64 \\ -drive file=disk.img,format=raw,if=virtio # Multiple disks qemu-system-x86_64 \\ -drive file=disk1.qcow2,if=virtio \\ -drive file=disk2.qcow2,if=virtio # CD-ROM qemu-system-x86_64 \\ -drive file=ubuntu.iso,media=cdrom","title":"Storage Configuration"},{"location":"virtualization/qemu/#nifty-behaviors","text":"","title":"Nifty Behaviors"},{"location":"virtualization/qemu/#qmp-programmatic-control","text":"# Start QEMU with QMP socket qemu-system-x86_64 -qmp tcp:localhost:4444,server,nowait # Connect to QMP echo '{\"execute\":\"qmp_capabilities\"}' | \\ socat TCP:localhost:4444 - # Query status echo '{\"execute\":\"query-status\"}' | \\ socat TCP:localhost:4444 - # Start VM echo '{\"execute\":\"cont\"}' | \\ socat TCP:localhost:4444 - Nifty : Programmatic control of running VMs","title":"QMP Programmatic Control"},{"location":"virtualization/qemu/#virtio-multiqueue","text":"# Create VM with multiqueue qemu-system-x86_64 \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on # Libvirt XML <interface type='network'> <model type='virtio'/> <driver name='vhost' queues='4'/> </interface> Nifty : Multi-queue network, better throughput","title":"Virtio Multiqueue"},{"location":"virtualization/qemu/#memory-ballooning","text":"# Attach balloon device qemu-system-x86_64 \\ -device virtio-balloon-pci # Adjust memory via QMP echo '{\"execute\":\"balloon\",\"arguments\":{\"size\":1073741824}}' | \\ socat TCP:localhost:4444 - # Libvirt balloon virsh setmem vm1 2G virsh setmem vm1 4G Nifty : Dynamically adjust VM memory","title":"Memory Ballooning"},{"location":"virtualization/qemu/#live-migration","text":"# Source VM qemu-system-x86_64 \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,if=virtio \\ -incoming tcp:0:4444 # Destination VM qemu-system-x86_64 \\ -name vm1 \\ -m 4096 \\ -smp 4 \\ -drive file=disk.qcow2,if=virtio \\ -monitor stdio # Trigger migration (in destination monitor) migrate -tcp:source:4444 Nifty : Zero-downtime VM migration","title":"Live Migration"},{"location":"virtualization/qemu/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"virtualization/qemu/#cpu-optimization","text":"# Host CPU passthrough qemu-system-x86_64 -cpu host # CPU features qemu-system-x86_64 -cpu host,+vmx,+pdpe1gb # CPU pinning qemu-system-x86_64 -smp 4 -numa node,memdev=mem","title":"CPU Optimization"},{"location":"virtualization/qemu/#memory-optimization","text":"# Hugepages qemu-system-x86_64 \\ -m 4096 \\ -mem-path /dev/hugepages \\ -mem-prealloc # KSM (Kernel Same-page Merging) echo 1 > /sys/kernel/mm/ksm/run echo 1000 > /sys/kernel/mm/ksm/pages_to_scan # Memory balloon qemu-system-x86_64 -device virtio-balloon-pci","title":"Memory Optimization"},{"location":"virtualization/qemu/#io-optimization","text":"# IO thread qemu-system-x86_64 \\ -object iothread,id=io1 \\ -drive file=disk.qcow2,if=virtio,iothread=io1 # Multiple IO threads qemu-system-x86_64 \\ -object iothread,id=io1 \\ -object iothread,id=io2 \\ -drive file=disk1.qcow2,if=virtio,iothread=io1 \\ -drive file=disk2.qcow2,if=virtio,iothread=io2 # Cache options qemu-system-x86_64 \\ -drive file=disk.qcow2,if=virtio,cache=writeback,async=on # Native AIO qemu-system-x86_64 \\ -drive file=disk.img,if=virtio,aio=native","title":"I/O Optimization"},{"location":"virtualization/qemu/#network-optimization","text":"# Vhost-net qemu-system-x86_64 \\ -net nic,model=virtio \\ -net tap,vhost=on # Multiple queues qemu-system-x86_64 \\ -net nic,model=virtio,vectors=6 \\ -net tap,queues=4,vhost=on # Large receive offload qemu-system-x86_64 \\ -net nic,model=virtio \\ -net tap,gro=on","title":"Network Optimization"},{"location":"virtualization/qemu/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"virtualization/qemu/#performance-issues","text":"# Check if KVM is enabled qemu-system-x86_64 -accel kvm -version # Check VCPU load top -H -p $(pidof qemu-system-x86_64) # Check I/O iostat -x 1 # Check network sar -n DEV 1","title":"Performance Issues"},{"location":"virtualization/qemu/#migration-issues","text":"# Check migration status echo '{\"execute\":\"query-migrate\"}' | \\ socat TCP:localhost:4444 - # Cancel migration echo '{\"execute\":\"migrate_cancel\"}' | \\ socat TCP:localhost:4444 - # Check compatibility echo '{\"execute\":\"query-migrate-capabilities\"}' | \\ socat TCP:localhost:4444 -","title":"Migration Issues"},{"location":"virtualization/qemu/#best-practices","text":"Always use KVM for x86 guests on supported hardware Use virtio devices for optimal performance Enable IO threads for storage-intensive workloads Use hugepages for memory-intensive workloads Enable vhost-net for network performance Configure NUMA for large VMs Use cache=writeback for better disk performance Test migration in production environment Monitor VM performance regularly Use snapshots for backup/rollback","title":"Best Practices"},{"location":"virtualization/qemu/#source-code","text":"Repository : https://gitlab.com/qemu-project/qemu Documentation : https://www.qemu.org/documentation/","title":"Source Code"},{"location":"virtualization/qemu/#key-source-locations","text":"Component Location Description Main loop cpus.c CPU execution loop TCG tcg/ Tiny Code Generator KVM accel/kvm/ KVM acceleration Memory exec.c Memory management I/O ioport.c Port I/O MMIO memory.c Memory Mapped I/O Virtio hw/virtio/ Virtio devices Block block/ Block devices Network net/ Network devices","title":"Key Source Locations"},{"location":"virtualization/virsh/","text":"Command-line interface for managing libvirt virtualization with comprehensive VM lifecycle management. Overview virsh provides a unified interface for managing virtual machines, networks, storage, and other libvirt resources. virsh Connection Modes : # Local connection virsh # Remote connection virsh -c qemu+ssh://user@host/system virsh -c qemu+tcp://host/system virsh -c qemu+tls://host/system # Read-only connection virsh -r Domain Lifecycle Management VM Creation # Create VM from XML virsh define vm1.xml # Create VM from ISO (interactive) virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path=/var/lib/libvirt/images/vm1.qcow2,size=20 \\ --cdrom /path/to/ubuntu.iso \\ --graphics vnc # Clone existing VM virt-clone --original vm1 --name vm2 --file /var/lib/libvirt/images/vm2.qcow2 VM Start/Stop # Start VM virsh start vm1 # Start VM with autostart virsh start vm1 --console # Shutdown VM (graceful) virsh shutdown vm1 # Shutdown with force virsh shutdown vm1 --mode=force # Destroy VM (force stop) virsh destroy vm1 # Reboot VM virsh reboot vm1 # Reset VM (hardware reset) virsh reset vm1 # Suspend VM virsh suspend vm1 # Resume VM virsh resume vm1 VM Deletion # Undefine VM (remove configuration) virsh undefine vm1 # Undefine with storage virsh undefine vm1 --remove-all-storage # Undefine with snapshots virsh undefine vm1 --remove-all-storage --snapshots-metadata # Undefine with NVRAM virsh undefine vm1 --nvram VM Autostart # Enable autostart virsh autostart vm1 # Disable autostart virsh autostart --disable vm1 # List autostart VMs virsh list --autostart VM Information and Monitoring Domain Information # Domain information virsh dominfo vm1 # Domain ID and UUID virsh domid vm1 virsh domuuid vm1 # Domain state virsh domstate vm1 # Domain flags virsh domcontrol vm1 # Domain time virsh domtime vm1 CPU Information # VCPU information virsh vcpuinfo vm1 # VCPU pinning virsh vcpupin vm1 # Set VCPU pinning virsh vcpupin vm1 0 0-1 virsh vcpupin vm1 1 2-3 # VCPU count virsh vcpuinfo vm1 virsh setvcpus vm1 4 --live virsh setvcpus vm1 4 --config virsh setvcpus vm1 4 --current # Maximum VCPU virsh maxvcpus vm1 Memory Information # Memory statistics virsh dommemstat vm1 # Set memory virsh setmem vm1 2G --live virsh setmem vm1 2G --config virsh setmem vm1 2G --current # Maximum memory virsh setmaxmem vm1 4G --live # Memory balloon virsh dommemstat vm1 | grep actual Device Information # Block devices virsh domblklist vm1 virsh domblkinfo vm1 vda # Network devices virsh domiflist vm1 virsh domifstat vm1 vnet0 # All devices virsh dumpxml vm1 | grep '<disk' virsh dumpxml vm1 | grep '<interface' Performance Statistics # Block statistics virsh domblkstat vm1 # Network statistics virsh domifstat vm1 vnet0 # CPU time virsh cpu-stats vm1 # CPU usage (total) virsh cpu-stats vm1 --total # CPU usage (per vcpu) virsh cpu-stats vm1 VM Configuration XML Configuration # Dump XML configuration virsh dumpxml vm1 # Dump XML to file virsh dumpxml vm1 > vm1.xml # Edit XML configuration virsh edit vm1 # Edit specific network interface virsh net-edit default # Validate XML virsh define vm1.xml Device Management # Attach device virsh attach-device vm1 disk.xml # Attach device from command virsh attach-device vm1 --file disk.xml --persistent # Detach device virsh detach-device vm1 disk.xml # Update device virsh update-device vm1 disk.xml # List devices virsh domblklist vm1 virsh domiflist vm1 Disk Device XML : <disk type='file' device='disk'> <driver name='qemu' type='qcow2'/> <source file='/var/lib/libvirt/images/disk.qcow2'/> <target dev='vdb' bus='virtio'/> </disk> Network Device XML : <interface type='network'> <mac address='52:54:00:71:b1:b6'/> <source network='default'/> <model type='virtio'/> </interface> Console Access Serial Console # Connect to serial console virsh console vm1 # Connect to specific console virsh console vm1 --devname hvc0 # Force console connection virsh console vm1 --force VNC Console # Find VNC display virsh vncdisplay vm1 # Find VNC port virsh qemu-monitor-command vm1 \"info vnc\" QEMU Monitor # QEMU monitor command virsh qemu-monitor-command vm1 \"info status\" virsh qemu-monitor-command vm1 \"info cpus\" virsh qemu-monitor-command vm1 \"info block\" virsh qemu-monitor-command vm1 \"info network\" # QMP command virsh qemu-monitor-command vm1 '{\"execute\":\"query-status\"}' Snapshot Management Creating Snapshots # Create snapshot virsh snapshot-create vm1 # Create snapshot with name virsh snapshot-create-as vm1 snap1 # Create snapshot from XML virsh snapshot-create vm1 snap.xml # Create snapshot with description virsh snapshot-create-as vm1 snap1 \\ --description \"Snapshot before update\" Snapshot XML : <domainsnapshot> <name>snap1</name> <description>Snapshot before update</description> <memory snapshot='no'/> <disks> <disk name='vda' snapshot='internal'/> </disks> </domainsnapshot> Managing Snapshots # List snapshots virsh snapshot-list vm1 # List snapshots with tree virsh snapshot-list vm1 --tree # Snapshot information virsh snapshot-info vm1 snap1 # Dump snapshot XML virsh snapshot-dumpxml vm1 snap1 # Snapshot parent virsh snapshot-parent vm1 snap1 Reverting Snapshots # Revert to snapshot virsh snapshot-revert vm1 snap1 # Revert with force virsh snapshot-revert vm1 snap1 --force # Revert current state virsh snapshot-revert vm1 --current Deleting Snapshots # Delete snapshot virsh snapshot-delete vm1 snap1 # Delete all snapshots virsh snapshot-list vm1 --name | xargs -I {} virsh snapshot-delete vm1 {} Migration Live Migration # Live migration virsh migrate --live vm1 qemu+ssh://dest/system # Live migration with compression virsh migrate --live --compress vm1 qemu+ssh://dest/system # Live migration with timeout virsh migrate --live --timeout 600 vm1 qemu+ssh://dest/system # Live migration with bandwidth limit virsh migrate --live --bandwidth 100 vm1 qemu+ssh://dest/system Cold Migration # Cold migration virsh migrate vm1 qemu+ssh://dest/system # Cold migration with XML virsh migrate --xml vm1.xml qemu+ssh://dest/system Migration Options Option Description --live Live migration --p2p Peer-to-peer migration --direct Direct migration --tunnelled Tunnelled migration --persistent Persist on destination --undefinesource Undefine source --persistent Make destination persistent --suspend Suspend on migration --copy-storage-all Copy all storage --copy-storage-inc Copy storage incrementally --compress Compress migration data --abort-on-error Abort on errors --timeout Set timeout Migration Monitoring # Monitor migration virsh migrate --live --verbose vm1 qemu+ssh://dest/system # Monitor QEMU monitor virsh qemu-monitor-command vm1 \"info migrate\" # Cancel migration virsh migrate vm1 --abort Network Management Network Operations # List networks virsh net-list virsh net-list --all virsh net-list --inactive virsh net-list --active # Network information virsh net-info default # Dump network XML virsh net-dumpxml default # Define network virsh net-define network.xml # Undefine network virsh net-undefine default # Start network virsh net-start default # Stop network virsh net-stop default # Network autostart virsh net-autostart default virsh net-autostart --disable default Network XML : <network> <name>default</name> <forward mode='nat'/> <bridge name='virbr0' stp='on' delay='0'/> <ip address='192.168.122.1' netmask='255.255.255.0'> <dhcp> <range start='192.168.122.2' end='192.168.122.254'/> </dhcp> </ip> </network> Storage Management Storage Pool Operations # List storage pools virsh pool-list virsh pool-list --all virsh pool-list --inactive virsh pool-list --active # Pool information virsh pool-info default # Dump pool XML virsh pool-dumpxml default # Define pool virsh pool-define pool.xml # Undefine pool virsh pool-undefine default # Build pool virsh pool-build default # Start pool virsh pool-start default # Stop pool virsh pool-stop default # Pool autostart virsh pool-autostart default virsh pool-autostart --disable default Storage Pool XML : <pool type='dir'> <name>default</name> <target> <path>/var/lib/libvirt/images</path> <permissions> <mode>0755</mode> <owner>-1</owner> <group>-1</group> </permissions> </target> </pool> Storage Volume Operations # List volumes virsh vol-list default # Volume information virsh vol-info vm1.qcow2 # Dump volume XML virsh vol-dumpxml vm1.qcow2 # Create volume virsh vol-create-as default vm1.qcow2 20G # Create volume from XML virsh vol-create default volume.xml # Delete volume virsh vol-delete default vm1.qcow2 # Upload to volume virsh vol-upload default vm1.qcow2 disk.qcow2 # Download from volume virsh vol-download default vm1.qcow2 disk.qcow2 # Clone volume virsh vol-clone default vm1.qcow2 vm2.qcow2 # Wipe volume virsh vol-wipe default vm1.qcow2 # Resize volume virsh vol-resize default vm1.qcow2 30G Secret Management Secret Operations # Define secret cat > secret.xml <<EOF <secret ephemeral='no' private='no'> <description>libvirt secret</description> <usage type='ceph'> <name>client.admin secret</name> </usage> </secret> EOF virsh secret-define secret.xml # Set secret value virsh secret-set-value secret.xml /path/to/key # Get secret value virsh secret-get-value secret.xml # Delete secret virsh secret-undefine secret.xml # List secrets virsh secret-list Advanced Commands Guest Operations # Guest agent command virsh qemu-agent-command vm1 '{\"execute\":\"guest-ping\"}' # Guest shutdown virsh qemu-agent-command vm1 '{\"execute\":\"guest-shutdown\",\"arguments\":{\"mode\":\"poweroff\"}}' # Guest info virsh qemu-agent-command vm1 '{\"execute\":\"guest-info\"}' Host Management # Node info virsh nodeinfo # Host capabilities virsh capabilities # CPU models virsh cpu-models # Hostname virsh hostname # Sysinfo virsh sysinfo Version Information # Version virsh version # Compiled information virsh version # Daemon version virsh version --daemon Troubleshooting VM Issues # Check VM logs tail -f /var/log/libvirt/qemu/vm1.log # Check libvirt logs tail -f /var/log/libvirt/libvirtd.log # Check VM status virsh domstate vm1 virsh domcontrol vm1 # Check XML configuration virsh dumpxml vm1 # Check console virsh console vm1 Network Issues # Check network status virsh net-list --all # Check network XML virsh net-dumpxml default # Check network logs journalctl -u libvirtd -f # Check bridge brctl show ip addr show virbr0 Storage Issues # Check pool status virsh pool-list --all # Check pool XML virsh pool-dumpxml default # Check volume status virsh vol-list default # Check storage logs journalctl -u libvirtd -f Best Practices Use --persistent for permanent changes Create snapshots before major changes Test migration before production Use proper storage for volumes Monitor VMs regularly Use autostart for critical VMs Document configurations with XML Test backups regularly Use virsh edit for configuration changes Check logs for troubleshooting Quick Reference Common Commands Command Description virsh list List running VMs virsh list --all List all VMs virsh start vm1 Start VM virsh shutdown vm1 Shutdown VM virsh destroy vm1 Force stop VM virsh reboot vm1 Reboot VM virsh suspend vm1 Suspend VM virsh resume vm1 Resume VM virsh define vm1.xml Define VM virsh undefine vm1 Remove VM virsh dumpxml vm1 Export XML virsh edit vm1 Edit XML virsh dominfo vm1 VM information virsh domstate vm1 VM state virsh console vm1 Connect console virsh snapshot-create vm1 Create snapshot virsh snapshot-list vm1 List snapshots virsh snapshot-revert vm1 snap1 Revert snapshot virsh migrate --live vm1 dest Live migration Documentation virsh Manual : man virsh libvirt Documentation : https://libvirt.org/docs/ virsh Cheat Sheet : https://libvirt.org/virshcmdref/","title":"virsh"},{"location":"virtualization/virsh/#overview","text":"virsh provides a unified interface for managing virtual machines, networks, storage, and other libvirt resources. virsh Connection Modes : # Local connection virsh # Remote connection virsh -c qemu+ssh://user@host/system virsh -c qemu+tcp://host/system virsh -c qemu+tls://host/system # Read-only connection virsh -r","title":"Overview"},{"location":"virtualization/virsh/#domain-lifecycle-management","text":"","title":"Domain Lifecycle Management"},{"location":"virtualization/virsh/#vm-creation","text":"# Create VM from XML virsh define vm1.xml # Create VM from ISO (interactive) virt-install --name vm1 --memory 2048 --vcpus 2 \\ --disk path=/var/lib/libvirt/images/vm1.qcow2,size=20 \\ --cdrom /path/to/ubuntu.iso \\ --graphics vnc # Clone existing VM virt-clone --original vm1 --name vm2 --file /var/lib/libvirt/images/vm2.qcow2","title":"VM Creation"},{"location":"virtualization/virsh/#vm-startstop","text":"# Start VM virsh start vm1 # Start VM with autostart virsh start vm1 --console # Shutdown VM (graceful) virsh shutdown vm1 # Shutdown with force virsh shutdown vm1 --mode=force # Destroy VM (force stop) virsh destroy vm1 # Reboot VM virsh reboot vm1 # Reset VM (hardware reset) virsh reset vm1 # Suspend VM virsh suspend vm1 # Resume VM virsh resume vm1","title":"VM Start/Stop"},{"location":"virtualization/virsh/#vm-deletion","text":"# Undefine VM (remove configuration) virsh undefine vm1 # Undefine with storage virsh undefine vm1 --remove-all-storage # Undefine with snapshots virsh undefine vm1 --remove-all-storage --snapshots-metadata # Undefine with NVRAM virsh undefine vm1 --nvram","title":"VM Deletion"},{"location":"virtualization/virsh/#vm-autostart","text":"# Enable autostart virsh autostart vm1 # Disable autostart virsh autostart --disable vm1 # List autostart VMs virsh list --autostart","title":"VM Autostart"},{"location":"virtualization/virsh/#vm-information-and-monitoring","text":"","title":"VM Information and Monitoring"},{"location":"virtualization/virsh/#domain-information","text":"# Domain information virsh dominfo vm1 # Domain ID and UUID virsh domid vm1 virsh domuuid vm1 # Domain state virsh domstate vm1 # Domain flags virsh domcontrol vm1 # Domain time virsh domtime vm1","title":"Domain Information"},{"location":"virtualization/virsh/#cpu-information","text":"# VCPU information virsh vcpuinfo vm1 # VCPU pinning virsh vcpupin vm1 # Set VCPU pinning virsh vcpupin vm1 0 0-1 virsh vcpupin vm1 1 2-3 # VCPU count virsh vcpuinfo vm1 virsh setvcpus vm1 4 --live virsh setvcpus vm1 4 --config virsh setvcpus vm1 4 --current # Maximum VCPU virsh maxvcpus vm1","title":"CPU Information"},{"location":"virtualization/virsh/#memory-information","text":"# Memory statistics virsh dommemstat vm1 # Set memory virsh setmem vm1 2G --live virsh setmem vm1 2G --config virsh setmem vm1 2G --current # Maximum memory virsh setmaxmem vm1 4G --live # Memory balloon virsh dommemstat vm1 | grep actual","title":"Memory Information"},{"location":"virtualization/virsh/#device-information","text":"# Block devices virsh domblklist vm1 virsh domblkinfo vm1 vda # Network devices virsh domiflist vm1 virsh domifstat vm1 vnet0 # All devices virsh dumpxml vm1 | grep '<disk' virsh dumpxml vm1 | grep '<interface'","title":"Device Information"},{"location":"virtualization/virsh/#performance-statistics","text":"# Block statistics virsh domblkstat vm1 # Network statistics virsh domifstat vm1 vnet0 # CPU time virsh cpu-stats vm1 # CPU usage (total) virsh cpu-stats vm1 --total # CPU usage (per vcpu) virsh cpu-stats vm1","title":"Performance Statistics"},{"location":"virtualization/virsh/#vm-configuration","text":"","title":"VM Configuration"},{"location":"virtualization/virsh/#xml-configuration","text":"# Dump XML configuration virsh dumpxml vm1 # Dump XML to file virsh dumpxml vm1 > vm1.xml # Edit XML configuration virsh edit vm1 # Edit specific network interface virsh net-edit default # Validate XML virsh define vm1.xml","title":"XML Configuration"},{"location":"virtualization/virsh/#device-management","text":"# Attach device virsh attach-device vm1 disk.xml # Attach device from command virsh attach-device vm1 --file disk.xml --persistent # Detach device virsh detach-device vm1 disk.xml # Update device virsh update-device vm1 disk.xml # List devices virsh domblklist vm1 virsh domiflist vm1 Disk Device XML : <disk type='file' device='disk'> <driver name='qemu' type='qcow2'/> <source file='/var/lib/libvirt/images/disk.qcow2'/> <target dev='vdb' bus='virtio'/> </disk> Network Device XML : <interface type='network'> <mac address='52:54:00:71:b1:b6'/> <source network='default'/> <model type='virtio'/> </interface>","title":"Device Management"},{"location":"virtualization/virsh/#console-access","text":"","title":"Console Access"},{"location":"virtualization/virsh/#serial-console","text":"# Connect to serial console virsh console vm1 # Connect to specific console virsh console vm1 --devname hvc0 # Force console connection virsh console vm1 --force","title":"Serial Console"},{"location":"virtualization/virsh/#vnc-console","text":"# Find VNC display virsh vncdisplay vm1 # Find VNC port virsh qemu-monitor-command vm1 \"info vnc\"","title":"VNC Console"},{"location":"virtualization/virsh/#qemu-monitor","text":"# QEMU monitor command virsh qemu-monitor-command vm1 \"info status\" virsh qemu-monitor-command vm1 \"info cpus\" virsh qemu-monitor-command vm1 \"info block\" virsh qemu-monitor-command vm1 \"info network\" # QMP command virsh qemu-monitor-command vm1 '{\"execute\":\"query-status\"}'","title":"QEMU Monitor"},{"location":"virtualization/virsh/#snapshot-management","text":"","title":"Snapshot Management"},{"location":"virtualization/virsh/#creating-snapshots","text":"# Create snapshot virsh snapshot-create vm1 # Create snapshot with name virsh snapshot-create-as vm1 snap1 # Create snapshot from XML virsh snapshot-create vm1 snap.xml # Create snapshot with description virsh snapshot-create-as vm1 snap1 \\ --description \"Snapshot before update\" Snapshot XML : <domainsnapshot> <name>snap1</name> <description>Snapshot before update</description> <memory snapshot='no'/> <disks> <disk name='vda' snapshot='internal'/> </disks> </domainsnapshot>","title":"Creating Snapshots"},{"location":"virtualization/virsh/#managing-snapshots","text":"# List snapshots virsh snapshot-list vm1 # List snapshots with tree virsh snapshot-list vm1 --tree # Snapshot information virsh snapshot-info vm1 snap1 # Dump snapshot XML virsh snapshot-dumpxml vm1 snap1 # Snapshot parent virsh snapshot-parent vm1 snap1","title":"Managing Snapshots"},{"location":"virtualization/virsh/#reverting-snapshots","text":"# Revert to snapshot virsh snapshot-revert vm1 snap1 # Revert with force virsh snapshot-revert vm1 snap1 --force # Revert current state virsh snapshot-revert vm1 --current","title":"Reverting Snapshots"},{"location":"virtualization/virsh/#deleting-snapshots","text":"# Delete snapshot virsh snapshot-delete vm1 snap1 # Delete all snapshots virsh snapshot-list vm1 --name | xargs -I {} virsh snapshot-delete vm1 {}","title":"Deleting Snapshots"},{"location":"virtualization/virsh/#migration","text":"","title":"Migration"},{"location":"virtualization/virsh/#live-migration","text":"# Live migration virsh migrate --live vm1 qemu+ssh://dest/system # Live migration with compression virsh migrate --live --compress vm1 qemu+ssh://dest/system # Live migration with timeout virsh migrate --live --timeout 600 vm1 qemu+ssh://dest/system # Live migration with bandwidth limit virsh migrate --live --bandwidth 100 vm1 qemu+ssh://dest/system","title":"Live Migration"},{"location":"virtualization/virsh/#cold-migration","text":"# Cold migration virsh migrate vm1 qemu+ssh://dest/system # Cold migration with XML virsh migrate --xml vm1.xml qemu+ssh://dest/system","title":"Cold Migration"},{"location":"virtualization/virsh/#migration-options","text":"Option Description --live Live migration --p2p Peer-to-peer migration --direct Direct migration --tunnelled Tunnelled migration --persistent Persist on destination --undefinesource Undefine source --persistent Make destination persistent --suspend Suspend on migration --copy-storage-all Copy all storage --copy-storage-inc Copy storage incrementally --compress Compress migration data --abort-on-error Abort on errors --timeout Set timeout","title":"Migration Options"},{"location":"virtualization/virsh/#migration-monitoring","text":"# Monitor migration virsh migrate --live --verbose vm1 qemu+ssh://dest/system # Monitor QEMU monitor virsh qemu-monitor-command vm1 \"info migrate\" # Cancel migration virsh migrate vm1 --abort","title":"Migration Monitoring"},{"location":"virtualization/virsh/#network-management","text":"","title":"Network Management"},{"location":"virtualization/virsh/#network-operations","text":"# List networks virsh net-list virsh net-list --all virsh net-list --inactive virsh net-list --active # Network information virsh net-info default # Dump network XML virsh net-dumpxml default # Define network virsh net-define network.xml # Undefine network virsh net-undefine default # Start network virsh net-start default # Stop network virsh net-stop default # Network autostart virsh net-autostart default virsh net-autostart --disable default Network XML : <network> <name>default</name> <forward mode='nat'/> <bridge name='virbr0' stp='on' delay='0'/> <ip address='192.168.122.1' netmask='255.255.255.0'> <dhcp> <range start='192.168.122.2' end='192.168.122.254'/> </dhcp> </ip> </network>","title":"Network Operations"},{"location":"virtualization/virsh/#storage-management","text":"","title":"Storage Management"},{"location":"virtualization/virsh/#storage-pool-operations","text":"# List storage pools virsh pool-list virsh pool-list --all virsh pool-list --inactive virsh pool-list --active # Pool information virsh pool-info default # Dump pool XML virsh pool-dumpxml default # Define pool virsh pool-define pool.xml # Undefine pool virsh pool-undefine default # Build pool virsh pool-build default # Start pool virsh pool-start default # Stop pool virsh pool-stop default # Pool autostart virsh pool-autostart default virsh pool-autostart --disable default Storage Pool XML : <pool type='dir'> <name>default</name> <target> <path>/var/lib/libvirt/images</path> <permissions> <mode>0755</mode> <owner>-1</owner> <group>-1</group> </permissions> </target> </pool>","title":"Storage Pool Operations"},{"location":"virtualization/virsh/#storage-volume-operations","text":"# List volumes virsh vol-list default # Volume information virsh vol-info vm1.qcow2 # Dump volume XML virsh vol-dumpxml vm1.qcow2 # Create volume virsh vol-create-as default vm1.qcow2 20G # Create volume from XML virsh vol-create default volume.xml # Delete volume virsh vol-delete default vm1.qcow2 # Upload to volume virsh vol-upload default vm1.qcow2 disk.qcow2 # Download from volume virsh vol-download default vm1.qcow2 disk.qcow2 # Clone volume virsh vol-clone default vm1.qcow2 vm2.qcow2 # Wipe volume virsh vol-wipe default vm1.qcow2 # Resize volume virsh vol-resize default vm1.qcow2 30G","title":"Storage Volume Operations"},{"location":"virtualization/virsh/#secret-management","text":"","title":"Secret Management"},{"location":"virtualization/virsh/#secret-operations","text":"# Define secret cat > secret.xml <<EOF <secret ephemeral='no' private='no'> <description>libvirt secret</description> <usage type='ceph'> <name>client.admin secret</name> </usage> </secret> EOF virsh secret-define secret.xml # Set secret value virsh secret-set-value secret.xml /path/to/key # Get secret value virsh secret-get-value secret.xml # Delete secret virsh secret-undefine secret.xml # List secrets virsh secret-list","title":"Secret Operations"},{"location":"virtualization/virsh/#advanced-commands","text":"","title":"Advanced Commands"},{"location":"virtualization/virsh/#guest-operations","text":"# Guest agent command virsh qemu-agent-command vm1 '{\"execute\":\"guest-ping\"}' # Guest shutdown virsh qemu-agent-command vm1 '{\"execute\":\"guest-shutdown\",\"arguments\":{\"mode\":\"poweroff\"}}' # Guest info virsh qemu-agent-command vm1 '{\"execute\":\"guest-info\"}'","title":"Guest Operations"},{"location":"virtualization/virsh/#host-management","text":"# Node info virsh nodeinfo # Host capabilities virsh capabilities # CPU models virsh cpu-models # Hostname virsh hostname # Sysinfo virsh sysinfo","title":"Host Management"},{"location":"virtualization/virsh/#version-information","text":"# Version virsh version # Compiled information virsh version # Daemon version virsh version --daemon","title":"Version Information"},{"location":"virtualization/virsh/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"virtualization/virsh/#vm-issues","text":"# Check VM logs tail -f /var/log/libvirt/qemu/vm1.log # Check libvirt logs tail -f /var/log/libvirt/libvirtd.log # Check VM status virsh domstate vm1 virsh domcontrol vm1 # Check XML configuration virsh dumpxml vm1 # Check console virsh console vm1","title":"VM Issues"},{"location":"virtualization/virsh/#network-issues","text":"# Check network status virsh net-list --all # Check network XML virsh net-dumpxml default # Check network logs journalctl -u libvirtd -f # Check bridge brctl show ip addr show virbr0","title":"Network Issues"},{"location":"virtualization/virsh/#storage-issues","text":"# Check pool status virsh pool-list --all # Check pool XML virsh pool-dumpxml default # Check volume status virsh vol-list default # Check storage logs journalctl -u libvirtd -f","title":"Storage Issues"},{"location":"virtualization/virsh/#best-practices","text":"Use --persistent for permanent changes Create snapshots before major changes Test migration before production Use proper storage for volumes Monitor VMs regularly Use autostart for critical VMs Document configurations with XML Test backups regularly Use virsh edit for configuration changes Check logs for troubleshooting","title":"Best Practices"},{"location":"virtualization/virsh/#quick-reference","text":"","title":"Quick Reference"},{"location":"virtualization/virsh/#common-commands","text":"Command Description virsh list List running VMs virsh list --all List all VMs virsh start vm1 Start VM virsh shutdown vm1 Shutdown VM virsh destroy vm1 Force stop VM virsh reboot vm1 Reboot VM virsh suspend vm1 Suspend VM virsh resume vm1 Resume VM virsh define vm1.xml Define VM virsh undefine vm1 Remove VM virsh dumpxml vm1 Export XML virsh edit vm1 Edit XML virsh dominfo vm1 VM information virsh domstate vm1 VM state virsh console vm1 Connect console virsh snapshot-create vm1 Create snapshot virsh snapshot-list vm1 List snapshots virsh snapshot-revert vm1 snap1 Revert snapshot virsh migrate --live vm1 dest Live migration","title":"Common Commands"},{"location":"virtualization/virsh/#documentation","text":"virsh Manual : man virsh libvirt Documentation : https://libvirt.org/docs/ virsh Cheat Sheet : https://libvirt.org/virshcmdref/","title":"Documentation"}]}